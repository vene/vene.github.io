<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Vlad Niculae - scikit-learn</title><link href="//vene.ro/" rel="alternate"/><link href="//vene.ro/feeds/scikit-learn.atom.xml" rel="self"/><id>//vene.ro/</id><updated>2012-08-20T02:44:00+02:00</updated><entry><title>Scikit-learn-speed: An overview on the final day</title><link href="//vene.ro/blog/scikit-learn-speed-an-overview-on-the-final-day.html" rel="alternate"/><published>2012-08-20T02:44:00+02:00</published><updated>2012-08-20T02:44:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-08-20:/blog/scikit-learn-speed-an-overview-on-the-final-day.html</id><summary type="html">&lt;p&gt;This summer, I was granted the project called &lt;em&gt;scikit-learn-speed&lt;/em&gt;,
consisting of developing a benchmarking platform for &lt;em&gt;scikit-learn&lt;/em&gt; and
using it to find potential speedups, and in the end, make the library go
faster wherever I&amp;nbsp;can.&lt;/p&gt;
&lt;p&gt;On the official closing day of this work, I&amp;#8217;d like to take a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This summer, I was granted the project called &lt;em&gt;scikit-learn-speed&lt;/em&gt;,
consisting of developing a benchmarking platform for &lt;em&gt;scikit-learn&lt;/em&gt; and
using it to find potential speedups, and in the end, make the library go
faster wherever I&amp;nbsp;can.&lt;/p&gt;
&lt;p&gt;On the official closing day of this work, I&amp;#8217;d like to take a moment and
recall the accomplishments and failures of this project, and all the
lessons to be&amp;nbsp;learned.&lt;/p&gt;
&lt;h2 id="the-scikit-learn-speed-benchmark-platform"&gt;The &lt;em&gt;scikit-learn-speed&lt;/em&gt; benchmark platform&lt;a class="headerlink" href="#the-scikit-learn-speed-benchmark-platform" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/skl-speed-300x163.png" title="skl-speed" /&gt;]&lt;a href="http://localhost:8001/wp-content/uploads/2012/08/skl-speed-300x163.png" title="skl-speed"&gt;&lt;/a&gt;&lt;br /&gt;
[&lt;em&gt;Scikit-learn-speed&lt;/em&gt;][&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/skl-speed-300x163.png" title="skl-speed" /&gt;] is a continuous benchmark suite for the
&lt;a href="http://scikit-learn.org"&gt;&lt;em&gt;scikit-learn&lt;/em&gt;&lt;/a&gt; library. It has the following&amp;nbsp;features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;vbench&lt;/em&gt;-powered integration with&amp;nbsp;Git&lt;/li&gt;
&lt;li&gt;Easily triggered build and report generation: just&amp;nbsp;type &lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Easily readable and writeable template for benchmarks:
    &lt;p&gt;
    [sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
    {&lt;br /&gt;
    &amp;#8216;obj&amp;#8217;: &amp;#8216;LogisticRegression&amp;#8217;,&lt;br /&gt;
    &amp;#8216;init_params&amp;#8217;: {&amp;#8216;C&amp;#8217;: 1e5},&lt;br /&gt;
    &amp;#8216;datasets&amp;#8217;: (&amp;#8216;arcene&amp;#8217;, &amp;#8216;madelon&amp;#8217;),&lt;br /&gt;
    &amp;#8216;statements&amp;#8217;: (&amp;#8216;fit&amp;#8217;, &amp;#8216;predict&amp;#8217;)&lt;br /&gt;
    }, &amp;#8230;&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/li&gt;
&lt;li&gt;Many attributes recorded: time (w/ estimated standard deviation),
    memory usage, cProfiler output, line_profiler output,&amp;nbsp;tracebacks&lt;/li&gt;
&lt;li&gt;Multi-step benchmarks:&amp;nbsp;i.e. &lt;code&gt;fit&lt;/code&gt; followed&amp;nbsp;by &lt;code&gt;predict&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What were the lessons I learned&amp;nbsp;here?&lt;/p&gt;
&lt;h3 id="make-your-work-reusable-the-trade-off-between-good-design-and-get-it-working-now"&gt;Make your work reusable: the trade-off between good design and get-it-working-now&lt;a class="headerlink" href="#make-your-work-reusable-the-trade-off-between-good-design-and-get-it-working-now" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;For the task of rolling out a continuous benchmarking platform, we
decided pretty early in the project to adopt Wes McKinney&amp;#8217;s &lt;em&gt;vbench&lt;/em&gt;. If
my goal would&amp;#8217;ve been to maintain &lt;em&gt;vbench&lt;/em&gt; and extend it into a
multi-purpose, reusable benchmarking framework, the work would&amp;#8217;ve been
structured differently. It also would have been very open-ended and
difficult to&amp;nbsp;quantify.&lt;/p&gt;
&lt;p&gt;The way things have been, I came up with features that we need in
&lt;em&gt;scikit-learn-speed&lt;/em&gt;, and tried to implement them in &lt;em&gt;vbench&lt;/em&gt; without
refactoring too much, but still by trying to make them as reusable as&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;The result? I got all the features for &lt;em&gt;scikit-learn-speed&lt;/em&gt;, but the
implementation is not yet clean enough to be merged into &lt;em&gt;vbench&lt;/em&gt;. This
is fine for a project with a tight deadline such as this one: after it&amp;#8217;s
done, I will just spend another weekend on cleaning the work up and
making sure it&amp;#8217;s appreciated upstream. This will be easier because of
the constraint to keep compatibility with &lt;em&gt;scikit-learn-speed&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="never-work-quietly-unless-youre-a-ninja"&gt;Never work quietly (unless you&amp;#8217;re a ninja)&lt;a class="headerlink" href="#never-work-quietly-unless-youre-a-ninja" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I know some students who prefer that the professor doesn&amp;#8217;t even know
they exist until the final, when they would score an A, and (supposedly)
leave the professor amazed. In real life, plenty of people would be
interested in what you are doing, as long as they know about it. The &lt;span class="caps"&gt;PSF&lt;/span&gt;
goes a long way to help this, with the &amp;#8220;blog weekly&amp;#8221; rule. In the end,
however, it&amp;#8217;s all up to you to make sure that everybody who should know
finds out about your work. It will spare the world the duplicated work,
the abandoned projects, but most importantly, those people could point
you to things you have missed. Try to mingle in real-life as well,
attend conferences, meetups, coding&amp;nbsp;sprints.&lt;/p&gt;
&lt;p&gt;I was able to slightly &amp;#8220;join forces&amp;#8221; with a couple of people who
contacted me about my new &lt;em&gt;vbench&lt;/em&gt; features (Hi Jon and Joel!), I have
shaped my design slightly towards their requirements as well, and
hopefully the result will be a more general &lt;em&gt;vbench&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="the-speedups"&gt;The speedups&lt;a class="headerlink" href="#the-speedups" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Once &lt;em&gt;scikit-learn-speed&lt;/em&gt; was up and running, I couldn&amp;#8217;t believe how
useful it is to be able to scroll, catch slow code and jump straight at
the profiler output with one click. I jumped on the following&amp;nbsp;speed-ups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple outputs in linear models. (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/913"&gt;&lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;)
    &lt;p&gt;
    Some of them proved trickier than expected, so I didn&amp;#8217;t implement it
    for all the module yet, but it is ready for some&amp;nbsp;estimators.&lt;/li&gt;
&lt;li&gt;Less callable functions passed around&amp;nbsp;in &lt;code&gt;FastICA&lt;/code&gt; (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/927"&gt;merged&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Speed&amp;nbsp;up &lt;code&gt;euclidean_distances&lt;/code&gt; by rewriting in Cython. (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/1006"&gt;&lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;)
    &lt;p&gt;
    This meant making more operations support&amp;nbsp;an &lt;code&gt;out&lt;/code&gt; argument, for
    passing preallocated memory. This touches many&lt;br /&gt;
    different objects in the codebase: clustering, manifold learning,
    nearest neighbour&amp;nbsp;methods.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://localhost:8001/2012/08/18/inverses-pseudoinverses-numerical-issues-speed-symmetry/" title="Inverses and pseudoinverses. Numerical issues, speed, symmetry."&gt;Insight into inverse and pseudoinverse computation&lt;/a&gt;,&amp;nbsp;new &lt;code&gt;pinvh&lt;/code&gt;
    function for inverting symmetric/hermitian matrices. (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/1015"&gt;&lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;)
    &lt;p&gt;
    This speeds up the covariance module&amp;nbsp;(especially &lt;code&gt;MinCovDet&lt;/code&gt;),
    &lt;code&gt;ARDRegression&lt;/code&gt; and the mixture models. It also lead to an &lt;a href="https://github.com/scipy/scipy/pull/289"&gt;upstream
    contribution to&amp;nbsp;Scipy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OrthogonalMatchingPursuit&lt;/code&gt; forward stepwise path for
    cross-validation (&lt;a href="https://github.com/scikit-learn/scikit-learn/pull/1042"&gt;&lt;span class="caps"&gt;PR&lt;/span&gt;&lt;/a&gt;)
    &lt;p&gt;
    This is only halfway finished, but it will lead to faster and easier
    optimization of&amp;nbsp;the &lt;code&gt;OMP&lt;/code&gt; sparsity&amp;nbsp;parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lessons? These will be pretty&amp;nbsp;obvious.&lt;/p&gt;
&lt;h3 id="write-tests-tests-tests"&gt;Write tests, tests, tests!&lt;a class="headerlink" href="#write-tests-tests-tests" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is a no-brainer, but it still didn&amp;#8217;t stick. In that one case out of
10 that I didn&amp;#8217;t explicitly test, a bug was obviously hiding. When you
want to add a new feature, it&amp;#8217;s best to start by writing a failing test,
and then &lt;a href="http://c2.com/cgi/wiki?MakeItWorkMakeItRightMakeItFast"&gt;making it pass&lt;/a&gt;. Sure, you will miss tricky bugs, but you
will never have embarrassing, obvious bugs in your code&amp;nbsp;:)&lt;/p&gt;
&lt;h3 id="optimization-doesnt-have-to-be-ugly"&gt;Optimization doesn&amp;#8217;t have to be ugly&lt;a class="headerlink" href="#optimization-doesnt-have-to-be-ugly" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Developers often shun optimization. It&amp;#8217;s true, you should profile first,
and you shouldn&amp;#8217;t focus on speeding up stuff that is dominated by other
computations that are orders of magnitude slower. However, there is an
elephant in the room: the assumption that making code faster invariably
makes it less clear, and takes a lot of&amp;nbsp;effort.&lt;/p&gt;
&lt;p&gt;The following code is a part of&amp;nbsp;scipy&amp;#8217;s &lt;code&gt;pinv2&lt;/code&gt; function as it currently
is written:&lt;br /&gt;
[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
cutoff = cond*np.maximum.reduce(s)&lt;br /&gt;
psigma = np.zeros((m, n), t)&lt;br /&gt;
for i in range(len(s)):&lt;br /&gt;
if s[i] &amp;gt; cutoff:&lt;br /&gt;
psigma[i,i] = 1.0/np.conjugate(s[i])&lt;br /&gt;
return np.transpose(np.conjugate(np.dot(np.dot(u,psigma),vh)))&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;&lt;code&gt;psigma&lt;/code&gt; is a diagonal matrix, and some time and memory can be saved
with simple vectorization. However, this part of the code dominated by
an above call&amp;nbsp;to &lt;code&gt;svd&lt;/code&gt;. The profiler output would say that we shouldn&amp;#8217;t
bother, but is it really a bother? Look at Jake&amp;#8217;s new&amp;nbsp;version:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
above_cutoff = (s &amp;gt; cond * np.max(s))&lt;br /&gt;
psigma_diag = np.zeros_like(s)&lt;br /&gt;
psigma_diag[above_cutoff] = 1.0 /&amp;nbsp;s[above_cutoff]&lt;/p&gt;
&lt;p&gt;return np.transpose(np.conjugate(np.dot(u * psigma_diag, vh)))&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s shorter, more elegant, easier to read, and nevertheless faster. I
would say it is worth&amp;nbsp;it.&lt;/p&gt;
&lt;h3 id="small-speed-ups-can-propagate"&gt;Small speed-ups can propagate&lt;a class="headerlink" href="#small-speed-ups-can-propagate" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Sure, it&amp;#8217;s great if you can compute an inverse two times faster, say in
0.5s instead of 1s. But if some algorithm calls this function in a loop
that might iterate 100, 300, or 1000 times, this small speed-up seems
much more important, doesn&amp;#8217;t&amp;nbsp;it?&lt;/p&gt;
&lt;p&gt;What I&amp;#8217;m trying to say with this is that in a well-engineered system, a
performance improvement to a relatively small component (such as the
function that computes a pseudoinverse) can lead to multiple spread out
improvements. Be careful of the double edge of this sword, a bug
introduced in a small part can cause multiple failures downstream. But
you &lt;em&gt;are&lt;/em&gt; fully covered by your test suite, aren&amp;#8217;t&amp;nbsp;you?&lt;/p&gt;
&lt;p&gt;Overall it has been a fruitful project that may have not resulted in a
large number of speed-ups, but a few considerable ones nonetheless. And
I venture the claim that the &lt;em&gt;scikit-learn-speed&lt;/em&gt; tool will prove useful
over time, and that the efforts deployed during this project have
stretched beyond the boundary of the &lt;em&gt;scikit-learn&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/skl-speed-300x163.png" title="skl-speed" /&gt;]:&amp;nbsp;http://jenkins-scikit-learn.github.com/scikit-learn-speed/&lt;/p&gt;</content><category term="scikit-learn"/><category term="gsoc"/><category term="optimization"/><category term="scikit-learn-speed"/><category term="speedup"/><category term="summary"/><category term="vbench"/><category term="benchmarking"/><category term="python"/><category term="scikit-learn"/></entry><entry><title>The scikit-learn-speed ship has set sail! Faster than ever, with multi-step benchmarks!</title><link href="//vene.ro/blog/the-scikit-learn-speed-ship-has-set-sail-faster-than-ever-with-multi-step-benchmarks.html" rel="alternate"/><published>2012-08-11T17:32:00+02:00</published><updated>2012-08-11T17:32:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-08-11:/blog/the-scikit-learn-speed-ship-has-set-sail-faster-than-ever-with-multi-step-benchmarks.html</id><summary type="html">&lt;p&gt;I am pleased to announce that last night at 2:03 &lt;span class="caps"&gt;AM&lt;/span&gt;, the first fully
automated run of the scikit-learn-speed test suite has run on our
Jenkins instance! You can admire it at &lt;a href="http://jenkins-scikit-learn.github.com/scikit-learn-speed/"&gt;its temporary home&lt;/a&gt; for now.
As soon as we verify that everything is good, we will move …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am pleased to announce that last night at 2:03 &lt;span class="caps"&gt;AM&lt;/span&gt;, the first fully
automated run of the scikit-learn-speed test suite has run on our
Jenkins instance! You can admire it at &lt;a href="http://jenkins-scikit-learn.github.com/scikit-learn-speed/"&gt;its temporary home&lt;/a&gt; for now.
As soon as we verify that everything is good, we will move this to the
official scikit-learn&amp;nbsp;page.&lt;/p&gt;
&lt;p&gt;I would like to take this opportunity to tell you about our latest
changeset. We made running the benchmark suite tons simpler by adding a
friendly Makefile. You can read more about its usage in the guide. But
by far, our coolest new toy&amp;nbsp;is:&lt;/p&gt;
&lt;h2 id="multi-step-benchmarks"&gt;Multi-step benchmarks&lt;a class="headerlink" href="#multi-step-benchmarks" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A standard vbench benchmark has three units of code, represented as&amp;nbsp;strings: &lt;code&gt;code&lt;/code&gt;, &lt;code&gt;setup&lt;/code&gt; and &lt;code&gt;cleanup&lt;/code&gt;. With the original timeit-based
benchmarks, this means that for every run, the setup would be executed
once. Then, the main loop&amp;nbsp;runs &lt;code&gt;repeat&lt;/code&gt; times, and within each
iteration,&amp;nbsp;the &lt;code&gt;code&lt;/code&gt; is&amp;nbsp;run &lt;code&gt;ncalls&lt;/code&gt; times.&amp;nbsp;Then &lt;code&gt;cleanup&lt;/code&gt; happens, the
best time is returned, and everybody is&amp;nbsp;happy.&lt;/p&gt;
&lt;p&gt;In scikit-learn, most of our interesting objects go through a state
change called &lt;em&gt;fitting&lt;/em&gt;. This metaphor is right at home in the machine
learning field, where we separate the learning phase for the prediction
phase. The prediction step cannot be invoked on an object that hasn&amp;#8217;t
been&amp;nbsp;fitted.&lt;/p&gt;
&lt;p&gt;For some algorithms, one of these steps is trivial. A brute force
Nearest Neighbors classifier can be instantaneously fit, but prediction
takes a while. On the opposite end we have linear models, with tons of
complicated algorithms to fit them, but evaluation is a simple
matrix-vector product that Numpy handles&amp;nbsp;perfectly.&lt;/p&gt;
&lt;p&gt;But many of scikit-learn&amp;#8217;s estimators have both steps interesting. Let&amp;#8217;s
take Non-negative Matrix Factorization. It has three interesting
functions:&amp;nbsp;The &lt;code&gt;fit&lt;/code&gt; that computes $latex X = &lt;span class="caps"&gt;WH&lt;/span&gt; $,&amp;nbsp;the &lt;code&gt;transform&lt;/code&gt;
that computes a non-negative projection on the components learned&amp;nbsp;in
&lt;code&gt;fit&lt;/code&gt;,&amp;nbsp;and &lt;code&gt;fit_transform&lt;/code&gt; that takes advantage of the observation that
when fitting, we also get the transformed $latex X $ for&amp;nbsp;free.&lt;/p&gt;
&lt;p&gt;When benchmarking &lt;span class="caps"&gt;NMF&lt;/span&gt;, we initially had to design 3&amp;nbsp;benchmarks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;setup =&lt;/code&gt;standard, &lt;code&gt;code = obj.fit(X)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setup =&lt;/code&gt;standard, &lt;code&gt;code = obj.fit_transform(X)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setup =&lt;/code&gt;standard&lt;code&gt;+ obj.fit(X)&lt;/code&gt;, &lt;code&gt;code = obj.transform(X)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="how-much-time-were-we-wasting"&gt;How much time were we wasting?&lt;a class="headerlink" href="#how-much-time-were-we-wasting" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s say it takes 10 seconds. For every benchmark, we time the code by
running it 3 times. We run it once more to measure memory usage, once
more&amp;nbsp;for &lt;code&gt;cProfile&lt;/code&gt; and one last time&amp;nbsp;for &lt;code&gt;line_profiler&lt;/code&gt;. This is a
total of 6 times per benchmark. We need to multiply this by 2 again for
running on two datasets. So when&amp;nbsp;benchmarking &lt;code&gt;NMF&lt;/code&gt;, because we need to
fit before predicting, we do it 12 extra times. If a fit takes 5
seconds, this means one minute wasted on benchmarking just one
estimator. &lt;em&gt;Wouldn&amp;#8217;t it be nice&amp;nbsp;to &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;fit_transform&lt;/code&gt; and
&lt;code&gt;transform&lt;/code&gt; in a&amp;nbsp;sequence?&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="behind-the-scenes"&gt;Behind the scenes&lt;a class="headerlink" href="#behind-the-scenes" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We made&amp;nbsp;the &lt;code&gt;PythonBenchmark code&lt;/code&gt; parameter also support getting a
sequence of strings, instead of just a string. On the database side,
every benchmark result entry gets an extra component in the primary key,
the number of the step it&amp;nbsp;measures.&lt;/p&gt;
&lt;p&gt;In the benchmark description files, nothing is&amp;nbsp;changed:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
{&lt;br /&gt;
&amp;#8216;obj&amp;#8217;: &amp;#8216;&lt;span class="caps"&gt;NMF&lt;/span&gt;&amp;#8217;,&lt;br /&gt;
&amp;#8216;init_params&amp;#8217;: {&amp;#8216;n_components&amp;#8217;: 2},&lt;br /&gt;
&amp;#8216;datasets&amp;#8217;: (&amp;#8216;blobs&amp;#8217;,),&lt;br /&gt;
&amp;#8216;statements&amp;#8217;: (&amp;#8216;fit_unsup&amp;#8217;, &amp;#8216;transform_unsup&amp;#8217;, &amp;#8216;fit_transform&amp;#8217;)&lt;br /&gt;
},&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;But before, we would take the cartesian product of datasets and
statements, and build&amp;nbsp;a &lt;code&gt;Benchmark&lt;/code&gt; object for every pairing. Now, we
just pass the tuple as it is, and vbench is smart enough to do the right
thing.&lt;br /&gt;
We avoided the extra calls&amp;nbsp;to &lt;code&gt;fit&lt;/code&gt; in a lot of benchmarks. The whole
suite now takes almost half the time to&amp;nbsp;run!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; This trick is currently hosted in&amp;nbsp;the
&lt;code&gt;abstract_multistep_benchmarks&lt;/code&gt; vbench branch in my&amp;nbsp;fork.&lt;/p&gt;</content><category term="scikit-learn"/><category term="multi-step"/><category term="multistep"/><category term="vbench"/><category term="benchmarking"/><category term="python"/><category term="scikit-learn"/></entry><entry><title>Profiler output, benchmark standard deviation and other goodies in scikit-learn-speed</title><link href="//vene.ro/blog/profiler-output-benchmark-standard-deviation-and-other-goodies-in-scikit-learn-speed.html" rel="alternate"/><published>2012-07-27T11:01:00+02:00</published><updated>2012-07-27T11:01:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-07-27:/blog/profiler-output-benchmark-standard-deviation-and-other-goodies-in-scikit-learn-speed.html</id><summary type="html">&lt;p&gt;This post is about the &lt;a href="http://scikit-learn.org"&gt;scikit-learn&lt;/a&gt;benchmarking project that I am
working on, called &lt;a href="https://github.com/vene/scikit-learn-speed"&gt;scikit-learn-speed&lt;/a&gt;. This is a continuous
benchmarking suite that runs and generates &lt;span class="caps"&gt;HTML&lt;/span&gt; reports using Wes
McKinney&amp;#8217;s &lt;a href="http://wesmckinney.com/blog/?p=373"&gt;vbench&lt;/a&gt; framework, to which I had to make some (useful, I
hope)&amp;nbsp;additions.&lt;/p&gt;
&lt;h2 id="what-it-looks-like-now"&gt;What it looks like now&lt;a class="headerlink" href="#what-it-looks-like-now" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is about the &lt;a href="http://scikit-learn.org"&gt;scikit-learn&lt;/a&gt;benchmarking project that I am
working on, called &lt;a href="https://github.com/vene/scikit-learn-speed"&gt;scikit-learn-speed&lt;/a&gt;. This is a continuous
benchmarking suite that runs and generates &lt;span class="caps"&gt;HTML&lt;/span&gt; reports using Wes
McKinney&amp;#8217;s &lt;a href="http://wesmckinney.com/blog/?p=373"&gt;vbench&lt;/a&gt; framework, to which I had to make some (useful, I
hope)&amp;nbsp;additions.&lt;/p&gt;
&lt;h2 id="what-it-looks-like-now"&gt;What it looks like now&lt;a class="headerlink" href="#what-it-looks-like-now" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You can check out a &lt;a href="http://vene.github.com/scikit-learn-speed"&gt;teaser/demo&lt;/a&gt; that was run on equidistant releases
from the last two months. What has changed since the last version?
Here&amp;#8217;s a list in order of&amp;nbsp;obviousness:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We now use the lovely scikit-learn&amp;nbsp;theme&lt;/li&gt;
&lt;li&gt;Timing graphs now show the ±1 standard deviation&amp;nbsp;range&lt;/li&gt;
&lt;li&gt;cProfile output is displayed for all the benchmarks, so we can
    easily see at a glance what&amp;#8217;s&amp;nbsp;up&lt;/li&gt;
&lt;li&gt;Said profiler output is collapsible using &lt;a href="http://www.jqueryui.com/demos/accordion/"&gt;JQueryUI&amp;nbsp;goodness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;There now is an improved &lt;a href="http://vene.github.com/scikit-learn-speed/quick_start.html"&gt;Quick Start guide&lt;/a&gt; to running vbench on
    your&amp;nbsp;machine&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-made-this-possible"&gt;What made this possible&lt;a class="headerlink" href="#what-made-this-possible" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I have done some more refactoring in my vbench fork, because I didn&amp;#8217;t
want to have a huge,&amp;nbsp;monolithic &lt;code&gt;Benchmark&lt;/code&gt; class that was specific to
what we want in scikit-learn-speed. So on this branch, I set up a
mixin/multiple inheritance hierarchy of benchmark&amp;nbsp;classes.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Benchmark&lt;/code&gt; class in vbench is now an abstract base class, with some
common functionality and structure.&lt;br /&gt;&amp;nbsp;Our &lt;code&gt;SklBenchmark&lt;/code&gt; class is defined in scikit-learn-speed&amp;nbsp;as:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;class SklBenchmark(CProfileBenchmarkMixin,  MemoryBenchmarkMixin, PythonBenchmark):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s read this from right to&amp;nbsp;left:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PythonBenchmark&lt;/code&gt;: This class&amp;nbsp;stores &lt;code&gt;code&lt;/code&gt;, &lt;code&gt;setup&lt;/code&gt; and &lt;code&gt;cleanup&lt;/code&gt;
    Python code as strings, and implements simple timing mechanisms
    using&amp;nbsp;the &lt;code&gt;time&lt;/code&gt; module.&lt;/li&gt;
&lt;li&gt;Bonus: &lt;code&gt;TimeitBenchmark&lt;/code&gt;: This class&amp;nbsp;extends &lt;code&gt;PythonBenchmark&lt;/code&gt; with&amp;nbsp;the &lt;code&gt;timeit&lt;/code&gt; micro-benchmark timing method previously used in
    vbench. We turned this off in&amp;nbsp;scikit-learn-speed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MemoryBenchmarkMixin&lt;/code&gt;: This adds memory benchmarking using&amp;nbsp;[memory_profiler][].&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CProfileBenchmarkMixin&lt;/code&gt;: This runs the code through &lt;a href="http://docs.python.org/library/profile.html#module-cProfile"&gt;cProfile&lt;/a&gt;
    and implements mechanisms to report the output.&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The database is not flexible enough to adapt to arbitrary benchmark
structure right now, so if anybody would like to help the effort, it
would be very&amp;nbsp;appreciated.&lt;/p&gt;</content><category term="scikit-learn"/><category term="gsoc"/><category term="memory_profiler"/><category term="scikit-learn-speed"/><category term="vbench"/><category term="benchmarking"/><category term="python"/><category term="scikit-learn"/></entry><entry><title>Scikit-learn-speed HTML reports teaser</title><link href="//vene.ro/blog/scikit-learn-speed-html-reports-teaser.html" rel="alternate"/><published>2012-07-20T14:40:00+02:00</published><updated>2012-07-20T14:40:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-07-20:/blog/scikit-learn-speed-html-reports-teaser.html</id><summary type="html">&lt;p&gt;&lt;span class="caps"&gt;EDIT&lt;/span&gt;: I made the plots a little more readable, check it&amp;nbsp;out!&lt;/p&gt;
&lt;p&gt;Last time, I teased you with a screenshot of local output. Now, I will
tease you with the benchmarks run on a couple of recent commits, along
with some from earlier this&amp;nbsp;year.&lt;/p&gt;
&lt;p&gt;After some effort and bugfixes …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span class="caps"&gt;EDIT&lt;/span&gt;: I made the plots a little more readable, check it&amp;nbsp;out!&lt;/p&gt;
&lt;p&gt;Last time, I teased you with a screenshot of local output. Now, I will
tease you with the benchmarks run on a couple of recent commits, along
with some from earlier this&amp;nbsp;year.&lt;/p&gt;
&lt;p&gt;After some effort and bugfixes, the project now reliably runs on
different machines, so the next step to host it on a remote server and
invoke it daily is getting closer. In the mean time, you can have a look
at &lt;a href="http://vene.github.com/scikit-learn-speed/" title="scikit-learn-speed"&gt;the sample output&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that just last time, the plots look jagged but the differences are
mostly minor and significant conclusions cannot be drawn yet, but as the
suite will start running daily, the plots will become much more
meaningful. I could waste time running the suite on more previous
commits, but the results wouldn&amp;#8217;t be comparable with the ones from the
deployed system, because of hardware&amp;nbsp;differences.&lt;/p&gt;
&lt;p&gt;Playing around with this makes me want a couple of features in vbench.
One is the possibility to overlay related benchmarks on the same plot
(for example, different parameters for the same algorithm and data):
this could be useful to spot patterns. A second one is some query /
sorting support: see what are the most expensive benchmarks, see what
benchmarks show the biggest jump in performance (but this could become a
historical wall of fame or&amp;nbsp;shame).&lt;/p&gt;</content><category term="scikit-learn"/><category term="gsoc"/><category term="scikit-learn-speed"/><category term="vbench"/><category term="benchmarking"/><category term="python"/><category term="scikit-learn"/></entry><entry><title>Dynamically generated benchmarks with vbench</title><link href="//vene.ro/blog/dynamically-generated-benchmarks-with-vbench.html" rel="alternate"/><published>2012-06-07T01:57:00+02:00</published><updated>2012-06-07T01:57:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-06-07:/blog/dynamically-generated-benchmarks-with-vbench.html</id><summary type="html">&lt;p&gt;To construct&amp;nbsp;a &lt;code&gt;vbench&lt;/code&gt; benchmark you need a setup string and a code
string. The constructor&amp;#8217;s signature&amp;nbsp;is:  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;Benchmark(self, code, setup, ncalls=None, repeat=3, cleanup=None, name=None, description=None, start_date=None, logy=False)&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="why-generate-benchmarks-dynamically"&gt;Why generate benchmarks dynamically?&lt;a class="headerlink" href="#why-generate-benchmarks-dynamically" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For&amp;nbsp;most &lt;code&gt;scikit-learn&lt;/code&gt; purposes,&amp;nbsp;the &lt;code&gt;code&lt;/code&gt; string will be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;To construct&amp;nbsp;a &lt;code&gt;vbench&lt;/code&gt; benchmark you need a setup string and a code
string. The constructor&amp;#8217;s signature&amp;nbsp;is:  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;Benchmark(self, code, setup, ncalls=None, repeat=3, cleanup=None, name=None, description=None, start_date=None, logy=False)&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="why-generate-benchmarks-dynamically"&gt;Why generate benchmarks dynamically?&lt;a class="headerlink" href="#why-generate-benchmarks-dynamically" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For&amp;nbsp;most &lt;code&gt;scikit-learn&lt;/code&gt; purposes,&amp;nbsp;the &lt;code&gt;code&lt;/code&gt; string will be very close&amp;nbsp;to &lt;code&gt;"algorithm.fit(X, y)"&lt;/code&gt;, &lt;code&gt;"algorithm.transform(X)"&lt;/code&gt; or
&lt;code&gt;"algorithm.predict(X)"&lt;/code&gt;. We can generate a lot of benchmarks by
changing what the algorithm is, and changing what the data is or the way
it is&amp;nbsp;generated.&lt;/p&gt;
&lt;p&gt;A possible idea would be to create a
&lt;abbr title="domain-specific language" lang="en"&gt;&lt;span class="caps"&gt;DSL&lt;/span&gt;&lt;/abbr&gt; in which to
specify scikit-learn tests and create benchmarks from them. However,
before engineering such a solution, I wanted to test out how to generate
three related benchmarks using different arguments for the dataset
generation&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;This is what I came up&amp;nbsp;with:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
from vbench.benchmark import&amp;nbsp;Benchmark&lt;/p&gt;
&lt;p&gt;_setup = &amp;#8220;&amp;#8221;&amp;#8220;&lt;br /&gt;
from deps import&amp;nbsp;*&lt;/p&gt;
&lt;p&gt;kwargs = %s&lt;br /&gt;
X, y = make_regression(random_state=0, **kwargs)&lt;br /&gt;
lr = LinearRegression()&lt;br /&gt;&amp;nbsp;&amp;#8220;&amp;#8221;&amp;#8220;&lt;/p&gt;
&lt;p&gt;_configurations = [&lt;br /&gt;
(&amp;#8216;linear_regression_many_samples&amp;#8217;,&lt;br /&gt;
{&amp;#8216;n_samples&amp;#8217;: 10000, &amp;#8216;n_features&amp;#8217;: 100}),&lt;br /&gt;
(&amp;#8216;linear_regression_many_features&amp;#8217;,&lt;br /&gt;
{&amp;#8216;n_samples&amp;#8217;: 100, &amp;#8216;n_features&amp;#8217;: 10000}),&lt;br /&gt;
(&amp;#8216;linear_regression_many_targets&amp;#8217;,&lt;br /&gt;
{&amp;#8216;n_samples&amp;#8217;: 1000, &amp;#8216;n_features&amp;#8217;: 100, &amp;#8216;n_targets&amp;#8217;: 100})&lt;br /&gt;&amp;nbsp;]&lt;/p&gt;
&lt;p&gt;_statement = &amp;#8220;lr.fit(X,&amp;nbsp;y)&amp;#8221;&lt;/p&gt;
&lt;p&gt;_globs = globals()&lt;br /&gt;
_globs.update({name: Benchmark(_statement, _setup % str(kwargs),
name=name)&lt;br /&gt;
for name, kwargs in&amp;nbsp;_configurations})&lt;/p&gt;
&lt;p&gt;[/sourcecode]&lt;/p&gt;
&lt;p&gt;It works perfectly, but I don&amp;#8217;t like having to hack the globals to make
the benchmarks detectable. This is because of the way the vbench suite
gathers benchmarks.&amp;nbsp;In &lt;code&gt;__init__.py&lt;/code&gt; we have to&amp;nbsp;do
&lt;code&gt;from linear_regression import *&lt;/code&gt;. With a small update to the detection
method, we could replace the hacky part with a public lists of Benchmark&amp;nbsp;objects.&lt;/p&gt;
&lt;h2 id="exposed-issues"&gt;Exposed issues&lt;a class="headerlink" href="#exposed-issues" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;While working on this, after my first attempt, I was surprised to see
that there were no results added to the database, and output plots were
empty. It turns out that the generated benchmarks weren&amp;#8217;t running, even
though if I copied and pasted their source code from the generated html,
it would run. Vbench was not issuing any sort of message to let me know
that anything was&amp;nbsp;wrong.&lt;/p&gt;
&lt;p&gt;So what was the problem? My fault, of course, whitespace. But in all
fairness, we should add better&amp;nbsp;feedback.&lt;/p&gt;
&lt;p&gt;This is what I was doing to generate the setup&amp;nbsp;string:&lt;/p&gt;
&lt;p&gt;[sourcecode lang=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
def _make_setup(kwargs):&lt;br /&gt;
return &amp;#8220;&amp;#8221;&amp;#8220;&lt;br /&gt;
from deps import&amp;nbsp;*&lt;/p&gt;
&lt;p&gt;kwargs = %s&lt;br /&gt;
X, y = make_regression(random_state=0, **kwargs)&lt;br /&gt;
lr = LinearRegression()&lt;br /&gt;
&amp;#8220;&amp;#8221;&amp;#8221; % str(kwargs)&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s clear as daylight now that I overzealously indented the multiline
string. But man, was it hard to debug! Also, in this example, the bug
led to a refactoring that made the whole thing nicer and more direct.
Hopefully, my experience with vbench will lead to some improvements to
this cool and highly useful piece of&amp;nbsp;software.&lt;/p&gt;</content><category term="scikit-learn"/><category term="gsoc"/><category term="vbench"/><category term="benchmarking"/><category term="python"/></entry><entry><title>First contact with vbench</title><link href="//vene.ro/blog/first-contact-with-vbench.html" rel="alternate"/><published>2012-05-29T12:57:00+02:00</published><updated>2012-05-29T12:57:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-05-29:/blog/first-contact-with-vbench.html</id><summary type="html">&lt;p&gt;With a slight delay caused by going to lovely lovely Istanbul for the
&lt;span class="caps"&gt;LREC&lt;/span&gt; conference where I presented a &lt;a href="http://vene.ro/papers/lrec12-poster.pdf"&gt;poster&lt;/a&gt;, I am back to work on the
Google Summer of Code project. By the way, this year&amp;#8217;s logo and swag
looks a lot nicer than last year&amp;#8217;s, thank …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With a slight delay caused by going to lovely lovely Istanbul for the
&lt;span class="caps"&gt;LREC&lt;/span&gt; conference where I presented a &lt;a href="http://vene.ro/papers/lrec12-poster.pdf"&gt;poster&lt;/a&gt;, I am back to work on the
Google Summer of Code project. By the way, this year&amp;#8217;s logo and swag
looks a lot nicer than last year&amp;#8217;s, thank you Google!&lt;br /&gt;
[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/05/P5280194-300x225.jpg" title="GSoC swag" /&gt;]&lt;a href="http://localhost:8001/wp-content/uploads/2012/05/P5280194-300x225.jpg" title="GSoC swag"&gt;&lt;/a&gt;&lt;br /&gt;
The backbone of my GSoC consists of putting together a continuous
benchmark platform. I took a good look at &lt;a href="https://github.com/pydata/vbench"&gt;vbench&lt;/a&gt; and spent an
evening hacking Wes&amp;#8217;s benchmarks suite config into something that will
run on my machine. These are the key points I got from this&amp;nbsp;experience.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vbench is, at least for the moment, very specific to &lt;a href="http://pandas.pydata.org/pandas-docs/vbench/"&gt;Wes&amp;#8217; and
    Pandas&amp;#8217; needs&lt;/a&gt;. This is also because there weren&amp;#8217;t so many other
    users that could have brought&amp;nbsp;contributions.&lt;/li&gt;
&lt;li&gt;Even though it has support for some configuration and automation,
    vbench seems largely suited for running on a local machine.
    Specifically, it is &lt;span class="caps"&gt;NOT&lt;/span&gt; designed to run continuously but in one-off
    runs, going back in git history and getting the last commit for each
    day, and running the benchmark with it. Of course, it is trivial to
    patch it into getting just one&amp;nbsp;commit.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;code-as-strings&lt;/em&gt; approach is not ideal. The first thought is
    that it should be replaced with&amp;nbsp;reading &lt;code&gt;.py&lt;/code&gt; files into strings,
    but there are two issues with this:&lt;ol&gt;
&lt;li&gt;One benchmark file can have a lot of setup code and several key
    lines that need to actually be benched. This can be fixed using
    convensions (ie. setup functions&amp;nbsp;and &lt;code&gt;bench_*&lt;/code&gt; functions) in the
    spirit of testing suites, or using&amp;nbsp;decorators.&lt;/li&gt;
&lt;li&gt;I would like to be able to run bench files as python scripts,
    but the vbench import system breaks this. This can be fixed by
    hijacking the imports when reading the&amp;nbsp;file.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our project has different dynamics than Pandas, so it&amp;#8217;s important that
the published results run on an independent machine, but it would be
great if an individual developer can run the benchmark himself while
coding but before pushing his changes upstream. Of course, his numbers
would only be comparable to the numbers he gets on his own machine
before his changes, but a developer shouldn&amp;#8217;t wait for the daily
benchmark for knowing if he made an&amp;nbsp;improvement.&lt;/p&gt;
&lt;p&gt;On the other hand there is &lt;a href="http://code.google.com/p/unladen-swallow/"&gt;unladen-swallow&lt;/a&gt;&amp;#8216;s &lt;a href="http://code.google.com/p/unladen-swallow/wiki/Benchmarks"&gt;benchmark system&lt;/a&gt;
using the&amp;nbsp;[&lt;code&gt;perf.py&lt;/code&gt;]&lt;a href="http://localhost:8001/wp-content/uploads/2012/05/P5280194-300x225.jpg" title="GSoC swag"&gt;&lt;/a&gt; file. I didn&amp;#8217;t try it out yet, so I would like
feedback, but there are some key things that can be taken from&amp;nbsp;them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Memory usage&amp;nbsp;benchmarking&lt;/li&gt;
&lt;li&gt;Python scripts as benchmarks, with a simple but efficient Benchmark
    object&amp;nbsp;hierarchy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What&amp;#8217;s missing&amp;nbsp;is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A system to remember previous results and compare them, similar to
    vbench&amp;#8217;s&amp;nbsp;database&lt;/li&gt;
&lt;li&gt;The ability to bench only an area of the code without rerunning the
    setup. (Not really sure whether vbench&amp;#8217;s way is actually&amp;nbsp;better)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At a first glance, it seems that a very good system can be obtained by
combining these two excellent projects (or rather, improving vbench with
features&amp;nbsp;from &lt;code&gt;perf.py&lt;/code&gt;). While I continue exploring this, I would like
to hear feedback from people who had to do with similar issues. As for
the GSoC timeline, I plan to join forces with Immanuel and design a
solid benchmark suite for the linear models over the next 2&amp;nbsp;weeks.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://localhost:8001/wp-content/uploads/2012/05/P5280194-300x225.jpg" title="GSoC swag" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2012/05/P5280194.jpg&lt;/p&gt;</content><category term="scikit-learn"/><category term="benchmarks"/><category term="perf.py"/><category term="performance"/><category term="vbench"/><category term="scikit-learn"/></entry><entry><title>Support vector regression on Anscombe’s dataset</title><link href="//vene.ro/blog/support-vector-regression-on-anscombes-dataset.html" rel="alternate"/><published>2012-05-27T21:59:00+02:00</published><updated>2012-05-27T21:59:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-05-27:/blog/support-vector-regression-on-anscombes-dataset.html</id><summary type="html">&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Anscombe's_quartet" title="Anscombe's quartet"&gt;Anscombe&amp;#8217;s quartet&lt;/a&gt; is a set of four toy datasets that look very
different, but many of their statistics coincide. They were developed by
Francis Anscombe as a striking visual to show that even for small
datasets, blindly examining their statistical properties without
considering their structure can&amp;nbsp;mislead.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Anscombe's datasets" src="http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/640px-Anscombe%27s_quartet_3.svg.png" /&gt;&lt;/p&gt;
&lt;p&gt;Particularly, the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Anscombe's_quartet" title="Anscombe's quartet"&gt;Anscombe&amp;#8217;s quartet&lt;/a&gt; is a set of four toy datasets that look very
different, but many of their statistics coincide. They were developed by
Francis Anscombe as a striking visual to show that even for small
datasets, blindly examining their statistical properties without
considering their structure can&amp;nbsp;mislead.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Anscombe's datasets" src="http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/640px-Anscombe%27s_quartet_3.svg.png" /&gt;&lt;/p&gt;
&lt;p&gt;Particularly, the four datasets have the same &lt;a href="http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares" title="Ordinary least squares regression"&gt;least squares regression
line&lt;/a&gt;. While the second dataset is a clear example of a nonlinear
correlation which cannot be accurately captured by any linear model, the
third dataset is actually perfectly linear, with no noise, but just a
single outlier that shifts the regression line&amp;nbsp;considerably.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://scikit-learn.org/stable/modules/svm.html#regression" title="Support vector regression"&gt;Support vector regression&lt;/a&gt; is an extension of the support vector
machine idea to tackle the regression problem. It is based on the
observation that a &lt;span class="caps"&gt;SVM&lt;/span&gt; classifier builds its decision boundary as a
function of a (small) subset of training points. For regression, &lt;span class="caps"&gt;SVR&lt;/span&gt;
fits a &lt;em&gt;tube&lt;/em&gt; that is robust to noise within a width
[latex]\epsilon[/latex]. For this particular example, using a small
width makes the solution robust to the obvious outlier. For very small
but non-zero [latex]\epsilon[/latex], the solution is a combination of
the outlier and on two other points. For [latex]\epsilon=0[/latex], you
can see that every point except a non-outlier is highlighted. This is
actually the perfect solution but very&amp;nbsp;dense.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Support vector regression on Anscombe's third dataset with no
noise" src="http://localhost:8001/wp-content/uploads/2012/05/svr.gif" title="SVR on Anscombe's dataset with no noise" /&gt;][]&lt;/p&gt;
&lt;p&gt;Every frame displays the global mean squared error and the true mean
squared error, &lt;em&gt;i.e.&lt;/em&gt; over the inlying points. If the epsilon size is
well chosen, &lt;span class="caps"&gt;SVR&lt;/span&gt; can perform robustly with a sparse solution. Since our
interest was in avoiding the outlier, we assumed no noise in the inlying
data, so a very small epsilon is perfect. For real data a larger epsilon
is often useful because of variability in the data. When adding noise,
&lt;span class="caps"&gt;SVR&lt;/span&gt; still manages to avoid the outlier, but when the tube width becomes
zero, the solution is again very dense, very&amp;nbsp;non-parametric.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Support vector regression on Anscombe's third dataset with
noise" src="http://localhost:8001/wp-content/uploads/2012/05/svr_noise.gif" title="SVR on Anscombe's dataset with noise" /&gt;][]&lt;/p&gt;
&lt;p&gt;Here is the code you can use to play around with&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;[gist&amp;nbsp;id=2815589]&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Support vector regression on Anscombe's third dataset with no
  noise" src="http://localhost:8001/wp-content/uploads/2012/05/svr.gif" title="SVR on Anscombe's dataset with no noise" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2012/05/svr.gif&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Support vector regression on Anscombe's third dataset with
  noise" src="http://localhost:8001/wp-content/uploads/2012/05/svr_noise.gif" title="SVR on Anscombe's dataset with noise" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2012/05/svr_noise.gif&lt;/p&gt;</content><category term="scikit-learn"/><category term="anscombe"/><category term="outlier"/><category term="robust regression"/><category term="support vector regression"/><category term="svm"/><category term="svr"/><category term="python"/><category term="scikit-learn"/></entry><entry><title>GSoC 2012 proposal: Need for scikit-learn speed</title><link href="//vene.ro/blog/gsoc-2012-proposal-need-for-scikit-learn-speed.html" rel="alternate"/><published>2012-04-16T00:37:00+02:00</published><updated>2012-04-16T00:37:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2012-04-16:/blog/gsoc-2012-proposal-need-for-scikit-learn-speed.html</id><summary type="html">&lt;p&gt;This summer I hope to be able to put in another full-time amount of
effort into scikit-learn. After a successful Google Summer of Code
project last year on dictionary learning, I now plan to do some
low-level work. The title of my proposal is: &amp;#8220;Need for scikit-learn
speed&amp;#8221; and, in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This summer I hope to be able to put in another full-time amount of
effort into scikit-learn. After a successful Google Summer of Code
project last year on dictionary learning, I now plan to do some
low-level work. The title of my proposal is: &amp;#8220;Need for scikit-learn
speed&amp;#8221; and, in a nutshell, will make the scikit go faster and will help
it stay that&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;Scikit-learn has always enforced standards of quality that kept all
implementations at a non-trivial level (i.e. faster than using &lt;a href="http://docs.scipy.org/doc/scipy/reference/optimize.html"&gt;the
generic optimizers in scipy&lt;/a&gt;). However, not all modules are equal:
some have received more attention for speed than others (for example the
&lt;span class="caps"&gt;SGD&lt;/span&gt; classes). I intend to raise the bar towards a more uniform&amp;nbsp;level.&lt;/p&gt;
&lt;h2 id="are-you-crazy-can-you-really-do-this"&gt;Are you crazy, can you really do this?&lt;a class="headerlink" href="#are-you-crazy-can-you-really-do-this" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Well, of course. This might not the usual GSoC proposal, but I can show
how I can do it and how it&amp;#8217;s easily quantifiable. Actually, a very
important part of the work will be to make scikit-learn&amp;#8217;s speed easily&amp;nbsp;measurable.&lt;/p&gt;
&lt;p&gt;As for the specific speed-ups, I have shown &lt;a href="http://localhost:8001/2011/08/07/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1/" title="Optimizing Orthogonal Matching Pursuit code in Numpy, part 1"&gt;in&lt;/a&gt; &lt;a href="http://localhost:8001/2011/08/11/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2/" title="Optimizing Orthogonal Matching Pursuit code in Numpy, part 2"&gt;the&lt;/a&gt; &lt;a href="http://localhost:8001/2011/11/18/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code/" title="The nasty bug crawling in my Orthogonal Matching Pursuit code"&gt;past&lt;/a&gt; that
I can do algorithmic and memory layout optimizations in numerical code.
There are parts in the scikit-learn that can benefit from such work: for
example only recently Peter merged this &lt;a href="https://github.com/scikit-learn/scikit-learn/pull/545"&gt;pull request&lt;/a&gt; significantly
improving SGDClassifier&amp;#8217;s test time performance by switching the memory
layout of the coefficients: they were laid out optimally for the
training phase, not for the prediction&amp;nbsp;phase.&lt;/p&gt;
&lt;p&gt;There are certainly more opportunities for such speed improvements in
the scikit. Of course there is a lot of code that can&amp;#8217;t reasonably be
made any faster (I have a feeling that SGDClassifier is at the moment
such a case, but we can&amp;#8217;t know for sure without heavy profiling). But
generally there are many speed fixes that could weigh a lot: for
example, a &lt;a href="http://cython.org/"&gt;Cython&lt;/a&gt; implementation of&amp;nbsp;the &lt;code&gt;euclidean_distances&lt;/code&gt;
function that is able to use preallocated memory will improve the
performance of raw NearestNeighbours queries as well as of the KMeans
and hierarchical clustering&amp;nbsp;algorithms.&lt;/p&gt;
&lt;h2 id="how-will-we-be-able-to-tell-if-you-succeed"&gt;How will we be able to tell if you succeed?&lt;a class="headerlink" href="#how-will-we-be-able-to-tell-if-you-succeed" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A key part of the GSoC project is setting up a
&lt;abbr title="Continuous Integration"&gt;&lt;span class="caps"&gt;CI&lt;/span&gt;&lt;/abbr&gt;-style benchmark platform.
The point is to be able to track how the speed of certain operations
evolves in time. For such purposes, Wes McKinney developed the
&lt;a href="https://github.com/pydata/vbench"&gt;vbench&lt;/a&gt; project, introduced in &lt;a href="http://wesmckinney.com/blog/?p=373"&gt;this blog post&lt;/a&gt;. The goal is for
every scikit-learn module to have several such benchmarks, for
differently shaped and structured&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Having such a benchmark suite available is the equivalent of a test
suite, in terms of performance. It makes developers be extra conscious
of the effect of their changes. It also makes it more fun to chase speed
improvements, thanks to the positive reinforcement it&amp;nbsp;gives.&lt;/p&gt;
&lt;p&gt;There are some static benchmarks comparing the performance of
scikit-learn algorithms with other well-known libraries in the
&lt;a href="http://scikit-learn.sourceforge.net/ml-benchmarks/"&gt;ml-benchmarks&lt;/a&gt; project. It would be very helpful to have such a
benchmark suite that automatically keeps&amp;nbsp;up-to-date.&lt;/p&gt;
&lt;h2 id="side-effects"&gt;Side effects&lt;a class="headerlink" href="#side-effects" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The cool thing about such a project is that it should raise the overall
quality of the scikit. The refactoring will lead to an increase in test
coverage, because the low-coverage modules are expected to be less
optimized as well. Also, the benchmarks will lead to well-backed
summaries in the documentation, such as &lt;a href="http://scikit-learn.org/dev/modules/clustering.html#overview-of-clustering-methods"&gt;the one recently added in the
clustering section&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since the scikit is reaching a state where many well-known algorithms
are available, the &lt;strong&gt;1.0&lt;/strong&gt; release is slowly approaching. My Google
Summer of Code project should bring the scikit significantly closer to
that&amp;nbsp;milestone.&lt;/p&gt;</content><category term="scikit-learn"/><category term="gsoc"/><category term="proposal"/><category term="scikit-learn"/></entry><entry><title>The nasty bug crawling in my Orthogonal Matching Pursuit code</title><link href="//vene.ro/blog/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code.html" rel="alternate"/><published>2011-11-18T20:51:00+01:00</published><updated>2011-11-18T20:51:00+01:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-11-18:/blog/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code.html</id><summary type="html">&lt;p&gt;A while back, Bob L. Sturm blogged about a &lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/efficient-omp.html"&gt;similar implementation of
&lt;span class="caps"&gt;OMP&lt;/span&gt;&lt;/a&gt; to the one in scikit-learn. Instead of using the Cholesky
decomposition like we did, his Matlab code uses the &lt;span class="caps"&gt;QR&lt;/span&gt; decomposition, to
a similar (or maybe even identical) outcome, in theory. So lucky that
Alejandro pointed out …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A while back, Bob L. Sturm blogged about a &lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/efficient-omp.html"&gt;similar implementation of
&lt;span class="caps"&gt;OMP&lt;/span&gt;&lt;/a&gt; to the one in scikit-learn. Instead of using the Cholesky
decomposition like we did, his Matlab code uses the &lt;span class="caps"&gt;QR&lt;/span&gt; decomposition, to
a similar (or maybe even identical) outcome, in theory. So lucky that
Alejandro pointed out to him the existence of the scikit-learn
implementation, and that Bob&amp;#8217;s code &lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/omp-in-python-strange-results.html"&gt;exposed a bug&lt;/a&gt; that all the test
coverage didn&amp;#8217;t catch! This plot should increase, certainly not
decrease! Something is clearly wrong here.&lt;br /&gt;
&lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/omp-in-python-strange-results.html"&gt;&lt;img alt="OMP buggy phase transition, decreasing instead of
increasing" src="http://media.aau.dk/null_space_pursuits/2011/10/17/OMPscikit.png" title="OMP buggy phase transition" /&gt;&lt;/a&gt;&lt;br /&gt;
Luckily we were able to find it and &lt;a href="http://media.aau.dk/null_space_pursuits/2011/10/to-the-rescue.html"&gt;fix it&lt;/a&gt; very quickly. I have
updated the old entries I wrote on the &lt;span class="caps"&gt;OMP&lt;/span&gt; optimizations, so they no
longer include the bug. But I take this opportunity to explain what
exactly went&amp;nbsp;wrong.&lt;/p&gt;
&lt;p&gt;A key part of the optimization was that slicing out arbitrary columns
out of an array is slow when they are passed to &lt;span class="caps"&gt;BLAS&lt;/span&gt; functions like
matrix multiplication. In order to make the most out of your code, the
data should have a contiguous layout. We achieved this by swapping
active dictionary atoms (columns) to the beginning of the&amp;nbsp;array.&lt;/p&gt;
&lt;p&gt;Something that can happen, but won&amp;#8217;t happen very often, is that after an
atom is selected as active, the atom that takes its place after swapping
needs to be selected. This is rare because dictionaries have many
columns, out of which only very very few will be active. But when it
happens, because the code didn&amp;#8217;t keep track of swapped indices, the
corresponding coefficient of the solution would get updated twice,
leading to more zero entries than we should have. A keen eye could have
noticed that the first `n_nonzero_coefs` entries in &lt;span class="caps"&gt;OMP&lt;/span&gt; solution
vectors were never non-zero. But alas, my eye was not a keen one at&amp;nbsp;all.&lt;/p&gt;
&lt;p&gt;In other words, the following test (that was written after the bug was
found, unfortunately) was failing:&lt;br /&gt;
[sourcecode lang=&amp;#8221;Python&amp;#8221;]&lt;br /&gt;
def test_swapped_regressors():&lt;br /&gt;
gamma = np.zeros(n_features)&lt;br /&gt;
# X[:, 21] should be selected first, then X[:, 0] selected second,&lt;br /&gt;
# which will take X[:, 21]&amp;#8217;s place in case the algorithm does&lt;br /&gt;
# column swapping for optimization (which is the case at the moment)&lt;br /&gt;
gamma[21] = 1.0&lt;br /&gt;
gamma[0] = 0.5&lt;br /&gt;
new_y = np.dot(X, gamma)&lt;br /&gt;
new_Xy = np.dot(X.T, new_y)&lt;br /&gt;
gamma_hat = orthogonal_mp(X, new_y, 2)&lt;br /&gt;
gamma_hat_gram = orthogonal_mp_gram(G, new_Xy, 2)&lt;br /&gt;
# active indices should be [0, 21], but prior to the bugfix&lt;br /&gt;
# the algorithm would update only [21] but twice&lt;br /&gt;
assert_equal(np.flatnonzero(gamma_hat), [0, 21])&lt;br /&gt;
assert_equal(np.flatnonzero(gamma_hat_gram), [0, 21])&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Note that this bug has been fixed for a while, but I didn&amp;#8217;t get the free
time to write this post until now. Good news is: we fixed it, and did so
very quickly after the report. So you can still trust me, I&amp;nbsp;guess!&lt;/p&gt;</content><category term="scikit-learn"/><category term="bug"/><category term="omp"/><category term="orthogonal matching pursuit"/><category term="dictionary learning"/><category term="scikit-learn"/></entry><entry><title>Dictionary learning in scikit-learn 0.9</title><link href="//vene.ro/blog/dictionary-learning-in-scikit-learn-0-9.html" rel="alternate"/><published>2011-09-19T19:15:00+02:00</published><updated>2011-09-19T19:15:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-09-19:/blog/dictionary-learning-in-scikit-learn-0-9.html</id><summary type="html">&lt;p&gt;Thanks to Olivier, Gaël and Alex, who reviewed the code heavily the last
couple of days, and with apologies for my lack of activity during a
sequence of conferences, Dictionary learning has officially been merged
into scikit-learn master, and just in time for the new scikit-learn 0.9
release. Here …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Thanks to Olivier, Gaël and Alex, who reviewed the code heavily the last
couple of days, and with apologies for my lack of activity during a
sequence of conferences, Dictionary learning has officially been merged
into scikit-learn master, and just in time for the new scikit-learn 0.9
release. Here are some glimpses of the examples you can run for&amp;nbsp;yourself:&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Dictionary learned from Lena patches" src="http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_1.png" title="plot_image_denoising_1" /&gt;][]&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Noisy image for denoising" src="http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_24.png" title="plot_image_denoising_24" /&gt;][]&lt;/p&gt;
&lt;p&gt;[![Image denoising with Dictionary learning and Orthogonal matching&amp;nbsp;pursuit][]][]&lt;/p&gt;
&lt;p&gt;The stars of this new release are: the manifold learning module by Jake
Vanderplas and Fabian Pedregosa, the Dirichlet process gaussian mixture
model by Alexandre Passos, and many others, as you can see from the
&lt;a href="http://scikit-learn.sourceforge.net/dev/whats_new.html" title="scikit-learn development changelog"&gt;development changelog&lt;/a&gt; (as soon as the release is made, I will update
this post with permanent&amp;nbsp;links).&lt;/p&gt;
&lt;p&gt;The release is due tomorrow. I will also be in charge with building the
Windows installers for this release, let&amp;#8217;s hope I do a good job and you
can think of me and smile when&amp;nbsp;installing!&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Dictionary learned from Lena patches" src="http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_1.png" title="plot_image_denoising_1" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_1.png&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Noisy image for denoising" src="http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_24.png" title="plot_image_denoising_24" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_24.png&lt;/p&gt;
&lt;p&gt;[![Image denoising with Dictionary learning and Orthogonal matching
  pursuit][]]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/09/plot_image_denoising_44.png&lt;/p&gt;</content><category term="scikit-learn"/><category term="dictionary learning"/><category term="scikit-learn"/></entry><entry><title>Optimizing Orthogonal Matching Pursuit code in Numpy, part 2</title><link href="//vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html" rel="alternate"/><published>2011-08-11T19:39:00+02:00</published><updated>2011-08-11T19:39:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-08-11:/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html</id><summary type="html">&lt;p&gt;&lt;span class="caps"&gt;EDIT&lt;/span&gt;: There was a bug in the final version of the code presented here.
It is fixed now, for its backstory, check out &lt;a href="http://venefrombucharest.wordpress.com/2011/11/18/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code/" title="The nasty bug crawling in my Orthogonal Matching Pursuit code"&gt;my blog post on it&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we last saw our hero, he was fighting with the dreaded
implementation of least-angle regression, knowing full well that it was …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span class="caps"&gt;EDIT&lt;/span&gt;: There was a bug in the final version of the code presented here.
It is fixed now, for its backstory, check out &lt;a href="http://venefrombucharest.wordpress.com/2011/11/18/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code/" title="The nasty bug crawling in my Orthogonal Matching Pursuit code"&gt;my blog post on it&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When we last saw our hero, he was fighting with the dreaded
implementation of least-angle regression, knowing full well that it was
his destiny to be&amp;nbsp;faster.&lt;/p&gt;
&lt;p&gt;We had come up with a more robust implementation, catching malformed
cases that would have broken the naive implementation, and also it was
orders of magnitude faster than said implementation. However, our
benchmark [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;] showed that it was a couple of times slower than
least-angle&amp;nbsp;regression.&lt;/p&gt;
&lt;p&gt;By poking around&amp;nbsp;the &lt;code&gt;scikits.learn&lt;/code&gt; codebase, I noticed that there is a
triangular system solver&amp;nbsp;in &lt;code&gt;scikits.learn.utils.arrayfuncs&lt;/code&gt;. Unlike&amp;nbsp;the
&lt;code&gt;scipy.linalg&lt;/code&gt; one, this one only works with lower triangular arrays,
and it forcefully&amp;nbsp;overwrites &lt;code&gt;b&lt;/code&gt;. Even though if weren&amp;#8217;t faster, it
should still be&amp;nbsp;used: &lt;code&gt;scikits.learn&lt;/code&gt; aims to be as backwards-compatible
with SciPy as possible,&amp;nbsp;and &lt;code&gt;linalg.solve_triangular&lt;/code&gt; was added in
0.9.0. Anyway, let&amp;#8217;s just see whether it&amp;#8217;s&amp;nbsp;faster:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In &lt;a href="#footnote-1"&gt;1&lt;/a&gt;: import numpy as&amp;nbsp;np&lt;/p&gt;
&lt;p&gt;In &lt;a href="#footnote-2"&gt;2&lt;/a&gt;: from scipy import&amp;nbsp;linalg&lt;/p&gt;
&lt;p&gt;In [3]: from scikits.learn.datasets import&amp;nbsp;make_spd_matrix&lt;/p&gt;
&lt;p&gt;In [4]: from scikits.learn.utils.arrayfuncs import&amp;nbsp;solve_triangular&lt;/p&gt;
&lt;p&gt;In [5]: G =&amp;nbsp;make_spd_matrix(1000)&lt;/p&gt;
&lt;p&gt;In [6]: L = linalg.cholesky(G,&amp;nbsp;lower=True)&lt;/p&gt;
&lt;p&gt;In [7]: x =&amp;nbsp;np.random.randn(1000)&lt;/p&gt;
&lt;p&gt;In [8]: y =&amp;nbsp;x.copy()&lt;/p&gt;
&lt;p&gt;In [9]: timeit solve_triangular(L, x)&lt;br /&gt;
100 loops, best of 3: 3.45 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [10]: timeit linalg.solve_triangular(L, y, lower=True,
overwrite_b=True)&lt;br /&gt;
10 loops, best of 3: 134 ms per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Wow! That&amp;#8217;s 40x faster. We&amp;#8217;re catching two rabbits with one stone here,
let&amp;#8217;s do the change! Notice that we can just copy [latex]
\mathbf{v}[/latex] into the appropriate place in [latex] L[/latex] and
then solve in&amp;nbsp;place.&lt;/p&gt;
&lt;p&gt;But whoops! When solving the [latex] &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;[/latex] system, we take
advantage of&amp;nbsp;the &lt;code&gt;transpose&lt;/code&gt; attribute&amp;nbsp;in &lt;code&gt;linalg.solve_triangular&lt;/code&gt;,
which&amp;nbsp;the &lt;code&gt;scikits.learn&lt;/code&gt; version does not expose. We could think of a
solution, but here&amp;#8217;s a better idea: Shouldn&amp;#8217;t there be some way to
directly solve the entire system in one&amp;nbsp;go?&lt;/p&gt;
&lt;p&gt;Well, there is. It is an &lt;span class="caps"&gt;LAPACK&lt;/span&gt; function by the name&amp;nbsp;of &lt;code&gt;potrs&lt;/code&gt;. If you
are not aware, &lt;span class="caps"&gt;LAPACK&lt;/span&gt; is a Fortran library with solvers for various
types of linear systems and eigenproblems. &lt;span class="caps"&gt;LAPACK&lt;/span&gt; along with &lt;span class="caps"&gt;BLAS&lt;/span&gt; (on
which it is based) pretty much powers all the scientific computation
that happens. &lt;span class="caps"&gt;BLAS&lt;/span&gt; is an &lt;span class="caps"&gt;API&lt;/span&gt; with multiple implementations dating from
1979, while &lt;span class="caps"&gt;LAPACK&lt;/span&gt; dates from 1992. If you ever used Matlab, this is
what was called behind the scenes. SciPy, again, provides a high-level
wrapper around this,&amp;nbsp;the &lt;code&gt;linalg.cho_solve&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;But SciPy also gives us the possibility to import functions directly
from &lt;span class="caps"&gt;LAPACK&lt;/span&gt;, through the use&amp;nbsp;of &lt;code&gt;linalg.lapack.get_lapack_funcs&lt;/code&gt;. Let&amp;#8217;s
see how the low-level &lt;span class="caps"&gt;LAPACK&lt;/span&gt; function compares to the SciPy wrapper, for
our use&amp;nbsp;case:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [11]: x =&amp;nbsp;np.random.randn(1000)&lt;/p&gt;
&lt;p&gt;In [12]: y =&amp;nbsp;x.copy()&lt;/p&gt;
&lt;p&gt;In [13]: timeit linalg.cho_solve((L, True), x)&lt;br /&gt;
1 loops, best of 3: 95.4 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [14]: potrs, = linalg.lapack.get_lapack_funcs((&amp;#8216;potrs&amp;#8217;,),&amp;nbsp;(G,))&lt;/p&gt;
&lt;p&gt;In [15]: potrs&lt;br /&gt;
Out[15]: &amp;lt;fortran&amp;nbsp;object&amp;gt;&lt;/p&gt;
&lt;p&gt;In [16]: timeit potrs(L, y)&lt;br /&gt;
100 loops, best of 3: 9.49 ms per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s 10 times faster! So now we found an obvious way to optimize the&amp;nbsp;code:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
def cholesky_omp(X, y, n_nonzero_coefs, eps=None):&lt;br /&gt;
min_float = np.finfo(X.dtype).eps&lt;br /&gt;
potrs, = get_lapack_funcs((&amp;#8216;potrs&amp;#8217;,), (X,))&lt;br /&gt;
alpha = np.dot(X.T, y)&lt;br /&gt;
residual = y&lt;br /&gt;
n_active = 0&lt;br /&gt;
idx =&amp;nbsp;[]&lt;/p&gt;
&lt;p&gt;max_features = X.shape&lt;a href="#footnote-1"&gt;1&lt;/a&gt; if eps is not None else n_nonzero_coefs&lt;br /&gt;
L = np.empty((max_features, max_features), dtype=X.dtype)&lt;br /&gt;
L[0, 0] =&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;while 1:&lt;br /&gt;
lam = np.abs(np.dot(X.T, residual)).argmax()&lt;br /&gt;
if lam &amp;lt; n_active or alpha[lam] ** 2 &amp;lt; min_float:&lt;br /&gt;
# atom already selected or inner product too small&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
if n_active &amp;gt; 0:&lt;br /&gt;
# Updates the Cholesky decomposition of X&amp;#8217; X&lt;br /&gt;
L[n_active, :n_active] = np.dot(X[:, idx].T, X[:, lam]&lt;br /&gt;
solve_triangular(L[:n_active, :n_active], L[n_active, :n_active])&lt;br /&gt;
d = np.dot(L[n_active, :n_active].T, L[n_active, :n_active])&lt;br /&gt;
if 1 - d &amp;lt;= min_float: # selected atoms are dependent&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
L[n_active, n_active] = np.sqrt(1 - d)&lt;br /&gt;
idx.append(lam)&lt;br /&gt;
# solve &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x = y in two steps:&lt;br /&gt;
gamma, _ = potrs(L[:n_active, :n_active], alpha[idx], lower=True,&lt;br /&gt;
overwrite_b=False)&lt;br /&gt;
residual = y - np.dot(X[:, idx], gamma)&lt;br /&gt;
if eps is not None and np.dot(residual.T, residual) &amp;lt;= eps:&lt;br /&gt;
break&lt;br /&gt;
elif n_active == max_features:&lt;br /&gt;
break&lt;br /&gt;
return gamma, idx&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Woohoo! But we still lag behind. Now that we delegated the trickiest
parts of the code to fast and reliable solvers, it&amp;#8217;s time to use a
profiler and see what the bottleneck is now. Python has excellent tools
for this purpose. What solved the problem in this case&amp;nbsp;was
&lt;code&gt;line_profiler&lt;/code&gt; [&lt;a href="#footnote-2"&gt;2&lt;/a&gt;]. There is a great article by Olivier Grisel here
&lt;a href="#footnote-2"&gt;2&lt;/a&gt; regarding how to use these profilers. I&amp;#8217;m just going to say&amp;nbsp;that
&lt;code&gt;line_profiler&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s output is very helpful, basically printing the time
taken by each line of code next to that&amp;nbsp;line.&lt;/p&gt;
&lt;p&gt;Running the profiler on this code, we found that 58% of the time is
spent on line 14, 20.5% on line 21, and 20.5% on line 32, with the rest
being insignificant&amp;nbsp;(&lt;code&gt;potrs&lt;/code&gt; takes 0.1%!). The code is clearly dominated
by the matrix multiplications. By running some more timings with
IPython, I found that multiplying such column-wise views of the data&amp;nbsp;as
&lt;code&gt;X[:, idx]&lt;/code&gt; is considerably slower then multiplying a contiguous array.
The least-angle regression code&amp;nbsp;in &lt;code&gt;scikits.learn&lt;/code&gt; avoids this by
swapping columns towards the front of the array as they are chosen, so
we can&amp;nbsp;replace &lt;code&gt;X[:, idx]&lt;/code&gt; with &lt;code&gt;X[:, :n_active]&lt;/code&gt;. The nice part is that
if the array is stored in Fortran-contiguous order (ie. column
contiguous order, as opposed to row contiguous order, as in C), swapping
two columns is a very fast operation!. Let&amp;#8217;s see some more&amp;nbsp;benchmarks!&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [17]: X = np.random.randn(5000,&amp;nbsp;5000)&lt;/p&gt;
&lt;p&gt;In [18]: Y = X.copy(&amp;#8216;F&amp;#8217;) #&amp;nbsp;fortran-ordered&lt;/p&gt;
&lt;p&gt;In [19]: a, b = 1000,&amp;nbsp;2500&lt;/p&gt;
&lt;p&gt;In [20]: swap, = linalg.get_blas_funcs((&amp;#8216;swap&amp;#8217;,),&amp;nbsp;(X,))&lt;/p&gt;
&lt;p&gt;In [21]: timeit X[:, a], X[:, b] = swap(X[:, a], X[:, b])&lt;br /&gt;
100 loops, best of 3: 6.29 ms per&amp;nbsp;loop&lt;/p&gt;
&lt;p&gt;In [22]: timeit Y[:, a], Y[:, b] = swap(Y[:, a], Y[:, b])&lt;br /&gt;
10000 loops, best of 3: 111 us per loop&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;We can see that using Fortran-order takes us from the order of
miliseconds to the order of&amp;nbsp;microseconds!&lt;/p&gt;
&lt;p&gt;Side note: I almost fell into the trap of swapping columns the pythonic
way. That doesn&amp;#8217;t work:&lt;br /&gt;
[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
In [23]: X[:, a], X[:, b] = X[:, b], X[:,&amp;nbsp;a]&lt;/p&gt;
&lt;p&gt;In [24]: np.testing.assert_array_equal(X[:, a], X[:,&amp;nbsp;b])&lt;/p&gt;
&lt;p&gt;In [25]:&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;However this trick works great for swapping elements of one-dimensional&amp;nbsp;arrays.&lt;/p&gt;
&lt;p&gt;Another small optimization that we can do: I found that on my system,
it&amp;#8217;s slightly faster to compute the norm using the &lt;span class="caps"&gt;BLAS&lt;/span&gt;&amp;nbsp;function &lt;code&gt;nrm2&lt;/code&gt;.
So by putting all of these together, we end up with the final version of
our&amp;nbsp;code:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;python&amp;#8221;]&lt;br /&gt;
def cholesky_omp(X, y, n_nonzero_coefs, eps=None,
overwrite_X=False):&lt;br /&gt;
if not overwrite_X:&lt;br /&gt;
X = X.copy(&amp;#8216;F&amp;#8217;)&lt;br /&gt;
else: # even if we are allowed to overwrite, still copy it if bad
order&lt;br /&gt;
X =&amp;nbsp;np.asfortranarray(X)&lt;/p&gt;
&lt;p&gt;min_float = np.finfo(X.dtype).eps&lt;br /&gt;
nrm2, swap = linalg.get_blas_funcs((&amp;#8216;nrm2&amp;#8217;, &amp;#8216;swap&amp;#8217;), (X,))&lt;br /&gt;
potrs, = get_lapack_funcs((&amp;#8216;potrs&amp;#8217;,),&amp;nbsp;(X,))&lt;/p&gt;
&lt;p&gt;indices = range(len(Gram)) # keeping track of swapping&lt;br /&gt;
alpha = np.dot(X.T, y)&lt;br /&gt;
residual = y&lt;br /&gt;
n_active =&amp;nbsp;0&lt;/p&gt;
&lt;p&gt;max_features = X.shape&lt;a href="#footnote-1"&gt;1&lt;/a&gt; if eps is not None else n_nonzero_coefs&lt;br /&gt;
L = np.empty((max_features, max_features), dtype=X.dtype)&lt;br /&gt;
L[0, 0] =&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;while True:&lt;br /&gt;
lam = np.abs(np.dot(X.T, residual)).argmax()&lt;br /&gt;
if lam &amp;lt; n_active or alpha[lam] ** 2 &amp;lt; min_float:&lt;br /&gt;
# atom already selected or inner product too small&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
if n_active &amp;gt; 0:&lt;br /&gt;
# Updates the Cholesky decomposition of X&amp;#8217; X&lt;br /&gt;
L[n_active, :n_active] = np.dot(X[:, :n_active].T, X[:, lam])&lt;br /&gt;
solve_triangular(L[:n_active, :n_active], L[n_active, :n_active])&lt;br /&gt;
v = nrm2(L[n_active, :n_active]) ** 2&lt;br /&gt;
if 1 - v &amp;lt;= min_float: # selected atoms are dependent&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
L[n_active, n_active] = np.sqrt(1 - v)&lt;br /&gt;
X.T[n_active], X.T[lam] = swap(X.T[n_active], X.T[lam])&lt;br /&gt;
alpha[n_active], alpha[lam] = alpha[lam], alpha[n_active]&lt;br /&gt;
indices[n_active], indices[lam] = indices[lam], indices[n_active]&lt;br /&gt;
n_active += 1&lt;br /&gt;
# solves &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x = y as a composition of two triangular systems&lt;br /&gt;
gamma, _ = potrs(L[:n_active, :n_active], alpha[:n_active],
lower=True,&lt;br /&gt;&amp;nbsp;overwrite_b=False)&lt;/p&gt;
&lt;p&gt;residual = y - np.dot(X[:, :n_active], gamma)&lt;br /&gt;
if eps is not None and nrm2(residual) ** 2 &amp;lt;= eps:&lt;br /&gt;
break&lt;br /&gt;
elif n_active == max_features:&lt;br /&gt;&amp;nbsp;break&lt;/p&gt;
&lt;p&gt;return gamma, indices[:n_active]&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Now, the benchmark at [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;] indicates victory over least-angle
regression! I hope you have enjoyed this short tour. See you next&amp;nbsp;time!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_plot_omp_lars.py" title="Orthogonal matching pursuit versus least-angle regression"&gt;&lt;span id="footnote-1"&gt;1&lt;/span&gt;&lt;/a&gt;[]&lt;br /&gt;
&lt;a href="http://scikit-learn.sourceforge.net/dev/developers/performance.html#profiling-python-code" title="Profiling Python code"&gt;&lt;span id="footnote-2"&gt;2&lt;/span&gt;&lt;/a&gt;[]&lt;/p&gt;</content><category term="scikit-learn"/><category term="blas"/><category term="efficient"/><category term="lapack"/><category term="numpy"/><category term="omp"/><category term="orthogonal matching pursuit"/><category term="potrs"/><category term="scipy"/><category term="dictionary learning"/><category term="python"/><category term="scikit-learn"/></entry><entry><title>Optimizing Orthogonal Matching Pursuit code in Numpy, part 1</title><link href="//vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1.html" rel="alternate"/><published>2011-08-07T20:50:00+02:00</published><updated>2011-08-07T20:50:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-08-07:/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1.html</id><summary type="html">&lt;p&gt;After intense code optimization work, my implementation of &lt;span class="caps"&gt;OMP&lt;/span&gt; finally
beat least-angle regression! This was the primary issue discussed during
the pull request, so once performance was taken care of, the code was
ready for merge. Orthogonal matching pursuit is now available in
scikits.learn as a sparse linear regression …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After intense code optimization work, my implementation of &lt;span class="caps"&gt;OMP&lt;/span&gt; finally
beat least-angle regression! This was the primary issue discussed during
the pull request, so once performance was taken care of, the code was
ready for merge. Orthogonal matching pursuit is now available in
scikits.learn as a sparse linear regression model. &lt;span class="caps"&gt;OMP&lt;/span&gt; is a key building
block of the dictionary learning code that we are working on&amp;nbsp;merging.&lt;/p&gt;
&lt;p&gt;I will go through the process of developing this particular piece of
code as an example of code refining and iterative improvements, as well
as for the useful notes it will provide on optimizing numerical Python
code. In the first part we will see how the code got from pseudocode
state to a reasonably efficient code with smart memory allocation. In
the next part we will see how to make it blazing fast by leveraging
[&lt;a href="#footnote-1"&gt;1&lt;/a&gt;] lower level &lt;span class="caps"&gt;BLAS&lt;/span&gt; and &lt;span class="caps"&gt;LAPACK&lt;/span&gt; routines, and how to use profiling
to find hot&amp;nbsp;spots.&lt;/p&gt;
&lt;p&gt;As stated before, orthogonal matching pursuit is a greedy algorithm for
finding a sparse solution [latex] \gamma[/latex] to a linear regression
problem [latex] X\gamma = y[/latex]. Mathematically, it approximates
the solution of the optimization&amp;nbsp;problem:&lt;/p&gt;
&lt;p&gt;$$ \text{argmin} {\big|\big|} \gamma {\big|\big|} _0 \text{
subject to }{\big |\big|}y-X\gamma{\big|\big|}_2\^2 \leq
\epsilon $$&lt;br /&gt;
or (under a different parametrization):&lt;br /&gt;
$$\text{argmin} {\big |\big|}y - X\gamma{\big |\big|}_2\^2
\text{ subject to } {\big|\big|}\gamma{\big|\big|}_0 \leq
n_{\text{nonzero&amp;nbsp;coefs}}$$&lt;/p&gt;
&lt;p&gt;In the code samples in this post I will omit the docstrings, but I will
follow the notation in the formulas&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important note:&lt;/strong&gt; The regressors/dictionary atoms (the columns of
[latex] X[/latex]) are assumed to be normalized throughout this post (as
well as usually any discussion of &lt;span class="caps"&gt;OMP&lt;/span&gt;). We also assume the following
imports:&lt;br /&gt;
[sourcecode language=&amp;#8221;Python&amp;#8221;]&lt;br /&gt;
import numpy as np&lt;br /&gt;
from scipy import linalg&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Orthogonal matching pursuit is a very simple algorithm in pseudocode,
and as I stated before, it almost writes itself in Numpy. For this
reason, instead of stating the pseudocode here, I will start with how
naively implemented &lt;span class="caps"&gt;OMP&lt;/span&gt; looks like in&amp;nbsp;Python:&lt;/p&gt;
&lt;p&gt;[sourcecode language=&amp;#8221;Python&amp;#8221;]&lt;br /&gt;
def orthogonal_mp(X, y, n_nonzero_coefs, eps=None):&lt;br /&gt;
residual = y&lt;br /&gt;
idx = []&lt;br /&gt;
if eps == None:&lt;br /&gt;
stopping_condition = lambda: len(idx) == n_nonzero_coefs&lt;br /&gt;
else:&lt;br /&gt;
stopping_condition = lambda: np.inner(residual, residual) &amp;lt;= eps&lt;br /&gt;
while not stopping_condition():&lt;br /&gt;
lam = np.abs(np.dot(residual, X)).argmax()&lt;br /&gt;
idx.append(lam)&lt;br /&gt;
gamma, _, _, _ = linalg.lstsq(X[:, idx], y)&lt;br /&gt;
residual = y - np.dot(X[:, idx], gamma)&lt;br /&gt;
return gamma, idx&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Using lambda expressions as stopping conditions never looked like a
brilliant idea, but it seems to me like the most elegant way to specify
such a variable stopping condition. However, the biggest slowdown in
this is the need for solving a least squares problem at each iteration,
while least-angle regression is known to produce the entire
regularization path for the cost of a single least squares problem. We
will also see that this implementation is more vulnerable to numerical
stability&amp;nbsp;issues.&lt;/p&gt;
&lt;p&gt;In [&lt;a href="#footnote-2"&gt;2&lt;/a&gt;], Rubinstein et al. described the Cholesky-&lt;span class="caps"&gt;OMP&lt;/span&gt; algorithm, an
implementation of &lt;span class="caps"&gt;OMP&lt;/span&gt; that avoids solving a new least squares problem at
each iteration by keeping a Cholesky decomposition [latex] &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;[/latex]
of the Gram matrix [latex] G =
X_{\text{idx}}&amp;#8217;X_{\text{idx}}[/latex]. Because [latex]
X_{\text{idx}}[/latex] grows by exactly one column at each iteration,
[latex] L[/latex] can be updated according to the following rule: Given
[latex] A = \begin{pmatrix} \tilde{A} &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; \mathbf{v}&amp;#8217; &amp;#92; \mathbf{v}
&lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; c \end{pmatrix}[/latex], and knowing the decomposition of [latex]
\tilde{A} = \tilde{L}\tilde{L}&amp;#8217;[/latex], the Cholesky decomposition
[latex] A = &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;[/latex] is given by $$ L = \begin{pmatrix}\tilde{L}
&lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; \mathbf{0} &amp;#92; \mathbf{w}&amp;#8217; &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; \sqrt{c - \mathbf{w}&amp;#8217;\mathbf{w}}
\end{pmatrix}, \text{ where } \tilde{L}\mathbf{w} =&amp;nbsp;\mathbf{v}$$&lt;/p&gt;
&lt;p&gt;Even if you are unfamiliar with the mathematical properties of the
Cholesky decomposition, you can see from the construction detailed above
that [latex] L[/latex] is always going to be a lower triangular matrix
(it will only have null elements above the main diagonal). Actually, the
letter L stands for lower. We have therefore replaced the step where we
needed to solve the least-squares problem [latex]
X_{\text{idx}}\gamma = y[/latex] with two much simpler computations:
solving [latex] \tilde{L}\mathbf{w} = \mathbf{v}[/latex] and solving
[latex] &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;\gamma = X_{\text{idx}}&amp;#8217;y[/latex]. Due to the [latex]
L[/latex]&amp;#8217;s structure, these are much quicker operations than a least
squares projection.&lt;br /&gt;
Here is the initial way I implemented&amp;nbsp;this:&lt;/p&gt;
&lt;p&gt;[sourcecode&amp;nbsp;language=&amp;#8221;Python&amp;#8221;]&lt;/p&gt;
&lt;p&gt;def cholesky_omp(X, y, n_nonzero_coefs, eps=None):&lt;br /&gt;
if eps == None:&lt;br /&gt;
stopping_condition = lambda: it == n_nonzero_coefs&lt;br /&gt;
else:&lt;br /&gt;
stopping_condition = lambda: np.inner(residual, residual) \&amp;lt;=&amp;nbsp;eps&lt;/p&gt;
&lt;p&gt;alpha = np.dot(X.T, y)&lt;br /&gt;
residual = y&lt;br /&gt;
idx = []&lt;br /&gt;
L =&amp;nbsp;np.ones((1,1))&lt;/p&gt;
&lt;p&gt;while not stopping_condition():&lt;br /&gt;
lam = np.abs(np.dot(residual, X)).argmax()&lt;br /&gt;
if len(idx) &amp;gt; 0:&lt;br /&gt;
w = linalg.solve_triangular(L, np.dot(X[:, idx].T, X[:, lam]),&lt;br /&gt;
lower=True)&lt;br /&gt;
L = np.r_[np.c_[L, np.zeros(len(L))],&lt;br /&gt;
np.atleast_2d(np.append(w, np.sqrt(1 - np.dot(w.T, w))))]&lt;br /&gt;
idx.append(lam)&lt;br /&gt;
# solve &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x = y in two steps:&lt;br /&gt;
Ltx = linalg.solve_triangular(L, alpha[idx], lower=True)&lt;br /&gt;
gamma = linalg.solve_triangular(L, Ltx, trans=1, lower=True)&lt;br /&gt;
residual = y - np.dot(X[:, idx],&amp;nbsp;gamma)&lt;/p&gt;
&lt;p&gt;return gamma, idx&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;Note that a lot of the code remained unchanged, this is the same
algorithm as before, only the Cholesky trick is used to improve
performance. According to the plot in [&lt;a href="#footnote-3"&gt;3&lt;/a&gt;], we can see that the naive
implementation has oscillations of the reconstruction error due to
numerical instability, while this Cholesky implementation is&amp;nbsp;well-behaved.&lt;/p&gt;
&lt;p&gt;Along with this I also implemented the Gram-based version of this
algorithm, which only needs [latex] X&amp;#8217;X[/latex] and [latex] X&amp;#8217;y[/latex]
(and [latex] {\big|\big|}y{\big|\big|}_2\^2[/latex], in case the
epsilon-parametrization is desired). This is called &lt;strong&gt;Batch &lt;span class="caps"&gt;OMP&lt;/span&gt;&lt;/strong&gt; in
[&lt;a href="#footnote-2"&gt;2&lt;/a&gt;], because it offers speed gains when many signals need to be
sparse coded against the same dictionary [latex] X[/latex]. A lot of
speed is gained because two large matrix multiplications are avoided at
each iteration, but for many datasets, the cost of the precomputations
dominates the procedure. I will not insist on Gram &lt;span class="caps"&gt;OMP&lt;/span&gt; in this post, it
can be found in&amp;nbsp;the &lt;code&gt;scikits.learn&lt;/code&gt; repository [&lt;a href="#footnote-4"&gt;4&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Now, the problems with this are a bit more subtle. At this point, I
moved on to code other things, since &lt;span class="caps"&gt;OMP&lt;/span&gt; was passing tests and the
signal recovery example was working. The following issues popped up
during&amp;nbsp;review:&lt;/p&gt;
&lt;p&gt;​1. The lambda stopping condition does not pickle.&lt;br /&gt;
2. For well-constructed signals and data matrices, assuming normal
atoms, [latex] \mathbf{w}[/latex] on line 14 will never have norm
greater than or equal to zero, unless the chosen feature happens to be
dependent of the already chosen set. In theory, this cannot happen,
since we do an orthogonal projection at each step. However, if the
matrix [latex] X[/latex] is not well-behaved (for example, if it has two
identical columns, and [latex] y[/latex] is built using non-zero
coefficients for those columns), then we end up with the square root of
a negative value on line 17.&lt;br /&gt;
3. It was orders of magnitude slower than least-angle regression, given
the same number of nonzero&amp;nbsp;coefficients.&lt;/p&gt;
&lt;p&gt;1 was an easy fix. 2 was a bit tricky since it was a little hidden: the
first time I encountered such an error, I wrongfully assumed that given
that the diagonal of [latex] X_\text{idx}&amp;#8217;X_\text{idx}[/latex] was
unit, then [latex] L[/latex] should also have a unit diagonal, so I
passed the&amp;nbsp;parameter &lt;code&gt;unit_diagonal=True&lt;/code&gt; to &lt;code&gt;linalg.solve_triangular&lt;/code&gt;,
and the plethora of NaN&amp;#8217;s along the diagonal were simply ignored. Let
this show what happens when you don&amp;#8217;t pay attention when&amp;nbsp;coding.&lt;/p&gt;
&lt;p&gt;When I realized my mistake, I first did something I saw&amp;nbsp;in &lt;code&gt;lars_path&lt;/code&gt;
from the scikit: take the absolute value of the argument&amp;nbsp;of &lt;code&gt;sqrt&lt;/code&gt;, and
also ensure it is practically larger than zero. However, tests started
failing randomly. Confusion ensued until the nature of the issue,
discussed above, was discovered. It&amp;#8217;s just not right to take&amp;nbsp;the &lt;code&gt;abs&lt;/code&gt;:
if that argument ends up less than zero, &lt;span class="caps"&gt;OMP&lt;/span&gt; simply cannot proceed and
must stop due to malformed data. The reference implementation from the
website of the authors of [&lt;a href="#footnote-2"&gt;2&lt;/a&gt;] includes explicit &lt;em&gt;early stopping&lt;/em&gt;
conditions for this, along with some other&amp;nbsp;cases.&lt;/p&gt;
&lt;p&gt;At the same time, I started to try a couple of optimizations. The most
obvious thing was the way I was building the matrix [latex] L[/latex]
was clearly suboptimal, reallocating it at each&amp;nbsp;iteration.&lt;/p&gt;
&lt;p&gt;This leads to the following&amp;nbsp;code:&lt;/p&gt;
&lt;p&gt;[sourcecode&amp;nbsp;language=&amp;#8221;Python&amp;#8221;]&lt;/p&gt;
&lt;p&gt;def cholesky_omp(X, y, n_nonzero_coefs, eps=None):&lt;br /&gt;
min_float = np.finfo(X.dtype).eps&lt;br /&gt;
alpha = np.dot(X.T, y)&lt;br /&gt;
residual = y&lt;br /&gt;
n_active = 0&lt;br /&gt;
idx =&amp;nbsp;[]&lt;/p&gt;
&lt;p&gt;max_features = X.shape&lt;a href="#footnote-1"&gt;1&lt;/a&gt; if eps is not None else n_nonzero_coefs&lt;br /&gt;
L = np.empty((max_features, max_features), dtype=X.dtype)&lt;br /&gt;
L[0, 0] =&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;while 1:&lt;br /&gt;
lam = np.abs(np.dot(X.T, residual)).argmax()&lt;br /&gt;
if lam \&amp;lt; n_active or alpha[lam] ** 2 &amp;gt; min_float:&lt;br /&gt;
# atom already selected or inner product too small&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
if n_active &amp;gt; 0:&lt;br /&gt;
# Updates the Cholesky decomposition of X&amp;#8217; X&lt;br /&gt;
w = linalg.solve_triangular(L[:n_active, :n_active],&lt;br /&gt;
np.dot(X[:, idx].T, X[:, lam]),&lt;br /&gt;
lower=True)&lt;br /&gt;
L[n_active, :n_active] = w&lt;br /&gt;
d = np.dot(w.T, w)&lt;br /&gt;
if 1 - d &amp;lt;= min_float: # selected atoms are dependent&lt;br /&gt;
warn(&amp;#8220;Stopping early&amp;#8221;)&lt;br /&gt;
break&lt;br /&gt;
L[n_active, n_active] = np.sqrt(1 - d)&lt;br /&gt;
idx.append(lam)&lt;br /&gt;
# solve &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x = y in two steps:&lt;br /&gt;
Ltx = linalg.solve_triangular(L[:n_active, :n_active], alpha[idx],
lower=True)&lt;br /&gt;
gamma = linalg.solve_triangular(L[:n_active, :n_active], Ltx,
trans=1, lower=True)&lt;br /&gt;
residual = y - np.dot(X[:, idx], gamma)&lt;br /&gt;
if eps is not None and np.dot(residual.T, residual) &amp;lt;= eps:&lt;br /&gt;
break&lt;br /&gt;
elif n_active == max_features:&lt;br /&gt;
break&lt;br /&gt;
return gamma, idx&lt;br /&gt;&amp;nbsp;[/sourcecode]&lt;/p&gt;
&lt;p&gt;What should be noted here, apart from the obvious fix for #1, are the
early stopping conditions. It is natural to stop if the same feature
gets picked twice: the residual is always orthogonalized with respect to
the chosen basis, so the only way this could happen is if there would be
no more unused independent regressors. This would either lead to this,
or to the stopping criterion on line 25, depending on which equally
insignificant vector gets picked. The other criterion for early stopping
is if the chosen atom is orthogonal to y, which would make it
uninformative and would again mean that there are no better ones left,
so we might as well quit&amp;nbsp;looking.&lt;/p&gt;
&lt;p&gt;Also, we now make sure that [latex] L[/latex] is preallocated. Note&amp;nbsp;that
&lt;code&gt;np.empty&lt;/code&gt; is marginally faster&amp;nbsp;than &lt;code&gt;np.zeros&lt;/code&gt; because it does not
initialize the array to zero after allocating, so the untouched parts of
the array will contain whatever happened to be in memory before. In our
case, this means only the values above the main diagonal: everything on
and beneath is initialized before access. Luckily,&amp;nbsp;the
&lt;code&gt;linalg.solve_triangular&lt;/code&gt; function ignores what it doesn&amp;#8217;t&amp;nbsp;need.&lt;/p&gt;
&lt;p&gt;This is a robust implementation, but still a couple of times slower than
least-angle regression. In the next part of the article we will see how
we can make it beat &lt;span class="caps"&gt;LARS&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span id="footnote-1"&gt;&lt;a href="#footnote-1"&gt;1&lt;/a&gt;&lt;/span&gt; I always wanted to use this word in a
serious context :P&lt;br /&gt;
&lt;span id="footnote-2"&gt;&lt;a href="#footnote-2"&gt;2&lt;/a&gt;&lt;/span&gt; Rubinstein, R., Zibulevsky, M. and
Elad, M., [Efficient Implementation of the K-&lt;span class="caps"&gt;SVD&lt;/span&gt; Algorithm using Batch
Orthogonal Matching Pursuit][] Technical Report - &lt;span class="caps"&gt;CS&lt;/span&gt; Technion, April
2008.&lt;br /&gt;
&lt;span id="footnote-3"&gt;&lt;a href="#footnote-3"&gt;3&lt;/a&gt;&lt;/span&gt; [First thoughts on Orthogonal
Matching Pursuit][] on this blog.&lt;br /&gt;
&lt;span id="footnote-4"&gt;&lt;a href="#footnote-4"&gt;4&lt;/a&gt;&lt;/span&gt; &lt;a href="https://github.com/scikit-learn/scikit-learn/blob/master/scikits/learn/linear_model/omp.py"&gt;omp.py&lt;/a&gt; on&amp;nbsp;github.&lt;/p&gt;</content><category term="scikit-learn"/><category term="efficient"/><category term="numpy"/><category term="omp"/><category term="orthogonal matching pursuit"/><category term="scipy"/><category term="dictionary learning"/><category term="scikit-learn"/><category term="python"/></entry><entry><title>Progress on Orthogonal Matching Pursuit</title><link href="//vene.ro/blog/progress-on-orthogonal-matching-pursuit.html" rel="alternate"/><published>2011-08-02T16:56:00+02:00</published><updated>2011-08-02T16:56:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-08-02:/blog/progress-on-orthogonal-matching-pursuit.html</id><summary type="html">&lt;p&gt;Since orthogonal matching pursuit (&lt;span class="caps"&gt;OMP&lt;/span&gt;) is an important part of signal
processing and therefore crucial to the image processing aspect of
dictionary learning, I am currently focusing on optimizing the &lt;span class="caps"&gt;OMP&lt;/span&gt; code
and making sure it is stable. &lt;span class="caps"&gt;OMP&lt;/span&gt; is a forward method like least-angle
regression, so it is natural …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Since orthogonal matching pursuit (&lt;span class="caps"&gt;OMP&lt;/span&gt;) is an important part of signal
processing and therefore crucial to the image processing aspect of
dictionary learning, I am currently focusing on optimizing the &lt;span class="caps"&gt;OMP&lt;/span&gt; code
and making sure it is stable. &lt;span class="caps"&gt;OMP&lt;/span&gt; is a forward method like least-angle
regression, so it is natural to bench them against one&amp;nbsp;another.&lt;/p&gt;
&lt;p&gt;This has helped find a couple of bottlenecks. Time has been gained by
preallocating the array to store the Cholesky decomposition. Also, using
the &lt;span class="caps"&gt;LAPACK&lt;/span&gt; &lt;code&gt;potrs&lt;/code&gt; function in order to solve a system of the shape
$latex &lt;span class="caps"&gt;LL&lt;/span&gt;&amp;#8217;x=y$ is faster than&amp;nbsp;using &lt;code&gt;solve_triangular&lt;/code&gt; twice.&lt;/p&gt;
&lt;p&gt;I am still trying to optimize the code. We are working hard to make sure
that scikits.learn contributions are up to standards before&amp;nbsp;merging.&lt;/p&gt;</content><category term="scikit-learn"/><category term="omp"/><category term="orthogonal matching pursuit"/><category term="scikit-learn"/></entry><entry><title>SparsePCA in scikits.learn-git</title><link href="//vene.ro/blog/sparsepca-in-scikits-learn-git.html" rel="alternate"/><published>2011-07-19T12:01:00+02:00</published><updated>2011-07-19T12:01:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-07-19:/blog/sparsepca-in-scikits-learn-git.html</id><summary type="html">&lt;p&gt;I am happy to announce that the Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; code has been reviewed and
merged into the&amp;nbsp;main &lt;code&gt;scikits.learn&lt;/code&gt; repository.&lt;/p&gt;
&lt;p&gt;You can use it if you install the bleeding&amp;nbsp;edge &lt;code&gt;scikits.learn&lt;/code&gt; git
version, by first downloading the source code as explained in the
&lt;a href="http://scikit-learn.sourceforge.net/stable/developers/index.html#git-repo" title="installation user's guide"&gt;user&amp;#8217;s guide&lt;/a&gt;, and then …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am happy to announce that the Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; code has been reviewed and
merged into the&amp;nbsp;main &lt;code&gt;scikits.learn&lt;/code&gt; repository.&lt;/p&gt;
&lt;p&gt;You can use it if you install the bleeding&amp;nbsp;edge &lt;code&gt;scikits.learn&lt;/code&gt; git
version, by first downloading the source code as explained in the
&lt;a href="http://scikit-learn.sourceforge.net/stable/developers/index.html#git-repo" title="installation user's guide"&gt;user&amp;#8217;s guide&lt;/a&gt;, and then&amp;nbsp;running &lt;code&gt;python setup.py install&lt;/code&gt;.&lt;br /&gt;
[caption id=&amp;#8221;&amp;#8221; align=&amp;#8221;aligncenter&amp;#8221; width=&amp;#8221;400&amp;#8221; caption=&amp;#8221;Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; on
images of the digit 3&amp;#8221;][&lt;img alt="" src="http://scikit-learn.sourceforge.net/dev/_images/plot_digits_decomposition_4.png" title="Sparse PCA on images of the digit 3" /&gt;]&lt;a href="http://scikit-learn.sourceforge.net/dev/_images/plot_digits_decomposition_4.png" title="Sparse PCA on images of the digit 3"&gt;&lt;/a&gt;[/caption]&lt;br /&gt;
To see what code is needed to produce an image such as the one above,&amp;nbsp;using &lt;code&gt;scikits.learn&lt;/code&gt;. check out this cool &lt;a href="http://scikit-learn.sourceforge.net/dev/auto_examples/decomposition/plot_digits_decomposition.html" title="decomposition example"&gt;decomposition example&lt;/a&gt;
that compares the results of most matrix decomposition models
implemented at the&amp;nbsp;moment.&lt;/p&gt;
&lt;p&gt;There are other new cool things that have been recently merged by other
contributors, such as support for sparse matrices in &lt;a href="http://scikit-learn.sourceforge.net/dev/modules/clustering.html#mini-batch-k-means" title="minibatch K-means"&gt;minibatch
K-means&lt;/a&gt;, and the &lt;a href="http://scikit-learn.sourceforge.net/dev/modules/mixture.html#infinite-gaussian-mixtures-dpgmm-classifier" title="variational infinite gaussian mixture model"&gt;variational infinite gaussian mixture model&lt;/a&gt;, so
I invite you to take a&amp;nbsp;look!&lt;/p&gt;
&lt;p&gt;[&lt;img alt="" src="http://scikit-learn.sourceforge.net/dev/_images/plot_digits_decomposition_4.png" title="Sparse PCA on images of the digit 3" /&gt;]:&amp;nbsp;http://scikit-learn.sourceforge.net/dev/_images/plot_digits_decomposition_4.png&lt;/p&gt;</content><category term="scikit-learn"/><category term="pca"/><category term="principal components analysis"/><category term="scikit-learn"/><category term="sparse pca"/><category term="SparsePCA"/><category term="spca"/></entry><entry><title>K-Means for dictionary learning</title><link href="//vene.ro/blog/k-means-for-dictionary-learning.html" rel="alternate"/><published>2011-07-10T14:27:00+02:00</published><updated>2011-07-10T14:27:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-07-10:/blog/k-means-for-dictionary-learning.html</id><summary type="html">&lt;p&gt;[![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset with whitening
&lt;span class="caps"&gt;PCA&lt;/span&gt;][]][][![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset without
whitening &lt;span class="caps"&gt;PCA&lt;/span&gt;][]][]&lt;/p&gt;
&lt;p&gt;One of the simplest, and yet most heavily constrained form of matrix
factorization, is vector quantization (&lt;span class="caps"&gt;VQ&lt;/span&gt;). Heavily used in image/video
compression, the &lt;span class="caps"&gt;VQ&lt;/span&gt; problem is a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset with whitening
&lt;span class="caps"&gt;PCA&lt;/span&gt;][]][][![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset without
whitening &lt;span class="caps"&gt;PCA&lt;/span&gt;][]][]&lt;/p&gt;
&lt;p&gt;One of the simplest, and yet most heavily constrained form of matrix
factorization, is vector quantization (&lt;span class="caps"&gt;VQ&lt;/span&gt;). Heavily used in image/video
compression, the &lt;span class="caps"&gt;VQ&lt;/span&gt; problem is a factorization [latex] X=&lt;span class="caps"&gt;WH&lt;/span&gt;[/latex]
where [latex] H[/latex] (our dictionary) is called the codebook and is
designed to cover the cloud of data points effectively, and each line of
[latex] W[/latex] is a unit&amp;nbsp;vector.&lt;/p&gt;
&lt;p&gt;This means that each each data point [latex] x_i[/latex] is
approximated as [latex] x_i \approx h_{k} = \sum_{j=1}\^{r}
\delta_{kj}h_{j}[/latex]. In other words, the closest row vector
(codeword/dictionary atom) [latex] h_k[/latex] of [latex] H[/latex] is
chosen as an approximation, and this is encoded as a unit vector [latex]
(\delta_{k1}, &amp;#8230;, \delta_{kr})[/latex]. The data representation
[latex] W[/latex] is composed of such&amp;nbsp;vectors.&lt;/p&gt;
&lt;p&gt;There is a variation called gain-shape &lt;span class="caps"&gt;VQ&lt;/span&gt; where instead of approximating
each point as one of the codewords, we allow a scalar multiplication
invariance: [latex] x_i \approx \alpha_ih_k[/latex]. This model
requires considerably more storage (each data point needs a floating
point number and an unsigned index, as opposed to just the index), but
it leads to a much better approximation.&lt;br /&gt;
Gain-shape &lt;span class="caps"&gt;VQ&lt;/span&gt; can equivalently be accomplished by normalizing each data
vector prior to fitting the&amp;nbsp;codebook.&lt;/p&gt;
&lt;p&gt;In order to fit a codebook [latex] H[/latex] for efficient &lt;span class="caps"&gt;VQ&lt;/span&gt; use, the
K-Means Clustering [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;] algorithm is a natural thought. K-means is an
iterative algorithm that incrementally improves the dispersion of k
cluster centers in the data space until convergence. The cluster centers
are initialized in a random or procedural fashion, then, at each
iteration, the data points are assigned to the closest cluster center,
which is subsequently moved to the center of the points assigned to&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;scikits.learn.decomposition.KMeansCoder&lt;/code&gt; object from our
work-in-progress dictionary learning toolkit can learn a dictionary from
image patches using the K-Means algorithm, with optional local contrast
normalization and a &lt;span class="caps"&gt;PCA&lt;/span&gt; whitening transform. Using a trained object to
transform data points with orthogonal matching pursuit, with the&amp;nbsp;parameter &lt;code&gt;n_atoms=1&lt;/code&gt; is equivalent to gain-shape &lt;span class="caps"&gt;VQ&lt;/span&gt;. Of course you are
free to use any method of sparse coding such as &lt;span class="caps"&gt;LARS&lt;/span&gt;. The code used to
produce the example images on top of this post can be found in [&lt;a href="#footnote-2"&gt;2&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Using K-Means for learning the dictionary does not optimize over linear
combinations of dictionary atoms, like standard dictionary learning
methods do. However, it&amp;#8217;s considerably faster, and Adam Coates and
Andrew Ng suggest in [&lt;a href="#footnote-3"&gt;3&lt;/a&gt;] that as long as the dictionary is filled
with a large enough number of atoms and it covers well enough the cloud
of data (and of future test data) points, then K-Means, or even random
sampling of image patches, can perform remarkably well for some&amp;nbsp;tasks.&lt;/p&gt;
&lt;div id="footnote-1"&gt;
[1] [Wikipedia article on K-Means clustering][]

&lt;/div&gt;
&lt;div id="footnote-2"&gt;
[2] [K-Means Coder example][]

&lt;/div&gt;
&lt;div id="footnote-3"&gt;
[3] [**The importance of encoding versus training with sparse coding and
vector quantization**, Adam Coates and Andrew Y. Ng. In Proceedings of
the Twenty-Eighth International Conference on Machine Learning, 2011.][]

&lt;/div&gt;

&lt;p&gt;[![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset with whitening
  &lt;span class="caps"&gt;PCA&lt;/span&gt;][]]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/07/kmeans_w.png&lt;/p&gt;
&lt;p&gt;[![Dictionary learned with K-Means on the &lt;span class="caps"&gt;LFW&lt;/span&gt; dataset without
  whitening &lt;span class="caps"&gt;PCA&lt;/span&gt;][]]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/07/kmeans_no_w.png&lt;/p&gt;</content><category term="scikit-learn"/><category term="dictionary learning"/><category term="k-means"/><category term="scikit-learn"/><category term="vq"/><category term="Uncategorized"/></entry><entry><title>Image denoising with dictionary learning</title><link href="//vene.ro/blog/image-denoising-with-dictionary-learning.html" rel="alternate"/><published>2011-07-07T20:00:00+02:00</published><updated>2011-07-07T20:00:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-07-07:/blog/image-denoising-with-dictionary-learning.html</id><summary type="html">&lt;p&gt;I am presenting an image denoising example that fully runs under my
local scikits-learn fork. Coming soon near&amp;nbsp;you!&lt;/p&gt;
&lt;p&gt;The 400 square pixels area covering Lena&amp;#8217;s face was distorted by
additive gaussian noise with a standard deviation of 50 (pixel values
are ranged&amp;nbsp;0-256.)&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Lena image denoising using dictionary learning" src="http://localhost:8001/wp-content/uploads/2011/07/denoise3.png" title="Lena denoising" /&gt;][]&lt;/p&gt;
&lt;p&gt;The dictionary contains 100 atoms …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am presenting an image denoising example that fully runs under my
local scikits-learn fork. Coming soon near&amp;nbsp;you!&lt;/p&gt;
&lt;p&gt;The 400 square pixels area covering Lena&amp;#8217;s face was distorted by
additive gaussian noise with a standard deviation of 50 (pixel values
are ranged&amp;nbsp;0-256.)&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Lena image denoising using dictionary learning" src="http://localhost:8001/wp-content/uploads/2011/07/denoise3.png" title="Lena denoising" /&gt;][]&lt;/p&gt;
&lt;p&gt;The dictionary contains 100 atoms of shape 4x4 and was trained using
10000 random patches extracted from the undistorted image. Then, each
one of the four 100 square pixel areas was reconstructed using the
dictionary learning model and a different transform&amp;nbsp;method.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="caps"&gt;OMP&lt;/span&gt;-1 reconstructs each patch as the closest dictionary atom,
    multiplied by a variable coefficient. This is similar to the idea of
    gain-shape vector&amp;nbsp;quantization.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;OMP&lt;/span&gt;-2 is like &lt;span class="caps"&gt;OMP&lt;/span&gt;-1, but it considers 2 atoms instead of just one.
    This takes advantage of the fact that the natural dictionary atoms
    are of such nature to efficiently represent random image patches
    when&amp;nbsp;combined.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;LARS&lt;/span&gt; finds a reconstruction of each image patch as a solution to a
    Lasso problem, solved using least angle&amp;nbsp;regression.&lt;/li&gt;
&lt;li&gt;Thresholding is a simple and quick non-linearity that (as it is
    currently implemented, based on [&lt;a href="#footnote-1"&gt;1&lt;/a&gt;], where it is not intended
    for reconstruction but for classification) breaks the local
    brightness of the image fragment. The bottom right fragment was
    forcefully renormalized to stretch fit into the 0-256 range, but
    brightness differences can be&amp;nbsp;seen.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id="footnote-1"&gt;
[1] [**The importance of encoding versus training with sparse coding and
vector quantization**, Adam Coates and Andrew Y. Ng. In Proceedings of
the Twenty-Eighth International Conference on Machine Learning, 2011.][]

&lt;/div&gt;

&lt;p&gt;[&lt;img alt="Lena image denoising using dictionary learning" src="http://localhost:8001/wp-content/uploads/2011/07/denoise3.png" title="Lena denoising" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/07/denoise3.png&lt;/p&gt;</content><category term="scikit-learn"/><category term="denoising"/><category term="dictionary learning"/><category term="scikit-learn"/></entry><entry><title>Summer of Code roadmap, part 1</title><link href="//vene.ro/blog/summer-of-code-roadmap-part-1.html" rel="alternate"/><published>2011-06-12T14:28:00+02:00</published><updated>2011-06-12T14:28:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-06-12:/blog/summer-of-code-roadmap-part-1.html</id><summary type="html">&lt;p&gt;After a little busy while, I have graduated and entered the summer
vacation, which means time for serious GSoC&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Me on graduation day" src="http://localhost:8001/wp-content/uploads/2011/06/p1080283.jpg" title="Graduation day" /&gt;][]&lt;/p&gt;
&lt;p&gt;So we had a little conference in order to discuss what will be done and
when. We gathered quite a few code snippets since the official start of
the project …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After a little busy while, I have graduated and entered the summer
vacation, which means time for serious GSoC&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Me on graduation day" src="http://localhost:8001/wp-content/uploads/2011/06/p1080283.jpg" title="Graduation day" /&gt;][]&lt;/p&gt;
&lt;p&gt;So we had a little conference in order to discuss what will be done and
when. We gathered quite a few code snippets since the official start of
the project, but it&amp;#8217;s now time to talk about integration and pull&amp;nbsp;requests.&lt;/p&gt;
&lt;p&gt;Here is the&amp;nbsp;plan:&lt;/p&gt;
&lt;h4 id="sparsepca"&gt;SparsePCA&lt;a class="headerlink" href="#sparsepca" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;First pull request due: &lt;strong&gt;June 15&lt;/strong&gt;&lt;br /&gt;
This will be the use case I blogged about &lt;a href="http://venefrombucharest.wordpress.com/2011/05/23/sparse-pca/" title="Sparse PCA"&gt;before&lt;/a&gt;. Specifically, we
want to learn a dictionary of sparse atoms, but representations of the
data will be&amp;nbsp;dense.&lt;/p&gt;
&lt;h4 id="sparsecoder"&gt;SparseCoder&lt;a class="headerlink" href="#sparsecoder" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;First pull request due: &lt;strong&gt;June 25&lt;/strong&gt;&lt;br /&gt;
This is the transpose of the SparsePCA problem. We are learning the
optimal, dense dictionary for sparse representations of the&amp;nbsp;data.&lt;/p&gt;
&lt;h4 id="kmeanscoder"&gt;KMeansCoder&lt;a class="headerlink" href="#kmeanscoder" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;First pull request due: &lt;strong&gt;June 30&lt;/strong&gt;&lt;br /&gt;
This method builds the dictionary out of cluster centers found by&amp;nbsp;K-means.&lt;/p&gt;
&lt;h4 id="onlinesparsecoder"&gt;OnlineSparseCoder&lt;a class="headerlink" href="#onlinesparsecoder" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;First pull request due: &lt;strong&gt;July 10&lt;/strong&gt;&lt;br /&gt;
This will involve the online learning tricks suggested in Julien
Mairal&amp;#8217;s work and will allow for faster computations of both sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;
and sparse coding. In the case of sparse coding, it will make use of the
scikits.learn &lt;span class="caps"&gt;API&lt;/span&gt; for online&amp;nbsp;learning.&lt;/p&gt;
&lt;p&gt;While I will try to keep the deadlines for the initial pull requests as
strictly as I can, we did not establish deadlines for merging, since
this will depend on more factors. As long as the pull requests are up,
the code review system will push it forward towards the merge. The focus
is on teamwork and on feedback cycles as short as possible, as opposed
to falling into the trap of delaying work until the night before the&amp;nbsp;deadline.&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Me on graduation day" src="http://localhost:8001/wp-content/uploads/2011/06/p1080283.jpg" title="Graduation day" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/06/p1080283.jpg&lt;/p&gt;</content><category term="scikit-learn"/><category term="gsoc"/><category term="Uncategorized"/></entry><entry><title>First thoughts on Orthogonal Matching Pursuit</title><link href="//vene.ro/blog/first-thoughts-on-orthogonal-matching-pursuit.html" rel="alternate"/><published>2011-05-30T13:02:00+02:00</published><updated>2011-05-30T13:02:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-05-30:/blog/first-thoughts-on-orthogonal-matching-pursuit.html</id><summary type="html">&lt;p&gt;I am working on implementing the Orthogonal Matching Pursuit (&lt;span class="caps"&gt;OMP&lt;/span&gt;)
algorithm for the scikit. It is an elegant algorithm (that almost writes
itself in Numpy!) to compute a greedy approximation to the solution of a
sparse coding&amp;nbsp;problem:&lt;/p&gt;
&lt;p&gt;$$ \text{argmin} \big|\big|\gamma\big|\big|_0 \text{ subject
to }\big …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am working on implementing the Orthogonal Matching Pursuit (&lt;span class="caps"&gt;OMP&lt;/span&gt;)
algorithm for the scikit. It is an elegant algorithm (that almost writes
itself in Numpy!) to compute a greedy approximation to the solution of a
sparse coding&amp;nbsp;problem:&lt;/p&gt;
&lt;p&gt;$$ \text{argmin} \big|\big|\gamma\big|\big|_0 \text{ subject
to }\big|\big|x-D\gamma\big|\big|_2\^2 \leq&amp;nbsp;\epsilon$$&lt;/p&gt;
&lt;p&gt;or (in a different&amp;nbsp;parametrization)&lt;/p&gt;
&lt;p&gt;$$ \text{argmin} \big|\big|x - D\gamma\big|\big|_2\^2\text{
subject to }\big|\big|\gamma\big|\big|_0 \leq&amp;nbsp;m$$&lt;/p&gt;
&lt;p&gt;The second formulation is interesting in that it gives one of the few
algorithms for sparse coding that can control the actual number of
non-zero entries in the solution. Some dictionary learning methods need
this (I&amp;#8217;m thinking of K-&lt;span class="caps"&gt;SVD&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Both problems are solved by the same algorithm, with a different
stopping condition. The gist of it is to include at each iteration, the
atom with the highest correlation to the current residual. However, as
opposed to regular Matching Pursuit, here, after choosing the atom, the
input signal is orthogonally projected to the space spanned by the
chosen atoms. This involves the solution of a least squares problem at
each step. However, because the problem is almost the same at each
iteration, only with one more column added to the matrix, this can be
easily solved by maintaining a &lt;span class="caps"&gt;QR&lt;/span&gt; or Cholesky decomposition of the
dictionary matrix that is updated at each&amp;nbsp;step.&lt;/p&gt;
&lt;p&gt;Rubinstein et al. [1] came up with a clever method to optimize the
calculations, based on the fact that usually in practice we never have
to find a sparse coding for a single signal, but usually for a batch.
They called this method Batch &lt;span class="caps"&gt;OMP&lt;/span&gt;, and it is based on a straightforward
modification of the Cholesky update algorithm, taking advantage of
precomputing the Gram matrix [latex]&amp;nbsp;G=D&amp;#8217;D[/latex].&lt;/p&gt;
&lt;p&gt;Based on my experiments, their batch update is the fastest, even though
it lags behind if invoked with too small a batch. As soon as I make sure
the implementation is robust and ready for use, I will make some&amp;nbsp;benchmarks.&lt;/p&gt;
&lt;p&gt;Update: Here&amp;#8217;s a little proof that it works!&lt;br /&gt;
[&lt;img alt="Stem plot for sparse signals recovered by OMP" src="http://localhost:8001/wp-content/uploads/2011/06/omp.png" title="Orthogonal Matching Pursuit sparse signal recovery" /&gt;][]&lt;/p&gt;
&lt;p&gt;Update 2: Here&amp;#8217;s a little benchmark:&lt;br /&gt;
[&lt;img alt="Orthogonal Matching Pursuit benchmark" src="http://localhost:8001/wp-content/uploads/2011/06/omp_bench.png" title="OMP benchmark, time and error" /&gt;][]&lt;br /&gt;
[1]
http://www.cs.technion.ac.il/\~ronrubin/Publications/&lt;span class="caps"&gt;KSVD&lt;/span&gt;-&lt;span class="caps"&gt;OMP&lt;/span&gt;-v2.pdf&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Stem plot for sparse signals recovered by OMP" src="http://localhost:8001/wp-content/uploads/2011/06/omp.png" title="Orthogonal Matching Pursuit sparse signal recovery" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/06/omp.png&lt;/p&gt;
&lt;p&gt;[&lt;img alt="Orthogonal Matching Pursuit benchmark" src="http://localhost:8001/wp-content/uploads/2011/06/omp_bench.png" title="OMP benchmark, time and error" /&gt;]:&amp;nbsp;http://localhost:8001/wp-content/uploads/2011/06/omp_bench.png&lt;/p&gt;</content><category term="scikit-learn"/><category term="Uncategorized"/><category term="dictionary learning"/><category term="omp"/><category term="orthogonal matching pursuit"/></entry><entry><title>Sparse PCA</title><link href="//vene.ro/blog/sparse-pca.html" rel="alternate"/><published>2011-05-23T15:19:00+02:00</published><updated>2011-05-23T15:19:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-05-23:/blog/sparse-pca.html</id><summary type="html">&lt;p&gt;I have been working on the integration into the scikits.learn codebase
of a sparse principal components analysis (SparsePCA) algorithm coded by
Gaël and Alexandre and based on [[1]][]. Because the name &amp;#8220;sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;
has some inherent ambiguity, I will describe in greater depth what
problem we are actually solving …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have been working on the integration into the scikits.learn codebase
of a sparse principal components analysis (SparsePCA) algorithm coded by
Gaël and Alexandre and based on [[1]][]. Because the name &amp;#8220;sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;#8221;
has some inherent ambiguity, I will describe in greater depth what
problem we are actually solving, and what it can be used&amp;nbsp;for.&lt;/p&gt;
&lt;h1 id="the-problem"&gt;The problem&lt;a class="headerlink" href="#the-problem" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Mathematically, this implementation of Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;nbsp;solves:&lt;/p&gt;
&lt;p&gt;$latex (U\^*,
V\^*)=\underset{U,V}{\mathrm{argmin\,}}\frac{1}{2}||X-&lt;span class="caps"&gt;UV&lt;/span&gt;||_2\^2+\alpha||V||_1$&lt;/p&gt;
&lt;p&gt;with $latex || U_k ||_2 = 1$ for all $latex 0 \leq k \&amp;lt;&amp;nbsp;n_{atoms}$&lt;/p&gt;
&lt;p&gt;This looks really abstract so let&amp;#8217;s try to interpret it. We are looking
for a matrix factorization $latex &lt;span class="caps"&gt;UV&lt;/span&gt;$ of $latex X \in
\mathbf{R}\^{n_{samples}\times n_{features}}$, just like in
ordinary &lt;span class="caps"&gt;PCA&lt;/span&gt;. The interpretation is that the $latex n_{atoms}$ lines
of $latex V$ are the extracted components, while the lines of $latex
U$ are the coordinates of the samples in this&amp;nbsp;projection.&lt;/p&gt;
&lt;p&gt;The most important difference between this and &lt;span class="caps"&gt;PCA&lt;/span&gt; is that we enforce
sparsity on the &lt;em&gt;components&lt;/em&gt;. In other words, we look for a
representation of the data as a linear combination of sparse&amp;nbsp;signals.&lt;/p&gt;
&lt;p&gt;Another difference is that, unlike in &lt;span class="caps"&gt;PCA&lt;/span&gt;, here we don&amp;#8217;t constrain U to
be orthogonal, just to consist of normalized column vectors. There are
different approaches where this constraint appears too, and they are on
the list for this summer, but I&amp;nbsp;digress.&lt;/p&gt;
&lt;h1 id="the-approach"&gt;The approach&lt;a class="headerlink" href="#the-approach" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;As usual, such optimization problems are solved by alternatively
minimizing one of the variables while keeping the other fixed, until
convergence is&amp;nbsp;reached.&lt;/p&gt;
&lt;p&gt;The update of $latex V$ (the dictionary) is computed as the solution
of a Lasso least squares problem.  We allow the user to choose between
the least angle regression method (&lt;span class="caps"&gt;LARS&lt;/span&gt;) or stochastic gradient descent
as algorithms to solve the Lasso&amp;nbsp;problem.&lt;/p&gt;
&lt;p&gt;The update of $latex U$ is block coordinate descent with warm restart.
This is a batch adaptation of an online algorithm proposed by Mairal et
al. in&amp;nbsp;[[1]][].&lt;/p&gt;
&lt;h1 id="sparse-pca-as-a-transformer"&gt;Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; as a transformer&lt;a class="headerlink" href="#sparse-pca-as-a-transformer" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Of course, in order to be of practical use, the code needs to be
refactored into a scikits.learn transformer object, just&amp;nbsp;like
&lt;code&gt;scikits.learn.decomposition.pca&lt;/code&gt;. This means that the optimization
problem described above corresponds to the fitting stage. The post-fit
state of the transformer is given by the learned components (the matrix
$latex V$&amp;nbsp;above).&lt;/p&gt;
&lt;p&gt;In order to transform new data according to the learned sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; model
(for example, prior to classification of the test data), we simply need
to do a least squares projection of the new data on the sparse&amp;nbsp;components.&lt;/p&gt;
&lt;h1 id="what-is-it-good-for"&gt;What is it good for?&lt;a class="headerlink" href="#what-is-it-good-for" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;For applications such as text and image processing, its great advantage
is interpretability. When running a regular &lt;span class="caps"&gt;PCA&lt;/span&gt; on a set of documents in
bag of words format, we can find an interesting visualisation on a
couple of components, and it can show discrimination or clusters. The
biggest problem is that the maximum variance components found by &lt;span class="caps"&gt;PCA&lt;/span&gt;
have very dense expressions as linear combinations of the initial
features. In practice, sometimes interpretation is made by simply
marking the $latex k$ variables with the highest coefficients in this
representation, and basically interpreting as if the rest are truncated
to 0 (this has been taught to me in a class on &lt;span class="caps"&gt;PCA&lt;/span&gt;&amp;nbsp;interpretation).&lt;/p&gt;
&lt;p&gt;Such an approximation can be highly misleading, and now we offer you the
sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; code that can extract components with only few non-zero
coefficients, and therefore easy to&amp;nbsp;interpret.&lt;/p&gt;
&lt;p&gt;For image data, sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; should extract local components such as,
famously, parts of the face in the case of face&amp;nbsp;recognition.&lt;/p&gt;
&lt;p&gt;Personally I can&amp;#8217;t wait to have it ready for the scikit so that I can
play with it in some of my projects. I have two tasks where I can&amp;#8217;t wait
to see the results: one is related to &lt;a href="http://venefrombucharest.wordpress.com/2011/04/14/a-look-at-romanian-verbs-with-scikits-learn/" title="A look at Romanian verbs with scikits-learn"&gt;Romanian infinitives&lt;/a&gt;, where
&lt;span class="caps"&gt;PCA&lt;/span&gt; revealed structure, and I would love to see how it looks with sparse
n-gram components. The other task is to plug it in as feature extractor
for handwritten digit classification, for my undergraduate&amp;nbsp;thesis.&lt;/p&gt;
&lt;p&gt;&lt;span id="footnote_1"&gt;[1] &lt;a href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf"&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;[[1]]:&amp;nbsp;#footnote_1&lt;/p&gt;</content><category term="scikit-learn"/><category term="dictionary learning"/><category term="pca"/><category term="sparse pca"/><category term="SparsePCA"/><category term="spca"/><category term="scikit-learn"/></entry><entry><title>Customizing scikits.learn for a specific text analysis task</title><link href="//vene.ro/blog/customizing-scikits-learn-for-a-specific-text-analysis-task.html" rel="alternate"/><published>2011-04-29T14:33:00+02:00</published><updated>2011-04-29T14:33:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-29:/blog/customizing-scikits-learn-for-a-specific-text-analysis-task.html</id><summary type="html">&lt;p&gt;Scikits.learn is a great general library, but machine learning has so
many different application, that it is often very helpful to be able to
extend its &lt;span class="caps"&gt;API&lt;/span&gt; to better integrate with your code. With scikits.learn,
this is extremely easy to do using inheritance and using the pipeline&amp;nbsp;module …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Scikits.learn is a great general library, but machine learning has so
many different application, that it is often very helpful to be able to
extend its &lt;span class="caps"&gt;API&lt;/span&gt; to better integrate with your code. With scikits.learn,
this is extremely easy to do using inheritance and using the pipeline&amp;nbsp;module.&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The problem&lt;a class="headerlink" href="#the-problem" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;While continuing the &lt;a href="http://venefrombucharest.wordpress.com/2011/04/14/a-look-at-romanian-verbs-with-scikits-learn/" title="A look at Romanian verbs with scikits-learn"&gt;morphophonetic analysis of Romanian verbal
forms&lt;/a&gt;, I found the need to streamline my workflow to allow for more
complex models. There were a lot of free model parameters and it would
have been painful to interactively tweak everything in order to find a
good&amp;nbsp;combination&lt;/p&gt;
&lt;p&gt;In my case, I needed to read a file containing infinitives and labels
corresponding to conjugation groups, and run a linear support vector
classifier on this data. The &lt;span class="caps"&gt;SVC&lt;/span&gt; has its C parameter that needs to be
tweaked, but I also had some ideas that arose from the images in my old
post. There, I compared the way the data looked when represented as
differently sized n-gram features. Furthermore, I compared the count
features (ie. features indicating the number of times an n-gram occurs
in a string) with binary features (ie. indicating only whether the
n-gram occurs in the string or not). It looked to me like, for such a
low-level text analysis task, using counts only adds&amp;nbsp;noise.&lt;/p&gt;
&lt;p&gt;For this reason,&amp;nbsp;the &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; was not
enough for me. It only returns count features. There was also another
thing that needed to be adjusted: by default, its analyzer uses a
preprocessor that strips accented characters, and I had strong reasons
to believe that Romanian diacritics are very relevant for the learning
task. So, I needed to extend the&amp;nbsp;vectorizer.&lt;/p&gt;
&lt;h2 id="the-solution"&gt;The solution&lt;a class="headerlink" href="#the-solution" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The code I came up with is &lt;a href="https://github.com/vene/misc-nlp/blob/master/conjugation/grid_search_example/preprocess.py"&gt;here&lt;/a&gt;. I tried to build a class that would
be as specific to my needs as possible. It is important to retain the
full &lt;span class="caps"&gt;API&lt;/span&gt;, however. Note&amp;nbsp;the &lt;code&gt;y=None&lt;/code&gt; parameter in the fit functions. Its
necessity will become clear in a&amp;nbsp;moment.&lt;/p&gt;
&lt;p&gt;Another tricky part was exposing&amp;nbsp;the &lt;code&gt;max_n&lt;/code&gt; parameter from the inner
analyzer. This was not really natural, but it simplified the
constructions later&amp;nbsp;on.&lt;/p&gt;
&lt;p&gt;My &lt;code&gt;InfinitivesExtractor&lt;/code&gt; class builds a data matrix from a list of
strings. After using it, the data needs to be passed to the classifier,
an instance&amp;nbsp;of &lt;code&gt;svm.LinearSVC&lt;/code&gt;.&amp;nbsp;The &lt;code&gt;pipeline&lt;/code&gt;module in scikits.learn
allows us to plug components into eachother in order to build a more
complex object. In this case, we would like a classifier that receives a
string as input, and directly outputs its label. We wouldn&amp;#8217;t want the
user to have to manually use the feature extractor prior to&amp;nbsp;classification.&lt;/p&gt;
&lt;p&gt;The pipeline is very easy to&amp;nbsp;build:  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;pipeline = Pipeline([('extr', InfinitivesExtractor()), ('svc', LinearSVC(multi_class=True))])&lt;/code&gt;&lt;br /&gt;
The pipeline object now works exactly as expected: we can call fit and
predict on it. It also exposes the parameters of its constituents, by
prefixing them with the name of that component. For example, the support
vector machine&amp;#8217;s C parameter can be accessed as&amp;nbsp;pipeline.svc__C.&lt;/p&gt;
&lt;p&gt;All that is left now is to see whether this is a good model, and what
combination of parameters makes it work the best. Scikits.learn provides
a great tool for choosing the parameters:&amp;nbsp;the &lt;code&gt;grid_search&lt;/code&gt; module. When
working with models like support vector machines, model parameters (such
as the radial basis kernel width) usually need to be chosen by cross
validation, because intuition doesn&amp;#8217;t help much when dealing with high
dimensional&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Grid search allows the definition of a discrete range of values for
multiple parameters. Then, for each combination of parameters, it fits
and evaluates a model using cross-validation, and the model with the
best score is the winner. Because we combined the components into a
pipeline, it is very easy to run grid search on the combined model, and
to simultaneously tweak the settings both for the extractor and for the&amp;nbsp;classifier.&lt;/p&gt;
&lt;p&gt;After running the grid search using the code &lt;a href="https://github.com/vene/misc-nlp/blob/master/conjugation/grid_search_example/gridsearch.py"&gt;here&lt;/a&gt;, I found that
indeed, using binary features instead of occurence counts improves
performance. I also found that the optimal n-gram length is 5, but the
gain is not that big when compared to a length of 3, which generates a
lot less&amp;nbsp;features.&lt;/p&gt;
&lt;h2 id="conclusions"&gt;Conclusions&lt;a class="headerlink" href="#conclusions" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I hope that I managed to show the strength of a well-designed &lt;span class="caps"&gt;API&lt;/span&gt;.
Because of it, it would be very easy to add, for example, an extra layer
for dimensionality reduction before classification. It would only
require an extra item in the pipeline constructor. A call from a
web-based frontend, for example, would be very short and simple. Because
of the consistency in the scikits.learn classes, we can write cleaner
and better code, and therefore work with greater&amp;nbsp;efficiency.&lt;/p&gt;</content><category term="scikit-learn"/><category term="nlp"/><category term="scikit-learn"/></entry><entry><title>My first scikits.learn coding sprint</title><link href="//vene.ro/blog/my-first-scikits-learn-coding-sprint.html" rel="alternate"/><published>2011-04-02T20:12:00+02:00</published><updated>2011-04-02T20:12:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-02:/blog/my-first-scikits-learn-coding-sprint.html</id><summary type="html">&lt;p&gt;The fifth &lt;a href="http://scikit-learn.sourceforge.net/" title="scikits.learn"&gt;scikits.learn&lt;/a&gt; coding sprint took place Friday, April 1st
2011. For anyone who is not familiar with it, scikits.learn is a fast
and easy to use machine learning toolkit for the pylab environment
(Python, NumPy, SciPy,&amp;nbsp;Matplotlib.)&lt;/p&gt;
&lt;p&gt;This was a good opportunity for me to get code …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The fifth &lt;a href="http://scikit-learn.sourceforge.net/" title="scikits.learn"&gt;scikits.learn&lt;/a&gt; coding sprint took place Friday, April 1st
2011. For anyone who is not familiar with it, scikits.learn is a fast
and easy to use machine learning toolkit for the pylab environment
(Python, NumPy, SciPy,&amp;nbsp;Matplotlib.)&lt;/p&gt;
&lt;p&gt;This was a good opportunity for me to get code reviews by the developers
in order to bring my &lt;span class="caps"&gt;NMF&lt;/span&gt; code up to standards, so that it can be merged.
Though I live far from every nucleus of scikits-learn developers, I
efficiently participated via &lt;span class="caps"&gt;IRC&lt;/span&gt;. This way, I also got the chance to
help out a bit on Mathieu Blondel&amp;#8217;s Kernel &lt;span class="caps"&gt;PCA&lt;/span&gt; code, which will also be
merged into main&amp;nbsp;soon.&lt;/p&gt;
&lt;h2 id="how-it-felt-like"&gt;How it felt like&lt;a class="headerlink" href="#how-it-felt-like" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Short answer:&amp;nbsp;awesome!&lt;/p&gt;
&lt;p&gt;Slightly longer answer: Everybody was very encouraging  and helpful.
They gave me a lot of feedback from which I learned a lot, and they
manifested the intention to merge soon. It is a pleasure to work on
projects that you like and use, especially when the projects leaders and
collaborators are so good to work&amp;nbsp;with.&lt;/p&gt;
&lt;p&gt;But the main reason why it makes me feel so good is that I&amp;#8217;m proud to
able to contribute on a project that I consider very significant and the
best in the field from many points of&amp;nbsp;view.&lt;/p&gt;
&lt;h2 id="what-i-got-done"&gt;What I got done&lt;a class="headerlink" href="#what-i-got-done" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Most of my work was on the non-negative matrix factorization module that
I began some while back, but only intermitently worked on. It is now a
solid module with high test coverage, documentation, and a cool simple
example showing a sparse set of features for the digits dataset in
scikits.learn.  Apart from all the minor fixes in overall code quality
and cleanliness, probably what is the most relevant is the improvement
and the study of the initialization methods. I will look into this
further and document it on this blog, the point is that the choice of
initialization method greatly influences the speed of convergence, and
in the case of a high-tolerance setting, also the error obtained. Some
initializations are more fit for sparsity settings, while others are
more fit for dense&amp;nbsp;settings.&lt;/p&gt;
&lt;p&gt;I have a theory that I plan to test out, regarding the use of different
initialization methods for components and for data in a sparse&amp;nbsp;setting.&lt;/p&gt;
&lt;h2 id="what-i-learned"&gt;What I learned&lt;a class="headerlink" href="#what-i-learned" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I think my greatest improvement was in terms of workflow and efficiency.
While my code was under review, I was receiving frequent comments on my
git pull request, and eventually I ended up responding to some comments
even before they were posted :). I sent small fixes as pull requests to
help other developers as much as I could. Before scikits.learn I had
never worked on a project with so many developers, and I think I handled
it well, even though I asked once or twice on the &lt;span class="caps"&gt;IRC&lt;/span&gt; channel for pieces
of&amp;nbsp;git-fu.&lt;/p&gt;
&lt;p&gt;I learned that it&amp;#8217;s difficult to tweak matplotlib subplots! I&amp;#8217;m still
staring at Alexandre Gramfort&amp;#8217;s tweak in my example and I have no idea
what he did to make it look so good. But I&amp;#8217;ll figure it out soon, I&amp;#8217;m&amp;nbsp;sure.&lt;/p&gt;
&lt;p&gt;I also learned a lot more about the intricacies of the scikits.learn
APIs, the philosophy of ease of use, and the project tree in&amp;nbsp;general.&lt;/p&gt;
&lt;p&gt;In short, the coding sprint has been a great and rewarding experience,
for which I thank all of you guys&amp;nbsp;there!&lt;/p&gt;</content><category term="scikit-learn"/><category term="coding sprint"/><category term="scikit-learn"/></entry></feed>