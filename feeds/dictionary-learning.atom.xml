<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Vlad Niculae (~vene)</title><link href="http://vene.ro/" rel="alternate"></link><link href="http://vene.ro/feeds/dictionary-learning.atom.xml" rel="self"></link><id>http://vene.ro/</id><updated>2011-04-15T14:10:00+02:00</updated><entry><title>An overview of dictionary learning: Terminology</title><link href="http://vene.ro/blog/an-overview-of-dictionary-learning-terminology.html" rel="alternate"></link><updated>2011-04-15T14:10:00+02:00</updated><author><name>vene</name></author><id>tag:vene.ro,2011-04-15:blog/an-overview-of-dictionary-learning-terminology.html</id><summary type="html">&lt;p&gt;My GSoC proposal is titled &amp;#8220;Dictionary learning in scikits.learn&amp;#8221; and in
the project, I plan to implement methods used in state of the art
research and industry applications in signal and image processing. In
this post, I want to clarify the terminology&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;Usually the terms &lt;em&gt;dictionary learning&lt;/em&gt; and &lt;em&gt;sparse coding&lt;/em&gt; are used
interchangably. Also my proposal contains methods such as Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt;
which are technically not &lt;em&gt;sparse coding&lt;/em&gt; but closely related&amp;nbsp;problems.&lt;/p&gt;
&lt;p&gt;The basic idea is the approximation of a signal vector [latex] x \in
\mathbb{R}\^d[/latex] by a linear combination of components, as good as
possible, under certain constraints. This can be formulated as a basic
(unconstrained) loss function measuring the quality of the
approximation: [latex] \mathcal{L}(x, D, \alpha) = \big|\big|x -
D\alpha\big|\big|\^2_2, D \in \mathbb{R}\^{d \times r}, \alpha
\in \mathbb{R}\^r[/latex], where [latex] r[/latex] is the dimension of
the dictionary (the number of &lt;em&gt;components)&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When working with a dataset of more signal vectors, the overall basic
loss for such an approximation is [latex] \mathcal{L}(X, D, A) =
\sum_{i = 1}\^N \mathcal{L}(x_i, D, \alpha_i) = \big|\big|X -
&lt;span class="caps"&gt;DA&lt;/span&gt;\big|\big|\^2_F[/latex]. Minimizing such a loss function amounts to
finding the closest (in the Frobenius sense) matrix factorization
[latex] &lt;span class="caps"&gt;DA&lt;/span&gt;[/latex] that approximates the data matrix [latex]&amp;nbsp;X[/latex]&lt;/p&gt;
&lt;p&gt;This generic problem is called a &lt;strong&gt;matrix factorization problem&lt;/strong&gt;. Many
classical problems are matrix factorization problems with additional
constraints. For example, &lt;span class="caps"&gt;PCA&lt;/span&gt; is a matrix factorization that constrains
[latex] D[/latex] to be orthogonal. &lt;span class="caps"&gt;NMF&lt;/span&gt; constrains both [latex]
D[/latex] and [latex] A[/latex] to have no negative elements. Sparse
variants of these two decompositions are useful for getting local
components, such as parts of faces. These are obtained by adding an
aditional constraint on the dictionary&amp;nbsp;columns.&lt;/p&gt;
&lt;p&gt;It can  be sometimes useful to consider the dictionary fixed. The signal
processing community has introduced over the years many such
dictionaries, for examples wavelets. These are used, for example, in the
&lt;span class="caps"&gt;JPEG2000&lt;/span&gt; compression&amp;nbsp;standard.&lt;/p&gt;
&lt;p&gt;A very useful represenation is when the dictionary is &lt;em&gt;overcomplete&lt;/em&gt;
([latex] r &gt; d[/latex]). The wavelets are an example of this. Given
such a dictionary, we are interested in an efficient encoding of a
vector [latex] x[/latex], in the sense of sparseness: we want to use as
few dictionary components as possible in our representation. Such a
solution is the vector [latex]\alpha[/latex] minimizing [latex]
\mathcal{L}(x, D, \alpha) +
\lambda\big|\big|\alpha\big|\big|_1[/latex] but other
sparsity-inducing constraints can be used. Such a vector is a &lt;strong&gt;sparse
coding&lt;/strong&gt; of [latex] x[/latex] and it can be solved using algorithms such
as least-angle regression and orthogonal matching&amp;nbsp;pursuit.&lt;/p&gt;
&lt;p&gt;However, we are not limited to using precomputed dictionaries. The term
&lt;strong&gt;dictionary learning&lt;/strong&gt; refers to methods of inferring, given [latex]
X[/latex], a (usually overcomplete) dictionary that will perform good at
sparsely encoding the data in [latex] X[/latex]. Such methods are more
expensive than using precomputed dictionaries, but they perform better,
since the dictionary is optimized for the current&amp;nbsp;dataset.&lt;/p&gt;
&lt;p&gt;Because usually such loss functions are non-convex in [latex] D[/latex]
and [latex] A[/latex] simultaneously, dictionary learning algorithms
alternate between minimizing each while keeping the other fixed. The
step that minimizes [latex] D[/latex] is sometimes called the
&lt;strong&gt;dictionary update&lt;/strong&gt; step, and the one minimizing [latex] A[/latex] is
(similarily to the case where the dictionary is always fixed) the
&lt;strong&gt;sparse coding&lt;/strong&gt; step. Dictionary learning algorithms differ in the
method used for each of this&amp;nbsp;step.&lt;/p&gt;
&lt;p&gt;To resume, many problems can be posed as matrix factorization problems.
Depending on the constraints imposed, the problem becomes interesting
for different applications. Dictionary learning is very good for image
reconstruction. Matrix decompositions with sparse undercomplete
dictionaries such as Sparse &lt;span class="caps"&gt;PCA&lt;/span&gt; can be used to find local features that
constitute the dataset, for example parts of faces, for a dataset of
facial images. &lt;span class="caps"&gt;NMF&lt;/span&gt; can be used in both under and overcomplete settings
and it offers a good model for additive data such as text or images. We
are interested in these variants and they are planned for implementation
in my GSoC&amp;nbsp;proposal.&lt;/p&gt;
&lt;p&gt;Julien Mairal&amp;#8217;s presentation of his work in this domain, available
&lt;a href="http://videolectures.net/icml09_mairal_odlsc/" title="Mairal dictionary learning"&gt;here&lt;/a&gt;, shows the theoretical background of such methods, along with
examples showing state of the art results in image&amp;nbsp;processing.&lt;/p&gt;</summary><category term="dictionary learning"></category><category term="scikit-learn"></category><category term="Uncategorized"></category></entry></feed>