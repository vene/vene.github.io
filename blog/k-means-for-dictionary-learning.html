<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>K-Means for dictionary learning</title>
  <meta name="author" content="Vlad" />
  <base href="//vene.ro">
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/main.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/pygment.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/typogrify.css" />
  <link rel="shortcut icon" href="//vene.ro/favicon.ico" />
  <link href="//vene.ro/" type="application/atom+xml"
        rel="alternate" title="Vlad Niculae ALL Atom Feed" />
  <link href="//fonts.googleapis.com/css?family=PT+Mono|PT+Serif" rel="stylesheet"> 

  <!-- OpenGraph Info -->


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
  integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
  crossorigin="anonymous"></script>
  <script defer
          src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
          integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
          crossorigin="anonymous"> </script>
  <script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
       macros: {"\\reals": "\\mathbb{R}"},
       throwOnError: true,
       delimiters: [
               {left: "$$", right: "$$", display: true},
               {left: "$", right: "$", display: false},
               {left: "\\(", right: "\\)", display: false},
               {left: "\\[", right: "\\]", display: true}
       ],
     });
  })
  </script>

</head>

<body>
<div id="container">
<header>
  <nav class="navmenu" id="navmenu">
    <li id="homelink"><a href="/">Vlad Niculae</a>
    </li><li class="menu"><a href="//vene.ro/papers.html">Papers</a>
    </li><li class="menu"><a href="//vene.ro/blog/">Blog</a>
    </li><li class="menu"><a href="//vene.ro/teaching.html">Teaching</a>
   </li>
   </nav>
 </header>
 <div id="main">
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="//vene.ro/blog/k-means-for-dictionary-learning.html" rel="bookmark"
           title="Permalink to K-Means for dictionary learning">K-Means for dictionary&nbsp;learning</a></h1>
<p class="subtitle"><time datetime="2011-07-10T14:27:00+02:00">Sun, 10 Jul 2011</time><label for="k-means-for-dictionary-learning" class="margin-toggle"> ⊕</label><input type="checkbox" id="k-means-for-dictionary-learning" class="margin-toggle" /><span class="marginnote">Category: <a href="//vene.ro/category/scikit-learn.html">scikit-learn</a><br />
 #<a href="//vene.ro/tag/dictionary-learning.html">dictionary learning</a> #<a href="//vene.ro/tag/k-means.html">k-means</a> #<a href="//vene.ro/tag/scikit-learn.html">scikit-learn</a> #<a href="//vene.ro/tag/vq.html">vq</a> #<a href="//vene.ro/tag/uncategorized.html">Uncategorized</a></span></p>    </header>

    <div class="entry-content">
      <p>[![Dictionary learned with K-Means on the <span class="caps">LFW</span> dataset with whitening
<span class="caps">PCA</span>][]][][![Dictionary learned with K-Means on the <span class="caps">LFW</span> dataset without
whitening <span class="caps">PCA</span>][]][]</p>
<p>One of the simplest, and yet most heavily constrained form of matrix
factorization, is vector quantization (<span class="caps">VQ</span>). Heavily used in image/video
compression, the <span class="caps">VQ</span> problem is a factorization [latex] X=<span class="caps">WH</span>[/latex]
where [latex] H[/latex] (our dictionary) is called the codebook and is
designed to cover the cloud of data points effectively, and each line of
[latex] W[/latex] is a unit&nbsp;vector.</p>
<p>This means that each each data point [latex] x_i[/latex] is
approximated as [latex] x_i \approx h_{k} = \sum_{j=1}\^{r}
\delta_{kj}h_{j}[/latex]. In other words, the closest row vector
(codeword/dictionary atom) [latex] h_k[/latex] of [latex] H[/latex] is
chosen as an approximation, and this is encoded as a unit vector [latex]
(\delta_{k1}, &#8230;, \delta_{kr})[/latex]. The data representation
[latex] W[/latex] is composed of such&nbsp;vectors.</p>
<p>There is a variation called gain-shape <span class="caps">VQ</span> where instead of approximating
each point as one of the codewords, we allow a scalar multiplication
invariance: [latex] x_i \approx \alpha_ih_k[/latex]. This model
requires considerably more storage (each data point needs a floating
point number and an unsigned index, as opposed to just the index), but
it leads to a much better approximation.<br>
Gain-shape <span class="caps">VQ</span> can equivalently be accomplished by normalizing each data
vector prior to fitting the&nbsp;codebook.</p>
<p>In order to fit a codebook [latex] H[/latex] for efficient <span class="caps">VQ</span> use, the
K-Means Clustering [<a href="#footnote-1">1</a>] algorithm is a natural thought. K-means is an
iterative algorithm that incrementally improves the dispersion of k
cluster centers in the data space until convergence. The cluster centers
are initialized in a random or procedural fashion, then, at each
iteration, the data points are assigned to the closest cluster center,
which is subsequently moved to the center of the points assigned to&nbsp;it.</p>
<p>The <code>scikits.learn.decomposition.KMeansCoder</code> object from our
work-in-progress dictionary learning toolkit can learn a dictionary from
image patches using the K-Means algorithm, with optional local contrast
normalization and a <span class="caps">PCA</span> whitening transform. Using a trained object to
transform data points with orthogonal matching pursuit, with the
parameter <code>n_atoms=1</code> is equivalent to gain-shape <span class="caps">VQ</span>. Of course you are
free to use any method of sparse coding such as <span class="caps">LARS</span>. The code used to
produce the example images on top of this post can be found in [<a href="#footnote-2">2</a>].</p>
<p>Using K-Means for learning the dictionary does not optimize over linear
combinations of dictionary atoms, like standard dictionary learning
methods do. However, it&#8217;s considerably faster, and Adam Coates and
Andrew Ng suggest in [<a href="#footnote-3">3</a>] that as long as the dictionary is filled
with a large enough number of atoms and it covers well enough the cloud
of data (and of future test data) points, then K-Means, or even random
sampling of image patches, can perform remarkably well for some&nbsp;tasks.</p>
<div id="footnote-1">
[1] [Wikipedia article on K-Means clustering][]

</div>

<div id="footnote-2">
[2] [K-Means Coder example][]

</div>

<div id="footnote-3">
[3] [**The importance of encoding versus training with sparse coding and
vector quantization**, Adam Coates and Andrew Y. Ng. In Proceedings of
the Twenty-Eighth International Conference on Machine Learning, 2011.][]

</div>

<p>[Dictionary learned with K-Means on the <span class="caps">LFW</span> dataset with whitening
  <span class="caps">PCA</span>]: http://localhost:8001/wp-content/uploads/2011/07/kmeans_w.png?w=250
    &#8220;K-Means dictionary with whitening <span class="caps">PCA</span>&#8221;
  [![Dictionary learned with K-Means on the <span class="caps">LFW</span> dataset with whitening
  <span class="caps">PCA</span>][]]: http://localhost:8001/wp-content/uploads/2011/07/kmeans_w.png
  [Dictionary learned with K-Means on the <span class="caps">LFW</span> dataset without whitening
  <span class="caps">PCA</span>]: http://localhost:8001/wp-content/uploads/2011/07/kmeans_no_w.png?w=250
    &#8220;K-Means dictionary without whitening <span class="caps">PCA</span>&#8221;
  [![Dictionary learned with K-Means on the <span class="caps">LFW</span> dataset without
  whitening <span class="caps">PCA</span>][]]:&nbsp;http://localhost:8001/wp-content/uploads/2011/07/kmeans_no_w.png</p>
<p>[<strong>The importance of encoding versus training with sparse coding and
  vector quantization</strong>, Adam Coates and Andrew Y. Ng. In Proceedings of
  the Twenty-Eighth International Conference on Machine Learning,
  2011.]:&nbsp;http://ai.stanford.edu/~ang/papers/icml11-EncodingVsTraining.pdf</p>
    </div><!-- /.entry-content -->
    <div class="comments">
      <h2>Comments !</h2>
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_shortname = 'vene';
        var disqus_identifier = 'blog/k-means-for-dictionary-learning.html';
        var disqus_url = '//vene.ro/blog/k-means-for-dictionary-learning.html';
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//vene.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>

  </article>
</section>
 </div>
<footer>
  <p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>.
  <a href="/privacy.html">Privacy policy</a>.</p>
</footer>
</div>
</body>
</html>