<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Optimizing Orthogonal Matching Pursuit code in Numpy, part 2</title>
  <meta name="author" content="Vlad" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/main.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/pygment.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/typogrify.css" />
  <link rel="shortcut icon" href="//vene.ro/favicon.ico" />
  <link href="//vene.ro/" type="application/atom+xml"
        rel="alternate" title="Vlad Niculae ALL Atom Feed" />
  <link href="//fonts.googleapis.com/css?family=PT+Mono|PT+Serif" rel="stylesheet"> 

  <!-- OpenGraph Info -->


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
  integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
  crossorigin="anonymous"></script>
  <script src="//vene.ro/theme/js/main.js"></script>

</head>

<body>
<div id="container">
<header>
  <nav class="navmenu" id="navmenu">
    <li id="homelink"><a href="/">Vlad Niculae</a>
    </li><li class="menu"><a href="//vene.ro/papers.html">Papers</a>
    </li><li class="menu"><a href="//vene.ro/blog/">Blog</a>
    </li><li class="menu"><a href="//vene.ro/teaching.html">Teaching</a>
   </li>
   </nav>
 </header>
 <div id="main">
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="//vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html" rel="bookmark"
           title="Permalink to Optimizing Orthogonal Matching Pursuit code in Numpy, part 2">Optimizing Orthogonal Matching Pursuit code in Numpy, part&nbsp;2</a></h1>
<p class="subtitle"><time datetime="2011-08-11T19:39:00+02:00">Thu, 11 Aug 2011</time><label for="optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2" class="margin-toggle"> ⊕</label><input type="checkbox" id="optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2" class="margin-toggle" /><span class="marginnote">Category: <a href="//vene.ro/category/scikit-learn.html">scikit-learn</a><br />
 #<a href="//vene.ro/tag/blas.html">blas</a> #<a href="//vene.ro/tag/efficient.html">efficient</a> #<a href="//vene.ro/tag/lapack.html">lapack</a> #<a href="//vene.ro/tag/numpy.html">numpy</a> #<a href="//vene.ro/tag/omp.html">omp</a> #<a href="//vene.ro/tag/orthogonal-matching-pursuit.html">orthogonal matching pursuit</a> #<a href="//vene.ro/tag/potrs.html">potrs</a> #<a href="//vene.ro/tag/scipy.html">scipy</a> #<a href="//vene.ro/tag/dictionary-learning.html">dictionary learning</a> #<a href="//vene.ro/tag/python.html">python</a> #<a href="//vene.ro/tag/scikit-learn.html">scikit-learn</a></span></p>    </header>

    <div class="entry-content">
      <p><span class="caps">EDIT</span>: There was a bug in the final version of the code presented here.
It is fixed now, for its backstory, check out <a href="http://venefrombucharest.wordpress.com/2011/11/18/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code/" title="The nasty bug crawling in my Orthogonal Matching Pursuit code">my blog post on it</a>.</p>
<p>When we last saw our hero, he was fighting with the dreaded
implementation of least-angle regression, knowing full well that it was
his destiny to be&nbsp;faster.</p>
<p>We had come up with a more robust implementation, catching malformed
cases that would have broken the naive implementation, and also it was
orders of magnitude faster than said implementation. However, our
benchmark [<a href="#footnote-1">1</a>] showed that it was a couple of times slower than
least-angle&nbsp;regression.</p>
<p>By poking around the <code>scikits.learn</code> codebase, I noticed that there is a
triangular system solver in <code>scikits.learn.utils.arrayfuncs</code>. Unlike the
<code>scipy.linalg</code> one, this one only works with lower triangular arrays,
and it forcefully overwrites <code>b</code>. Even though if weren&#8217;t faster, it
should still be used: <code>scikits.learn</code> aims to be as backwards-compatible
with SciPy as possible, and <code>linalg.solve_triangular</code> was added in
0.9.0. Anyway, let&#8217;s just see whether it&#8217;s&nbsp;faster:</p>
<p>[sourcecode language=&#8221;python&#8221;]<br />
In <a href="#footnote-1">1</a>: import numpy as&nbsp;np</p>
<p>In <a href="#footnote-2">2</a>: from scipy import&nbsp;linalg</p>
<p>In [3]: from scikits.learn.datasets import&nbsp;make_spd_matrix</p>
<p>In [4]: from scikits.learn.utils.arrayfuncs import&nbsp;solve_triangular</p>
<p>In [5]: G =&nbsp;make_spd_matrix(1000)</p>
<p>In [6]: L = linalg.cholesky(G,&nbsp;lower=True)</p>
<p>In [7]: x =&nbsp;np.random.randn(1000)</p>
<p>In [8]: y =&nbsp;x.copy()</p>
<p>In [9]: timeit solve_triangular(L, x)<br />
100 loops, best of 3: 3.45 ms per&nbsp;loop</p>
<p>In [10]: timeit linalg.solve_triangular(L, y, lower=True,
overwrite_b=True)<br />
10 loops, best of 3: 134 ms per loop<br />&nbsp;[/sourcecode]</p>
<p>Wow! That&#8217;s 40x faster. We&#8217;re catching two rabbits with one stone here,
let&#8217;s do the change! Notice that we can just copy [latex]
\mathbf{v}[/latex] into the appropriate place in [latex] L[/latex] and
then solve in&nbsp;place.</p>
<p>But whoops! When solving the [latex] <span class="caps">LL</span>&#8217;[/latex] system, we take
advantage of the <code>transpose</code> attribute in <code>linalg.solve_triangular</code>,
which the <code>scikits.learn</code> version does not expose. We could think of a
solution, but here&#8217;s a better idea: Shouldn&#8217;t there be some way to
directly solve the entire system in one&nbsp;go?</p>
<p>Well, there is. It is an <span class="caps">LAPACK</span> function by the name of <code>potrs</code>. If you
are not aware, <span class="caps">LAPACK</span> is a Fortran library with solvers for various
types of linear systems and eigenproblems. <span class="caps">LAPACK</span> along with <span class="caps">BLAS</span> (on
which it is based) pretty much powers all the scientific computation
that happens. <span class="caps">BLAS</span> is an <span class="caps">API</span> with multiple implementations dating from
1979, while <span class="caps">LAPACK</span> dates from 1992. If you ever used Matlab, this is
what was called behind the scenes. SciPy, again, provides a high-level
wrapper around this, the <code>linalg.cho_solve</code> function.</p>
<p>But SciPy also gives us the possibility to import functions directly
from <span class="caps">LAPACK</span>, through the use of <code>linalg.lapack.get_lapack_funcs</code>. Let&#8217;s
see how the low-level <span class="caps">LAPACK</span> function compares to the SciPy wrapper, for
our use&nbsp;case:</p>
<p>[sourcecode language=&#8221;python&#8221;]<br />
In [11]: x =&nbsp;np.random.randn(1000)</p>
<p>In [12]: y =&nbsp;x.copy()</p>
<p>In [13]: timeit linalg.cho_solve((L, True), x)<br />
1 loops, best of 3: 95.4 ms per&nbsp;loop</p>
<p>In [14]: potrs, = linalg.lapack.get_lapack_funcs((&#8216;potrs&#8217;,),&nbsp;(G,))</p>
<p>In [15]: potrs<br />
Out[15]: &lt;fortran&nbsp;object&gt;</p>
<p>In [16]: timeit potrs(L, y)<br />
100 loops, best of 3: 9.49 ms per loop<br />&nbsp;[/sourcecode]</p>
<p>That&#8217;s 10 times faster! So now we found an obvious way to optimize the&nbsp;code:</p>
<p>[sourcecode language=&#8221;python&#8221;]<br />
def cholesky_omp(X, y, n_nonzero_coefs, eps=None):<br />
min_float = np.finfo(X.dtype).eps<br />
potrs, = get_lapack_funcs((&#8216;potrs&#8217;,), (X,))<br />
alpha = np.dot(X.T, y)<br />
residual = y<br />
n_active = 0<br />
idx =&nbsp;[]</p>
<p>max_features = X.shape<a href="#footnote-1">1</a> if eps is not None else n_nonzero_coefs<br />
L = np.empty((max_features, max_features), dtype=X.dtype)<br />
L[0, 0] =&nbsp;1.</p>
<p>while 1:<br />
lam = np.abs(np.dot(X.T, residual)).argmax()<br />
if lam &lt; n_active or alpha[lam] ** 2 &lt; min_float:<br />
# atom already selected or inner product too small<br />
warn(&#8220;Stopping early&#8221;)<br />
break<br />
if n_active &gt; 0:<br />
# Updates the Cholesky decomposition of X&#8217; X<br />
L[n_active, :n_active] = np.dot(X[:, idx].T, X[:, lam]<br />
solve_triangular(L[:n_active, :n_active], L[n_active, :n_active])<br />
d = np.dot(L[n_active, :n_active].T, L[n_active, :n_active])<br />
if 1 - d &lt;= min_float: # selected atoms are dependent<br />
warn(&#8220;Stopping early&#8221;)<br />
break<br />
L[n_active, n_active] = np.sqrt(1 - d)<br />
idx.append(lam)<br />
# solve <span class="caps">LL</span>&#8217;x = y in two steps:<br />
gamma, _ = potrs(L[:n_active, :n_active], alpha[idx], lower=True,<br />
overwrite_b=False)<br />
residual = y - np.dot(X[:, idx], gamma)<br />
if eps is not None and np.dot(residual.T, residual) &lt;= eps:<br />
break<br />
elif n_active == max_features:<br />
break<br />
return gamma, idx<br />&nbsp;[/sourcecode]</p>
<p>Woohoo! But we still lag behind. Now that we delegated the trickiest
parts of the code to fast and reliable solvers, it&#8217;s time to use a
profiler and see what the bottleneck is now. Python has excellent tools
for this purpose. What solved the problem in this case was
<code>line_profiler</code> [<a href="#footnote-2">2</a>]. There is a great article by Olivier Grisel here
<a href="#footnote-2">2</a> regarding how to use these profilers. I&#8217;m just going to say that
<code>line_profiler</code><span class="quo">&#8216;</span>s output is very helpful, basically printing the time
taken by each line of code next to that&nbsp;line.</p>
<p>Running the profiler on this code, we found that 58% of the time is
spent on line 14, 20.5% on line 21, and 20.5% on line 32, with the rest
being insignificant (<code>potrs</code> takes 0.1%!). The code is clearly dominated
by the matrix multiplications. By running some more timings with
IPython, I found that multiplying such column-wise views of the data as
<code>X[:, idx]</code> is considerably slower then multiplying a contiguous array.
The least-angle regression code in <code>scikits.learn</code> avoids this by
swapping columns towards the front of the array as they are chosen, so
we can replace <code>X[:, idx]</code> with <code>X[:, :n_active]</code>. The nice part is that
if the array is stored in Fortran-contiguous order (ie. column
contiguous order, as opposed to row contiguous order, as in C), swapping
two columns is a very fast operation!. Let&#8217;s see some more&nbsp;benchmarks!</p>
<p>[sourcecode language=&#8221;python&#8221;]<br />
In [17]: X = np.random.randn(5000,&nbsp;5000)</p>
<p>In [18]: Y = X.copy(&#8216;F&#8217;) #&nbsp;fortran-ordered</p>
<p>In [19]: a, b = 1000,&nbsp;2500</p>
<p>In [20]: swap, = linalg.get_blas_funcs((&#8216;swap&#8217;,),&nbsp;(X,))</p>
<p>In [21]: timeit X[:, a], X[:, b] = swap(X[:, a], X[:, b])<br />
100 loops, best of 3: 6.29 ms per&nbsp;loop</p>
<p>In [22]: timeit Y[:, a], Y[:, b] = swap(Y[:, a], Y[:, b])<br />
10000 loops, best of 3: 111 us per loop<br />&nbsp;[/sourcecode]</p>
<p>We can see that using Fortran-order takes us from the order of
miliseconds to the order of&nbsp;microseconds!</p>
<p>Side note: I almost fell into the trap of swapping columns the pythonic
way. That doesn&#8217;t work:<br />
[sourcecode language=&#8221;python&#8221;]<br />
In [23]: X[:, a], X[:, b] = X[:, b], X[:,&nbsp;a]</p>
<p>In [24]: np.testing.assert_array_equal(X[:, a], X[:,&nbsp;b])</p>
<p>In [25]:<br />&nbsp;[/sourcecode]</p>
<p>However this trick works great for swapping elements of one-dimensional&nbsp;arrays.</p>
<p>Another small optimization that we can do: I found that on my system,
it&#8217;s slightly faster to compute the norm using the <span class="caps">BLAS</span> function <code>nrm2</code>.
So by putting all of these together, we end up with the final version of
our&nbsp;code:</p>
<p>[sourcecode language=&#8221;python&#8221;]<br />
def cholesky_omp(X, y, n_nonzero_coefs, eps=None,
overwrite_X=False):<br />
if not overwrite_X:<br />
X = X.copy(&#8216;F&#8217;)<br />
else: # even if we are allowed to overwrite, still copy it if bad
order<br />
X =&nbsp;np.asfortranarray(X)</p>
<p>min_float = np.finfo(X.dtype).eps<br />
nrm2, swap = linalg.get_blas_funcs((&#8216;nrm2&#8217;, &#8216;swap&#8217;), (X,))<br />
potrs, = get_lapack_funcs((&#8216;potrs&#8217;,),&nbsp;(X,))</p>
<p>indices = range(len(Gram)) # keeping track of swapping<br />
alpha = np.dot(X.T, y)<br />
residual = y<br />
n_active =&nbsp;0</p>
<p>max_features = X.shape<a href="#footnote-1">1</a> if eps is not None else n_nonzero_coefs<br />
L = np.empty((max_features, max_features), dtype=X.dtype)<br />
L[0, 0] =&nbsp;1.</p>
<p>while True:<br />
lam = np.abs(np.dot(X.T, residual)).argmax()<br />
if lam &lt; n_active or alpha[lam] ** 2 &lt; min_float:<br />
# atom already selected or inner product too small<br />
warn(&#8220;Stopping early&#8221;)<br />
break<br />
if n_active &gt; 0:<br />
# Updates the Cholesky decomposition of X&#8217; X<br />
L[n_active, :n_active] = np.dot(X[:, :n_active].T, X[:, lam])<br />
solve_triangular(L[:n_active, :n_active], L[n_active, :n_active])<br />
v = nrm2(L[n_active, :n_active]) ** 2<br />
if 1 - v &lt;= min_float: # selected atoms are dependent<br />
warn(&#8220;Stopping early&#8221;)<br />
break<br />
L[n_active, n_active] = np.sqrt(1 - v)<br />
X.T[n_active], X.T[lam] = swap(X.T[n_active], X.T[lam])<br />
alpha[n_active], alpha[lam] = alpha[lam], alpha[n_active]<br />
indices[n_active], indices[lam] = indices[lam], indices[n_active]<br />
n_active += 1<br />
# solves <span class="caps">LL</span>&#8217;x = y as a composition of two triangular systems<br />
gamma, _ = potrs(L[:n_active, :n_active], alpha[:n_active],
lower=True,<br />&nbsp;overwrite_b=False)</p>
<p>residual = y - np.dot(X[:, :n_active], gamma)<br />
if eps is not None and nrm2(residual) ** 2 &lt;= eps:<br />
break<br />
elif n_active == max_features:<br />&nbsp;break</p>
<p>return gamma, indices[:n_active]<br />&nbsp;[/sourcecode]</p>
<p>Now, the benchmark at [<a href="#footnote-1">1</a>] indicates victory over least-angle
regression! I hope you have enjoyed this short tour. See you next&nbsp;time!</p>
<p><a href="https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_plot_omp_lars.py" title="Orthogonal matching pursuit versus least-angle regression"><span id="footnote-1">1</span></a>[]<br />
<a href="http://scikit-learn.sourceforge.net/dev/developers/performance.html#profiling-python-code" title="Profiling Python code"><span id="footnote-2">2</span></a>[]</p>
    </div><!-- /.entry-content -->
    <div class="comments">
      <h2>Comments !</h2>
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_shortname = 'vene';
        var disqus_identifier = 'blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html';
        var disqus_url = 'https://vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html';
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://vene.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>

  </article>
</section>
 </div>
<footer>
  <p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>.
  <a href="/privacy.html">Privacy policy</a>.</p>
</footer>
</div>
</body>
</html>