<!DOCTYPE html>
<html lang="en">
<head>
        <title>Optimizing Orthogonal Matching Pursuit code in Numpy, part 2</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />
        <link href="http://vene.ro/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Vlad Niculae (~vene) Atom Feed" />
        <link href="http://vene.ro/feed/all.rss.xml" type="application/rss+xml" rel="alternate" title="Vlad Niculae (~vene) RSS Feed" />
        <link href='http://fonts.googleapis.com/css?family=Averia+Gruesa+Libre|Alegreya:400italic,400,700|Alegreya+SC&subset=latin-ext' rel='stylesheet' type='text/css'>

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://vene.ro/css/ie.css"/>
                <script src="http://vene.ro/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://vene.ro/css/ie6.css"/><![endif]-->
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
      },
      displayAlign: 'left', // Change this to 'center' to center equations.
      "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
      }
    });
    </script>
    <!-- End of mathjax configuration -->

    <script>
    //  We wait for the onload function to load MathJax after the page is completely loaded.
    //  MathJax is loaded 1 unit of time after the page is ready.
    //  This hack prevent problems when you load multiple js files.

    window.onload = function () {
      setTimeout(function () {
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src  = "https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS_HTML";
        document.getElementsByTagName("head")[0].appendChild(script);
      },1)
    }
    </script>

</head>

<body id="index" class="home">
<div id="container">
        <header id="banner" class="body">
                <div id="mainheader">Vlad Niculae (~vene)</div>
                <!--<nav><ul>
                    <li><a href="/publications.html">Publications</a></li>
                    <li><a href="http://vene.ro/blog/">Blog</a></li>
                </ul></nav> -->
        </header><!-- /#banner -->
        <div id="main" role="main">
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="http://vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html" rel="bookmark"
           title="Permalink to Optimizing Orthogonal Matching Pursuit code in Numpy, part 2">Optimizing Orthogonal Matching Pursuit code in Numpy, part&nbsp;2</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2011-08-11T19:39:00+02:00">
                Thu 11 August 2011
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="http://vene.ro/blog/author/vene.html">vene</a>
        </address>
<p>In <a href="http://vene.ro/blog/category/scikit-learn.html">scikit-learn</a>. </p>
<p>tags: <a href="http://vene.ro/blog/tag/blas.html">blas</a> <a href="http://vene.ro/blog/tag/efficient.html">efficient</a> <a href="http://vene.ro/blog/tag/lapack.html">lapack</a> <a href="http://vene.ro/blog/tag/numpy.html">numpy</a> <a href="http://vene.ro/blog/tag/omp.html">omp</a> <a href="http://vene.ro/blog/tag/orthogonal-matching-pursuit.html">orthogonal matching pursuit</a> <a href="http://vene.ro/blog/tag/potrs.html">potrs</a> <a href="http://vene.ro/blog/tag/scipy.html">scipy</a> <a href="http://vene.ro/blog/tag/dictionary-learning.html">dictionary learning</a> <a href="http://vene.ro/blog/tag/python.html">python</a> <a href="http://vene.ro/blog/tag/scikit-learn.html">scikit-learn</a> </p>
</footer><!-- /.post-info -->      <p><span class="caps">EDIT</span>: There was a bug in the final version of the code presented here.
It is fixed now, for its backstory, check out <a href="http://venefrombucharest.wordpress.com/2011/11/18/the-nasty-bug-crawling-in-my-orthogonal-matching-pursuit-code/" title="The nasty bug crawling in my Orthogonal Matching Pursuit code">my blog post on it</a>.</p>
<p>When we last saw our hero, he was fighting with the dreaded
implementation of least-angle regression, knowing full well that it was
his destiny to be&nbsp;faster.</p>
<p>We had come up with a more robust implementation, catching malformed
cases that would have broken the naive implementation, and also it was
orders of magnitude faster than said implementation. However, our
benchmark [<a href="#footnote-1">1</a>] showed that it was a couple of times slower than
least-angle&nbsp;regression.</p>
<p>By poking around the <code>scikits.learn</code> codebase, I noticed that there is a
triangular system solver in <code>scikits.learn.utils.arrayfuncs</code>. Unlike the
<code>scipy.linalg</code> one, this one only works with lower triangular arrays,
and it forcefully overwrites <code>b</code>. Even though if weren&#8217;t faster, it
should still be used: <code>scikits.learn</code> aims to be as backwards-compatible
with SciPy as possible, and <code>linalg.solve_triangular</code> was added in
0.9.0. Anyway, let&#8217;s just see whether it&#8217;s&nbsp;faster:</p>
<p>[sourcecode language=&#8221;python&#8221;]<br>
In <a href="#footnote-1">1</a>: import numpy as&nbsp;np</p>
<p>In <a href="#footnote-2">2</a>: from scipy import&nbsp;linalg</p>
<p>In [3]: from scikits.learn.datasets import&nbsp;make_spd_matrix</p>
<p>In [4]: from scikits.learn.utils.arrayfuncs import&nbsp;solve_triangular</p>
<p>In [5]: G =&nbsp;make_spd_matrix(1000)</p>
<p>In [6]: L = linalg.cholesky(G,&nbsp;lower=True)</p>
<p>In [7]: x =&nbsp;np.random.randn(1000)</p>
<p>In [8]: y =&nbsp;x.copy()</p>
<p>In [9]: timeit solve_triangular(L, x)<br>
100 loops, best of 3: 3.45 ms per&nbsp;loop</p>
<p>In [10]: timeit linalg.solve_triangular(L, y, lower=True,
overwrite_b=True)<br>
10 loops, best of 3: 134 ms per loop<br>&nbsp;[/sourcecode]</p>
<p>Wow! That&#8217;s 40x faster. We&#8217;re catching two rabbits with one stone here,
let&#8217;s do the change! Notice that we can just copy [latex]
\mathbf{v}[/latex] into the appropriate place in [latex] L[/latex] and
then solve in&nbsp;place.</p>
<p>But whoops! When solving the [latex] <span class="caps">LL</span>&#8217;[/latex] system, we take
advantage of the <code>transpose</code> attribute in <code>linalg.solve_triangular</code>,
which the <code>scikits.learn</code> version does not expose. We could think of a
solution, but here&#8217;s a better idea: Shouldn&#8217;t there be some way to
directly solve the entire system in one&nbsp;go?</p>
<p>Well, there is. It is an <span class="caps">LAPACK</span> function by the name of <code>potrs</code>. If you
are not aware, <span class="caps">LAPACK</span> is a Fortran library with solvers for various
types of linear systems and eigenproblems. <span class="caps">LAPACK</span> along with <span class="caps">BLAS</span> (on
which it is based) pretty much powers all the scientific computation
that happens. <span class="caps">BLAS</span> is an <span class="caps">API</span> with multiple implementations dating from
1979, while <span class="caps">LAPACK</span> dates from 1992. If you ever used Matlab, this is
what was called behind the scenes. SciPy, again, provides a high-level
wrapper around this, the <code>linalg.cho_solve</code> function.</p>
<p>But SciPy also gives us the possibility to import functions directly
from <span class="caps">LAPACK</span>, through the use of <code>linalg.lapack.get_lapack_funcs</code>. Let&#8217;s
see how the low-level <span class="caps">LAPACK</span> function compares to the SciPy wrapper, for
our use&nbsp;case:</p>
<p>[sourcecode language=&#8221;python&#8221;]<br>
In [11]: x =&nbsp;np.random.randn(1000)</p>
<p>In [12]: y =&nbsp;x.copy()</p>
<p>In [13]: timeit linalg.cho_solve((L, True), x)<br>
1 loops, best of 3: 95.4 ms per&nbsp;loop</p>
<p>In [14]: potrs, = linalg.lapack.get_lapack_funcs((&#8216;potrs&#8217;,),&nbsp;(G,))</p>
<p>In [15]: potrs<br>
Out[15]: &lt;fortran&nbsp;object&gt;</p>
<p>In [16]: timeit potrs(L, y)<br>
100 loops, best of 3: 9.49 ms per loop<br>&nbsp;[/sourcecode]</p>
<p>That&#8217;s 10 times faster! So now we found an obvious way to optimize the&nbsp;code:</p>
<p>[sourcecode language=&#8221;python&#8221;]<br>
def cholesky_omp(X, y, n_nonzero_coefs, eps=None):<br>
min_float = np.finfo(X.dtype).eps<br>
potrs, = get_lapack_funcs((&#8216;potrs&#8217;,), (X,))<br>
alpha = np.dot(X.T, y)<br>
residual = y<br>
n_active = 0<br>
idx =&nbsp;[]</p>
<p>max_features = X.shape<a href="#footnote-1">1</a> if eps is not None else n_nonzero_coefs<br>
L = np.empty((max_features, max_features), dtype=X.dtype)<br>
L[0, 0] =&nbsp;1.</p>
<p>while 1:<br>
lam = np.abs(np.dot(X.T, residual)).argmax()<br>
if lam &lt; n_active or alpha[lam] ** 2 &lt; min_float:<br>
# atom already selected or inner product too small<br>
warn(&#8220;Stopping early&#8221;)<br>
break<br>
if n_active &gt; 0:<br>
# Updates the Cholesky decomposition of X&#8217; X<br>
L[n_active, :n_active] = np.dot(X[:, idx].T, X[:, lam]<br>
solve_triangular(L[:n_active, :n_active], L[n_active, :n_active])<br>
d = np.dot(L[n_active, :n_active].T, L[n_active, :n_active])<br>
if 1 - d &lt;= min_float: # selected atoms are dependent<br>
warn(&#8220;Stopping early&#8221;)<br>
break<br>
L[n_active, n_active] = np.sqrt(1 - d)<br>
idx.append(lam)<br>
# solve <span class="caps">LL</span>&#8217;x = y in two steps:<br>
gamma, _ = potrs(L[:n_active, :n_active], alpha[idx], lower=True,<br>
overwrite_b=False)<br>
residual = y - np.dot(X[:, idx], gamma)<br>
if eps is not None and np.dot(residual.T, residual) &lt;= eps:<br>
break<br>
elif n_active == max_features:<br>
break<br>
return gamma, idx<br>&nbsp;[/sourcecode]</p>
<p>Woohoo! But we still lag behind. Now that we delegated the trickiest
parts of the code to fast and reliable solvers, it&#8217;s time to use a
profiler and see what the bottleneck is now. Python has excellent tools
for this purpose. What solved the problem in this case was
<code>line_profiler</code> [<a href="#footnote-2">2</a>]. There is a great article by Olivier Grisel here
<a href="#footnote-2">2</a> regarding how to use these profilers. I&#8217;m just going to say that
<code>line_profiler</code><span class="quo">&#8216;</span>s output is very helpful, basically printing the time
taken by each line of code next to that&nbsp;line.</p>
<p>Running the profiler on this code, we found that 58% of the time is
spent on line 14, 20.5% on line 21, and 20.5% on line 32, with the rest
being insignificant (<code>potrs</code> takes 0.1%!). The code is clearly dominated
by the matrix multiplications. By running some more timings with
IPython, I found that multiplying such column-wise views of the data as
<code>X[:, idx]</code> is considerably slower then multiplying a contiguous array.
The least-angle regression code in <code>scikits.learn</code> avoids this by
swapping columns towards the front of the array as they are chosen, so
we can replace <code>X[:, idx]</code> with <code>X[:, :n_active]</code>. The nice part is that
if the array is stored in Fortran-contiguous order (ie. column
contiguous order, as opposed to row contiguous order, as in C), swapping
two columns is a very fast operation!. Let&#8217;s see some more&nbsp;benchmarks!</p>
<p>[sourcecode language=&#8221;python&#8221;]<br>
In [17]: X = np.random.randn(5000,&nbsp;5000)</p>
<p>In [18]: Y = X.copy(&#8216;F&#8217;) #&nbsp;fortran-ordered</p>
<p>In [19]: a, b = 1000,&nbsp;2500</p>
<p>In [20]: swap, = linalg.get_blas_funcs((&#8216;swap&#8217;,),&nbsp;(X,))</p>
<p>In [21]: timeit X[:, a], X[:, b] = swap(X[:, a], X[:, b])<br>
100 loops, best of 3: 6.29 ms per&nbsp;loop</p>
<p>In [22]: timeit Y[:, a], Y[:, b] = swap(Y[:, a], Y[:, b])<br>
10000 loops, best of 3: 111 us per loop<br>&nbsp;[/sourcecode]</p>
<p>We can see that using Fortran-order takes us from the order of
miliseconds to the order of&nbsp;microseconds!</p>
<p>Side note: I almost fell into the trap of swapping columns the pythonic
way. That doesn&#8217;t work:<br>
[sourcecode language=&#8221;python&#8221;]<br>
In [23]: X[:, a], X[:, b] = X[:, b], X[:,&nbsp;a]</p>
<p>In [24]: np.testing.assert_array_equal(X[:, a], X[:,&nbsp;b])</p>
<p>In [25]:<br>&nbsp;[/sourcecode]</p>
<p>However this trick works great for swapping elements of one-dimensional&nbsp;arrays.</p>
<p>Another small optimization that we can do: I found that on my system,
it&#8217;s slightly faster to compute the norm using the <span class="caps">BLAS</span> function <code>nrm2</code>.
So by putting all of these together, we end up with the final version of
our&nbsp;code:</p>
<p>[sourcecode language=&#8221;python&#8221;]<br>
def cholesky_omp(X, y, n_nonzero_coefs, eps=None,
overwrite_X=False):<br>
if not overwrite_X:<br>
X = X.copy(&#8216;F&#8217;)<br>
else: # even if we are allowed to overwrite, still copy it if bad
order<br>
X =&nbsp;np.asfortranarray(X)</p>
<p>min_float = np.finfo(X.dtype).eps<br>
nrm2, swap = linalg.get_blas_funcs((&#8216;nrm2&#8217;, &#8216;swap&#8217;), (X,))<br>
potrs, = get_lapack_funcs((&#8216;potrs&#8217;,),&nbsp;(X,))</p>
<p>indices = range(len(Gram)) # keeping track of swapping<br>
alpha = np.dot(X.T, y)<br>
residual = y<br>
n_active =&nbsp;0</p>
<p>max_features = X.shape<a href="#footnote-1">1</a> if eps is not None else n_nonzero_coefs<br>
L = np.empty((max_features, max_features), dtype=X.dtype)<br>
L[0, 0] =&nbsp;1.</p>
<p>while True:<br>
lam = np.abs(np.dot(X.T, residual)).argmax()<br>
if lam &lt; n_active or alpha[lam] ** 2 &lt; min_float:<br>
# atom already selected or inner product too small<br>
warn(&#8220;Stopping early&#8221;)<br>
break<br>
if n_active &gt; 0:<br>
# Updates the Cholesky decomposition of X&#8217; X<br>
L[n_active, :n_active] = np.dot(X[:, :n_active].T, X[:, lam])<br>
solve_triangular(L[:n_active, :n_active], L[n_active, :n_active])<br>
v = nrm2(L[n_active, :n_active]) ** 2<br>
if 1 - v &lt;= min_float: # selected atoms are dependent<br>
warn(&#8220;Stopping early&#8221;)<br>
break<br>
L[n_active, n_active] = np.sqrt(1 - v)<br>
X.T[n_active], X.T[lam] = swap(X.T[n_active], X.T[lam])<br>
alpha[n_active], alpha[lam] = alpha[lam], alpha[n_active]<br>
indices[n_active], indices[lam] = indices[lam], indices[n_active]<br>
n_active += 1<br>
# solves <span class="caps">LL</span>&#8217;x = y as a composition of two triangular systems<br>
gamma, _ = potrs(L[:n_active, :n_active], alpha[:n_active],
lower=True,<br>&nbsp;overwrite_b=False)</p>
<p>residual = y - np.dot(X[:, :n_active], gamma)<br>
if eps is not None and nrm2(residual) ** 2 &lt;= eps:<br>
break<br>
elif n_active == max_features:<br>&nbsp;break</p>
<p>return gamma, indices[:n_active]<br>&nbsp;[/sourcecode]</p>
<p>Now, the benchmark at [<a href="#footnote-1">1</a>] indicates victory over least-angle
regression! I hope you have enjoyed this short tour. See you next&nbsp;time!</p>
<p><a href="https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_plot_omp_lars.py" title="Orthogonal matching pursuit versus least-angle regression"><span id="footnote-1">1</span></a>[]<br>
<a href="http://scikit-learn.sourceforge.net/dev/developers/performance.html#profiling-python-code" title="Profiling Python code"><span id="footnote-2">2</span></a>[]</p>
    </div><!-- /.entry-content -->
    <div class="comments">
      <h2>Comments !</h2>
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_identifier = "blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-2.html";
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://vene.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
    </div>

  </article>
</section>
        <section id="extras" class="body">
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Go <a href="/">home</a>. <a href="/privacy.html">Privacy policy.</a>
                Powered by <a href="http://getpelican.com/">Pelican</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
        </div>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-47024389-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script type="text/javascript">
    var disqus_shortname = 'vene';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</div>
</body>
</html>