<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Optimizing Orthogonal Matching Pursuit code in Numpy, part 1</title>
  <meta name="author" content="Vlad" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
  integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
  crossorigin="anonymous"></script>
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/main.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/pygment.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/typogrify.css" />
  <link rel="shortcut icon" href="//vene.ro/favicon.ico" />
  <link href="//vene.ro/" type="application/atom+xml"
        rel="alternate" title="Vlad Niculae ALL Atom Feed" />
  <link href="//fonts.googleapis.com/css?family=PT+Mono|PT+Serif" rel="stylesheet"> 

  <meta property="twitter:creator" content=@vnfrombucharest" />
  <!-- OpenGraph Info -->

  <meta name="twitter:card" content="summary"></meta>
  <meta name="description" content="After intense code optimization work, my implementation of OMP finally beat least-angle regression! This was the primary issue discussed during the pull...">
  <meta name="keywords" content="efficient, numpy, omp, orthogonal-matching-pursuit, scipy, dictionary-learning, scikit-learn, python">
  <meta property="og:title" content="Optimizing Orthogonal Matching Pursuit code in Numpy, part&nbsp;1">
  <meta property="og:description" content="After intense code optimization work, my implementation of OMP finally beat least-angle regression! This was the primary issue discussed during the pull...">
  <meta property="og:url" content="//vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1.html">
  <meta property="og:image" content="//vene.ro/">

  <script src="//vene.ro/theme/js/main.js"></script>

</head>

<body>
<div id="container">
<header>
  <nav class="navmenu" id="navmenu">
    <li id="homelink"><a href="/">Vlad Niculae</a>
    </li><li class="menu"><a href="//vene.ro/papers.html">Papers</a>
    </li><li class="menu"><a href="//vene.ro/blog/">Blog</a>
    </li><li class="menu"><a href="//vene.ro/teaching.html">Teaching</a>
   </li>
   </nav>
 </header>
 <div id="main">
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="//vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1.html" rel="bookmark"
           title="Permalink to Optimizing Orthogonal Matching Pursuit code in Numpy, part 1">Optimizing Orthogonal Matching Pursuit code in Numpy, part&nbsp;1</a></h1>
<p class="subtitle"><time datetime="2011-08-07T20:50:00+02:00">Sun, 07 Aug 2011</time><label for="optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1" class="margin-toggle"> ⊕</label><input type="checkbox" id="optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1" class="margin-toggle" /><span class="marginnote">Category: <a href="//vene.ro/category/scikit-learn.html">scikit-learn</a><br />
 #<a href="//vene.ro/tag/efficient.html">efficient</a> #<a href="//vene.ro/tag/numpy.html">numpy</a> #<a href="//vene.ro/tag/omp.html">omp</a> #<a href="//vene.ro/tag/orthogonal-matching-pursuit.html">orthogonal matching pursuit</a> #<a href="//vene.ro/tag/scipy.html">scipy</a> #<a href="//vene.ro/tag/dictionary-learning.html">dictionary learning</a> #<a href="//vene.ro/tag/scikit-learn.html">scikit-learn</a> #<a href="//vene.ro/tag/python.html">python</a></span></p>    </header>

    <div class="entry-content">
      <p>After intense code optimization work, my implementation of <span class="caps">OMP</span> finally
beat least-angle regression! This was the primary issue discussed during
the pull request, so once performance was taken care of, the code was
ready for merge. Orthogonal matching pursuit is now available in
scikits.learn as a sparse linear regression model. <span class="caps">OMP</span> is a key building
block of the dictionary learning code that we are working on&nbsp;merging.</p>
<p>I will go through the process of developing this particular piece of
code as an example of code refining and iterative improvements, as well
as for the useful notes it will provide on optimizing numerical Python
code. In the first part we will see how the code got from pseudocode
state to a reasonably efficient code with smart memory allocation. In
the next part we will see how to make it blazing fast by leveraging
[<a href="#footnote-1">1</a>] lower level <span class="caps">BLAS</span> and <span class="caps">LAPACK</span> routines, and how to use profiling
to find hot&nbsp;spots.</p>
<p>As stated before, orthogonal matching pursuit is a greedy algorithm for
finding a sparse solution [latex] \gamma[/latex] to a linear regression
problem [latex] X\gamma = y[/latex]. Mathematically, it approximates
the solution of the optimization&nbsp;problem:</p>
<p>$$ \text{argmin} {\big|\big|} \gamma {\big|\big|} _0 \text{
subject to }{\big |\big|}y-X\gamma{\big|\big|}_2\^2 \leq
\epsilon $$<br />
or (under a different parametrization):<br />
$$\text{argmin} {\big |\big|}y - X\gamma{\big |\big|}_2\^2
\text{ subject to } {\big|\big|}\gamma{\big|\big|}_0 \leq
n_{\text{nonzero&nbsp;coefs}}$$</p>
<p>In the code samples in this post I will omit the docstrings, but I will
follow the notation in the formulas&nbsp;above.</p>
<p><strong>Important note:</strong> The regressors/dictionary atoms (the columns of
[latex] X[/latex]) are assumed to be normalized throughout this post (as
well as usually any discussion of <span class="caps">OMP</span>). We also assume the following
imports:<br />
[sourcecode language=&#8221;Python&#8221;]<br />
import numpy as np<br />
from scipy import linalg<br />&nbsp;[/sourcecode]</p>
<p>Orthogonal matching pursuit is a very simple algorithm in pseudocode,
and as I stated before, it almost writes itself in Numpy. For this
reason, instead of stating the pseudocode here, I will start with how
naively implemented <span class="caps">OMP</span> looks like in&nbsp;Python:</p>
<p>[sourcecode language=&#8221;Python&#8221;]<br />
def orthogonal_mp(X, y, n_nonzero_coefs, eps=None):<br />
residual = y<br />
idx = []<br />
if eps == None:<br />
stopping_condition = lambda: len(idx) == n_nonzero_coefs<br />
else:<br />
stopping_condition = lambda: np.inner(residual, residual) &lt;= eps<br />
while not stopping_condition():<br />
lam = np.abs(np.dot(residual, X)).argmax()<br />
idx.append(lam)<br />
gamma, _, _, _ = linalg.lstsq(X[:, idx], y)<br />
residual = y - np.dot(X[:, idx], gamma)<br />
return gamma, idx<br />&nbsp;[/sourcecode]</p>
<p>Using lambda expressions as stopping conditions never looked like a
brilliant idea, but it seems to me like the most elegant way to specify
such a variable stopping condition. However, the biggest slowdown in
this is the need for solving a least squares problem at each iteration,
while least-angle regression is known to produce the entire
regularization path for the cost of a single least squares problem. We
will also see that this implementation is more vulnerable to numerical
stability&nbsp;issues.</p>
<p>In [<a href="#footnote-2">2</a>], Rubinstein et al. described the Cholesky-<span class="caps">OMP</span> algorithm, an
implementation of <span class="caps">OMP</span> that avoids solving a new least squares problem at
each iteration by keeping a Cholesky decomposition [latex] <span class="caps">LL</span>&#8217;[/latex]
of the Gram matrix [latex] G =
X_{\text{idx}}&#8217;X_{\text{idx}}[/latex]. Because [latex]
X_{\text{idx}}[/latex] grows by exactly one column at each iteration,
[latex] L[/latex] can be updated according to the following rule: Given
[latex] A = \begin{pmatrix} \tilde{A} <span class="amp">&amp;</span> \mathbf{v}&#8217; &#92; \mathbf{v}
<span class="amp">&amp;</span> c \end{pmatrix}[/latex], and knowing the decomposition of [latex]
\tilde{A} = \tilde{L}\tilde{L}&#8217;[/latex], the Cholesky decomposition
[latex] A = <span class="caps">LL</span>&#8217;[/latex] is given by $$ L = \begin{pmatrix}\tilde{L}
<span class="amp">&amp;</span> \mathbf{0} &#92; \mathbf{w}&#8217; <span class="amp">&amp;</span> \sqrt{c - \mathbf{w}&#8217;\mathbf{w}}
\end{pmatrix}, \text{ where } \tilde{L}\mathbf{w} =&nbsp;\mathbf{v}$$</p>
<p>Even if you are unfamiliar with the mathematical properties of the
Cholesky decomposition, you can see from the construction detailed above
that [latex] L[/latex] is always going to be a lower triangular matrix
(it will only have null elements above the main diagonal). Actually, the
letter L stands for lower. We have therefore replaced the step where we
needed to solve the least-squares problem [latex]
X_{\text{idx}}\gamma = y[/latex] with two much simpler computations:
solving [latex] \tilde{L}\mathbf{w} = \mathbf{v}[/latex] and solving
[latex] <span class="caps">LL</span>&#8217;\gamma = X_{\text{idx}}&#8217;y[/latex]. Due to the [latex]
L[/latex]&#8217;s structure, these are much quicker operations than a least
squares projection.<br />
Here is the initial way I implemented&nbsp;this:</p>
<p>[sourcecode&nbsp;language=&#8221;Python&#8221;]</p>
<p>def cholesky_omp(X, y, n_nonzero_coefs, eps=None):<br />
if eps == None:<br />
stopping_condition = lambda: it == n_nonzero_coefs<br />
else:<br />
stopping_condition = lambda: np.inner(residual, residual) \&lt;=&nbsp;eps</p>
<p>alpha = np.dot(X.T, y)<br />
residual = y<br />
idx = []<br />
L =&nbsp;np.ones((1,1))</p>
<p>while not stopping_condition():<br />
lam = np.abs(np.dot(residual, X)).argmax()<br />
if len(idx) &gt; 0:<br />
w = linalg.solve_triangular(L, np.dot(X[:, idx].T, X[:, lam]),<br />
lower=True)<br />
L = np.r_[np.c_[L, np.zeros(len(L))],<br />
np.atleast_2d(np.append(w, np.sqrt(1 - np.dot(w.T, w))))]<br />
idx.append(lam)<br />
# solve <span class="caps">LL</span>&#8217;x = y in two steps:<br />
Ltx = linalg.solve_triangular(L, alpha[idx], lower=True)<br />
gamma = linalg.solve_triangular(L, Ltx, trans=1, lower=True)<br />
residual = y - np.dot(X[:, idx],&nbsp;gamma)</p>
<p>return gamma, idx<br />&nbsp;[/sourcecode]</p>
<p>Note that a lot of the code remained unchanged, this is the same
algorithm as before, only the Cholesky trick is used to improve
performance. According to the plot in [<a href="#footnote-3">3</a>], we can see that the naive
implementation has oscillations of the reconstruction error due to
numerical instability, while this Cholesky implementation is&nbsp;well-behaved.</p>
<p>Along with this I also implemented the Gram-based version of this
algorithm, which only needs [latex] X&#8217;X[/latex] and [latex] X&#8217;y[/latex]
(and [latex] {\big|\big|}y{\big|\big|}_2\^2[/latex], in case the
epsilon-parametrization is desired). This is called <strong>Batch <span class="caps">OMP</span></strong> in
[<a href="#footnote-2">2</a>], because it offers speed gains when many signals need to be
sparse coded against the same dictionary [latex] X[/latex]. A lot of
speed is gained because two large matrix multiplications are avoided at
each iteration, but for many datasets, the cost of the precomputations
dominates the procedure. I will not insist on Gram <span class="caps">OMP</span> in this post, it
can be found in the <code>scikits.learn</code> repository [<a href="#footnote-4">4</a>].</p>
<p>Now, the problems with this are a bit more subtle. At this point, I
moved on to code other things, since <span class="caps">OMP</span> was passing tests and the
signal recovery example was working. The following issues popped up
during&nbsp;review:</p>
<p>​1. The lambda stopping condition does not pickle.<br />
2. For well-constructed signals and data matrices, assuming normal
atoms, [latex] \mathbf{w}[/latex] on line 14 will never have norm
greater than or equal to zero, unless the chosen feature happens to be
dependent of the already chosen set. In theory, this cannot happen,
since we do an orthogonal projection at each step. However, if the
matrix [latex] X[/latex] is not well-behaved (for example, if it has two
identical columns, and [latex] y[/latex] is built using non-zero
coefficients for those columns), then we end up with the square root of
a negative value on line 17.<br />
3. It was orders of magnitude slower than least-angle regression, given
the same number of nonzero&nbsp;coefficients.</p>
<p>1 was an easy fix. 2 was a bit tricky since it was a little hidden: the
first time I encountered such an error, I wrongfully assumed that given
that the diagonal of [latex] X_\text{idx}&#8217;X_\text{idx}[/latex] was
unit, then [latex] L[/latex] should also have a unit diagonal, so I
passed the parameter <code>unit_diagonal=True</code> to <code>linalg.solve_triangular</code>,
and the plethora of NaN&#8217;s along the diagonal were simply ignored. Let
this show what happens when you don&#8217;t pay attention when&nbsp;coding.</p>
<p>When I realized my mistake, I first did something I saw in <code>lars_path</code>
from the scikit: take the absolute value of the argument of <code>sqrt</code>, and
also ensure it is practically larger than zero. However, tests started
failing randomly. Confusion ensued until the nature of the issue,
discussed above, was discovered. It&#8217;s just not right to take the <code>abs</code>:
if that argument ends up less than zero, <span class="caps">OMP</span> simply cannot proceed and
must stop due to malformed data. The reference implementation from the
website of the authors of [<a href="#footnote-2">2</a>] includes explicit <em>early stopping</em>
conditions for this, along with some other&nbsp;cases.</p>
<p>At the same time, I started to try a couple of optimizations. The most
obvious thing was the way I was building the matrix [latex] L[/latex]
was clearly suboptimal, reallocating it at each&nbsp;iteration.</p>
<p>This leads to the following&nbsp;code:</p>
<p>[sourcecode&nbsp;language=&#8221;Python&#8221;]</p>
<p>def cholesky_omp(X, y, n_nonzero_coefs, eps=None):<br />
min_float = np.finfo(X.dtype).eps<br />
alpha = np.dot(X.T, y)<br />
residual = y<br />
n_active = 0<br />
idx =&nbsp;[]</p>
<p>max_features = X.shape<a href="#footnote-1">1</a> if eps is not None else n_nonzero_coefs<br />
L = np.empty((max_features, max_features), dtype=X.dtype)<br />
L[0, 0] =&nbsp;1.</p>
<p>while 1:<br />
lam = np.abs(np.dot(X.T, residual)).argmax()<br />
if lam \&lt; n_active or alpha[lam] ** 2 > min_float:<br />
# atom already selected or inner product too small<br />
warn(&#8220;Stopping early&#8221;)<br />
break<br />
if n_active &gt; 0:<br />
# Updates the Cholesky decomposition of X&#8217; X<br />
w = linalg.solve_triangular(L[:n_active, :n_active],<br />
np.dot(X[:, idx].T, X[:, lam]),<br />
lower=True)<br />
L[n_active, :n_active] = w<br />
d = np.dot(w.T, w)<br />
if 1 - d &lt;= min_float: # selected atoms are dependent<br />
warn(&#8220;Stopping early&#8221;)<br />
break<br />
L[n_active, n_active] = np.sqrt(1 - d)<br />
idx.append(lam)<br />
# solve <span class="caps">LL</span>&#8217;x = y in two steps:<br />
Ltx = linalg.solve_triangular(L[:n_active, :n_active], alpha[idx],
lower=True)<br />
gamma = linalg.solve_triangular(L[:n_active, :n_active], Ltx,
trans=1, lower=True)<br />
residual = y - np.dot(X[:, idx], gamma)<br />
if eps is not None and np.dot(residual.T, residual) &lt;= eps:<br />
break<br />
elif n_active == max_features:<br />
break<br />
return gamma, idx<br />&nbsp;[/sourcecode]</p>
<p>What should be noted here, apart from the obvious fix for #1, are the
early stopping conditions. It is natural to stop if the same feature
gets picked twice: the residual is always orthogonalized with respect to
the chosen basis, so the only way this could happen is if there would be
no more unused independent regressors. This would either lead to this,
or to the stopping criterion on line 25, depending on which equally
insignificant vector gets picked. The other criterion for early stopping
is if the chosen atom is orthogonal to y, which would make it
uninformative and would again mean that there are no better ones left,
so we might as well quit&nbsp;looking.</p>
<p>Also, we now make sure that [latex] L[/latex] is preallocated. Note that
<code>np.empty</code> is marginally faster than <code>np.zeros</code> because it does not
initialize the array to zero after allocating, so the untouched parts of
the array will contain whatever happened to be in memory before. In our
case, this means only the values above the main diagonal: everything on
and beneath is initialized before access. Luckily, the
<code>linalg.solve_triangular</code> function ignores what it doesn&#8217;t&nbsp;need.</p>
<p>This is a robust implementation, but still a couple of times slower than
least-angle regression. In the next part of the article we will see how
we can make it beat <span class="caps">LARS</span>.</p>
<p><span id="footnote-1"><a href="#footnote-1">1</a></span> I always wanted to use this word in a
serious context :P<br />
<span id="footnote-2"><a href="#footnote-2">2</a></span> Rubinstein, R., Zibulevsky, M. and
Elad, M., [Efficient Implementation of the K-<span class="caps">SVD</span> Algorithm using Batch
Orthogonal Matching Pursuit][] Technical Report - <span class="caps">CS</span> Technion, April
2008.<br />
<span id="footnote-3"><a href="#footnote-3">3</a></span> [First thoughts on Orthogonal
Matching Pursuit][] on this blog.<br />
<span id="footnote-4"><a href="#footnote-4">4</a></span> <a href="https://github.com/scikit-learn/scikit-learn/blob/master/scikits/learn/linear_model/omp.py">omp.py</a> on&nbsp;github.</p>
<p>[Efficient Implementation of the K-<span class="caps">SVD</span> Algorithm using Batch
  Orthogonal Matching Pursuit]: http://www.cs.technion.ac.il/~ronrubin/Publications/<span class="caps">KSVD</span>-<span class="caps">OMP</span>-v2.pdf</p>
    </div><!-- /.entry-content -->
    <div class="comments">
      <h2>Comments !</h2>
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_shortname = 'vene';
        var disqus_identifier = 'blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1.html';
        var disqus_url = 'https://vene.ro/blog/optimizing-orthogonal-matching-pursuit-code-in-numpy-part-1.html';
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://vene.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>

  </article>
</section>
 </div>
<footer>
  <p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>.
  <a href="/privacy.html">Privacy policy</a>.</p>
</footer>
</div>
</body>
</html>