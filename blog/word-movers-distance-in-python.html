<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Word Mover’s Distance in Python</title>
  <meta name="author" content="Vlad" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
  integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
  crossorigin="anonymous"></script>
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/main.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/pygment.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/typogrify.css" />
  <link rel="shortcut icon" href="//vene.ro/favicon.ico" />
  <link href="//vene.ro/" type="application/atom+xml"
        rel="alternate" title="Vlad Niculae ALL Atom Feed" />
  <link href="//fonts.googleapis.com/css?family=PT+Mono|PT+Serif" rel="stylesheet"> 

  <meta property="og:type" content="website" />
  <meta property="twitter:creator" content=@vnfrombucharest" />
  <meta name="twitter:card" content="summary">

  <!-- OpenGraph Info -->

  <meta name="description" content="In document classification and other natural language processing applications, having a good measure of the similarity of two texts can be a valuable..." />
  <meta name="keywords" content="word-embeddings, text-classification, earth-movers-distance" />
  <meta property="og:title" content="Word Mover&#8217;s Distance in&nbsp;Python" />
  <meta property="og:description" content="In document classification and other natural language processing applications, having a good measure of the similarity of two texts can be a valuable..." />
  <meta property="og:url" content="https://vene.ro/blog/word-movers-distance-in-python.html" />

  <meta name="twitter:card" content="summary_large_image" />
  <meta property="og:image" content="https://vene.ro///vene.ro/https://vene.ro/images/wmd-obama.png" />
  <meta property="og:image:alt" content="" />


  <script src="//vene.ro/theme/js/main.js"></script>

</head>

<body>
<div id="container">
<header>
  <nav class="navmenu" id="navmenu">
    <li id="homelink"><a href="/">Vlad Niculae</a>
    </li><li class="menu"><a href="//vene.ro/papers.html">Research</a>
    </li><li class="menu"><a href="//vene.ro/blog/">Blog</a>
    </li><li class="menu"><a href="//vene.ro/teaching.html">Teaching</a>
    </li><li class="menu"><a href="//vene.ro/students.html">Students</a>
   </li>
   </nav>
 </header>
 <div id="main">
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="//vene.ro/blog/word-movers-distance-in-python.html" rel="bookmark"
           title="Permalink to Word Mover’s Distance in Python">Word Mover&#8217;s Distance in&nbsp;Python</a></h1>
<p class="subtitle"><time datetime="2015-11-07T12:00:00+01:00">Sat, 07 Nov 2015</time><label for="word-movers-distance-in-python" class="margin-toggle"> ⊕</label><input type="checkbox" id="word-movers-distance-in-python" class="margin-toggle" /><span class="marginnote">Category: <a href="//vene.ro/category/python.html">python</a><br />
 #<a href="//vene.ro/tag/word-embeddings.html">word embeddings</a> #<a href="//vene.ro/tag/text-classification.html">text classification</a> #<a href="//vene.ro/tag/earth-movers-distance.html">earth mover's distance</a></span></p>    </header>

    <div class="entry-content">
      <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>A guide to scikit-learn compatible nearest neighbors classification using the recently introduced word mover&#8217;s distance (<span class="caps">WMD</span>). </em>
Joint post with the awesome <a href="http://matthewkusner.com">Matt Kusner</a>!</p>
<p><a href="http://nbviewer.jupyter.org/github/vene/vene.github.io/blob/pelican/content/blog/word-movers-distance-in-python.ipynb">Source of this Jupyter&nbsp;notebook.</a></p>
<p>In document classification and other natural language processing applications, having a good measure of the similarity of two texts can be a valuable building block.   Ideally, such a measure would capture semantic information.  Cosine similarity on bag-of-words vectors is known to do well in practice, but it inherently cannot capture when documents say the same thing in completely different&nbsp;words.</p>
<p>Take, for example, two&nbsp;headlines:</p>
<ul>
<li><em>Obama speaks to the media in&nbsp;Illinois</em></li>
<li><em>The President greets the press in&nbsp;Chicago</em></li>
</ul>
<p>These have no content words in common, so according to most bag of words&#8212;based metrics, their distance would be maximal.  (For such applications, you probably don&#8217;t want to count stopwords such as <em>the</em> and <em>in</em>, which don&#8217;t truly signal semantic&nbsp;similarity.)</p>
<p>One way out of this conundrum is the word mover&#8217;s distance (<span class="caps">WMD</span>), introduced in 
<a href="http://mkusner.github.io/publications/WMD.pdf"><em>From Word Embeddings To Document Distances</em></a>,
(Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger, <span class="caps">ICML</span> 2015).
<span class="caps">WMD</span> adapts the <a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance">earth mover&#8217;s distance</a> to the space of documents: the distance between two texts is given by the total amount of &#8220;mass&#8221; needed to move the words from one side into the other, multiplied by the distance the words need to move. So, starting from a measure of the distance between different words, we can get a principled document-level distance. Here is a visualisation of the idea, from the <span class="caps">ICML</span>&nbsp;slides:</p>
<p><img src="https://vene.ro/images/wmd-obama.png" alt="WMD example from Matt&#39;s slides"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Prepare-some-word-embeddings">Prepare some word embeddings<a class="anchor-link" href="#Prepare-some-word-embeddings">&#182;</a></h2><p>The key ingredient in <span class="caps">WMD</span> is a good distance measure between words.  Dense representations of words, also known by the trendier name &#8220;word embeddings&#8221; (because &#8220;distributed word representations&#8221; didn&#8217;t stick), do the trick here.  We could train the embeddings ourselves, but for meaningful results we would need tons of documents, and that might take a while. So let&#8217;s just use the ones from the <a href="https://code.google.com/p/word2vec/"><code>word2vec</code></a> team. <a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing">(download&nbsp;link)</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;data/embed.dat&quot;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Caching word embeddings in memmapped format...&quot;</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">gensim.models.word2vec</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
    <span class="n">wv</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span>
        <span class="s2">&quot;data/GoogleNews-vectors-negative300.bin.gz&quot;</span><span class="p">,</span>
        <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">fp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="s2">&quot;data/embed.dat&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w+&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0norm</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">fp</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">syn0norm</span><span class="p">[:]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;data/embed.vocab&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">((</span><span class="n">voc</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">voc</span> <span class="ow">in</span> <span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">fp</span><span class="p">,</span> <span class="n">wv</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="s2">&quot;data/embed.dat&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3000000</span><span class="p">,</span> <span class="mi">300</span><span class="p">))</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;data/embed.vocab&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">vocab_list</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="o">.</span><span class="n">strip</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vocab_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">)}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reproducing-the-demo-above">Reproducing the demo above<a class="anchor-link" href="#Reproducing-the-demo-above">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">d1</span> <span class="o">=</span> <span class="s2">&quot;Obama speaks to the media in Illinois&quot;</span>
<span class="n">d2</span> <span class="o">=</span> <span class="s2">&quot;The President addresses the press in Chicago&quot;</span>

<span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features:&quot;</span><span class="p">,</span>  <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Features: addresses, chicago, illinois, media, obama, president, press, speaks
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The two documents are completely orthogonal in terms of&nbsp;bag-of-words</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[71]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cosine</span>
<span class="n">v_1</span><span class="p">,</span> <span class="n">v_2</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">])</span>
<span class="n">v_1</span> <span class="o">=</span> <span class="n">v_1</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">v_2</span> <span class="o">=</span> <span class="n">v_2</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">v_1</span><span class="p">,</span> <span class="n">v_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;cosine(doc_1, doc_2) = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cosine</span><span class="p">(</span><span class="n">v_1</span><span class="p">,</span> <span class="n">v_2</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[0 0 1 1 1 0 0 1] [1 1 0 0 0 1 1 0]
cosine(doc_1, doc_2) = 1.00
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[58]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="n">W_</span> <span class="o">=</span> <span class="n">W</span><span class="p">[[</span><span class="n">vocab_dict</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()]]</span>
<span class="n">D_</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">W_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;d(addresses, speaks) = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">D_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;d(addresses, chicago) = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">D_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>d(addresses, speaks) = 1.16
d(addresses, chicago) = 1.37
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will be using <a href="https://github.com/wmayner/pyemd"><code>pyemd</code></a>, a Python wrapper for <a href="http://www.ariel.ac.il/sites/ofirpele/fastemd/">Pele and Werman&#8217;s implementation of the earth mover&#8217;s distance</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[74]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pyemd</span> <span class="kn">import</span> <span class="n">emd</span>

<span class="c1"># pyemd needs double precision input</span>
<span class="n">v_1</span> <span class="o">=</span> <span class="n">v_1</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">v_2</span> <span class="o">=</span> <span class="n">v_2</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">v_1</span> <span class="o">/=</span> <span class="n">v_1</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">v_2</span> <span class="o">/=</span> <span class="n">v_2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">D_</span> <span class="o">=</span> <span class="n">D_</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">D_</span> <span class="o">/=</span> <span class="n">D_</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>  <span class="c1"># just for comparison purposes</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;d(doc_1, doc_2) = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">emd</span><span class="p">(</span><span class="n">v_1</span><span class="p">,</span> <span class="n">v_2</span><span class="p">,</span> <span class="n">D_</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>d(doc_1, doc_2) = 0.74
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Document-classification">Document classification<a class="anchor-link" href="#Document-classification">&#182;</a></h2><p>We will use the <a href="http://qwone.com/~jason/20Newsgroups/"><em>20 Newsgroups</em></a> classification task.  Because <span class="caps">WMD</span> is an expensive computation, for this demo we just use a subset.  To emphasize the power of the method, we use a larger test size, but train on relatively few&nbsp;samples.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">newsgroups</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">()</span>
<span class="n">docs</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">newsgroups</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">newsgroups</span><span class="o">.</span><span class="n">target</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">docs_train</span><span class="p">,</span> <span class="n">docs_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                                          <span class="n">train_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                                          <span class="n">test_size</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
                                                          <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the <code>W</code> embedding array is pretty huge, we might as well restrict it to just the words that actually occur in the&nbsp;dataset.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs_train</span> <span class="o">+</span> <span class="n">docs_test</span><span class="p">)</span>
<span class="n">common</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab_dict</span><span class="p">]</span>
<span class="n">W_common</span> <span class="o">=</span> <span class="n">W</span><span class="p">[[</span><span class="n">vocab_dict</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">common</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can then create a fixed-vocabulary vectorizer using only the words we have embeddings&nbsp;for.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">=</span><span class="n">common</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One way to proceed is to just pre-compute the pairwise distances between all documents, and use them to search for hyperparameters and evaluate the model. However, that would incur some extra computation, and <span class="caps">WMD</span> is expensive. Also, it&#8217;s not the most pleasant user interface. So we define some scikit-learn compatible estimators for computing the <span class="caps">WMD</span>.</p>
<p><strong><code>WordMoversKNN</code></strong> subclasses from <code>KNeighborsClassifier</code> and overrides the <code>predict</code> function to compute the <span class="caps">WMD</span> between all training and test&nbsp;samples.</p>
<p>In practice, however, we often don&#8217;t know what is the best <code>n_neighbors</code> to use.  Simply wrapping <code>WordMoversKNN</code> in a <code>GridSearchCV</code> would be rather expensive because of all the distances that would need to be recomputed for every value of <code>n_neighbors</code>. So we introduce <strong><code>WordMoversKNNCV</code></strong>, which, when fitted, performs <em>cross-validation</em> to find the best value of <code>n_neighbors</code> (under any given evaluation metric), while only computing the <span class="caps">WMD</span> once per fold, and only across folds (saving <code>n_folds * fold_size ** 2</code> evaluations).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;%%file word_movers_knn.py&quot;&quot;&quot;</span>

<span class="c1"># Authors: Vlad Niculae, Matt Kusner</span>
<span class="c1"># License: Simplified BSD</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="kn">from</span> <span class="nn">sklearn.externals.joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">check_cv</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.scorer</span> <span class="kn">import</span> <span class="n">check_scoring</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">normalize</span>

<span class="kn">from</span> <span class="nn">pyemd</span> <span class="kn">import</span> <span class="n">emd</span>


<span class="k">class</span> <span class="nc">WordMoversKNN</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;K nearest neighbors classifier using the Word Mover&#39;s Distance.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    </span>
<span class="sd">    W_embed : array, shape: (vocab_size, embed_size)</span>
<span class="sd">        Precomputed word embeddings between vocabulary items.</span>
<span class="sd">        Row indices should correspond to the columns in the bag-of-words input.</span>

<span class="sd">    n_neighbors : int, optional (default = 5)</span>
<span class="sd">        Number of neighbors to use by default for :meth:`k_neighbors` queries.</span>

<span class="sd">    n_jobs : int, optional (default = 1)</span>
<span class="sd">        The number of parallel jobs to run for Word Mover&#39;s Distance computation.</span>
<span class="sd">        If ``-1``, then the number of jobs is set to the number of CPU cores.</span>
<span class="sd">    </span>
<span class="sd">    verbose : int, optional</span>
<span class="sd">        Controls the verbosity; the higher, the more messages. Defaults to 0.</span>
<span class="sd">        </span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    </span>
<span class="sd">    Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger</span>
<span class="sd">    From Word Embeddings To Document Distances</span>
<span class="sd">    The International Conference on Machine Learning (ICML), 2015</span>
<span class="sd">    http://mkusner.github.io/publications/WMD.pdf</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_pairwise</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W_embed</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_embed</span> <span class="o">=</span> <span class="n">W_embed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WordMoversKNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
                                            <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;brute&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_wmd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the WMD between training sample i and given test row.</span>
<span class="sd">        </span>
<span class="sd">        Assumes that `row` and train samples are sparse BOW vectors summing to 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">union_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">union1d</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>
        <span class="n">W_minimal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_embed</span><span class="p">[</span><span class="n">union_idx</span><span class="p">]</span>
        <span class="n">W_dist</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">W_minimal</span><span class="p">)</span>
        <span class="n">bow_i</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">union_idx</span><span class="p">]</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">bow_j</span> <span class="o">=</span> <span class="n">row</span><span class="p">[:,</span> <span class="n">union_idx</span><span class="p">]</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">emd</span><span class="p">(</span><span class="n">bow_i</span><span class="p">,</span> <span class="n">bow_j</span><span class="p">,</span> <span class="n">W_dist</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_wmd_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Wrapper to compute the WMD of a row with all training samples.</span>
<span class="sd">        </span>
<span class="sd">        Assumes that `row` and train samples are sparse BOW vectors summing to 1.</span>
<span class="sd">        Useful for parallelization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_wmd</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples_train</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">_pairwise_wmd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the word mover&#39;s distance between all train and test points.</span>
<span class="sd">        </span>
<span class="sd">        Parallelized over rows of X_test.</span>
<span class="sd">        </span>
<span class="sd">        Assumes that train and test samples are sparse BOW vectors summing to 1.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X_test: scipy.sparse matrix, shape: (n_test_samples, vocab_size)</span>
<span class="sd">            Test samples.</span>
<span class="sd">        </span>
<span class="sd">        X_train: scipy.sparse matrix, shape: (n_train_samples, vocab_size)</span>
<span class="sd">            Training samples. If `None`, uses the samples the estimator was fit with.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dist : array, shape: (n_test_samples, n_train_samples)</span>
<span class="sd">            Distances between all test samples and all train samples.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="n">X_train</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">X_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_X</span>

        <span class="n">dist</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_wmd_row</span><span class="p">)(</span><span class="n">test_sample</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">test_sample</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the model using X as training data and y as target values</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : scipy sparse matrix, shape: (n_samples, n_features)</span>
<span class="sd">            Training data. </span>

<span class="sd">        y : {array-like, sparse matrix}</span>
<span class="sd">            Target values of shape = [n_samples] or [n_samples, n_outputs]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">WordMoversKNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict the class labels for the provided data</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : scipy.sparse matrix, shape (n_test_samples, vocab_size)</span>
<span class="sd">            Test samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape [n_samples]</span>
<span class="sd">            Class labels for each data sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pairwise_wmd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">WordMoversKNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
    
    
<span class="k">class</span> <span class="nc">WordMoversKNNCV</span><span class="p">(</span><span class="n">WordMoversKNN</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Cross-validated KNN classifier using the Word Mover&#39;s Distance.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    W_embed : array, shape: (vocab_size, embed_size)</span>
<span class="sd">        Precomputed word embeddings between vocabulary items.</span>
<span class="sd">        Row indices should correspond to the columns in the bag-of-words input.</span>

<span class="sd">    n_neighbors_try : sequence, optional</span>
<span class="sd">        List of ``n_neighbors`` values to try.</span>
<span class="sd">        If None, tries 1-5 neighbors.</span>

<span class="sd">    scoring : string, callable or None, optional, default: None</span>
<span class="sd">        A string (see model evaluation documentation) or</span>
<span class="sd">        a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``.</span>

<span class="sd">    cv : int, cross-validation generator or an iterable, optional</span>
<span class="sd">        Determines the cross-validation splitting strategy.</span>
<span class="sd">        Possible inputs for cv are:</span>
<span class="sd">          - None, to use the default 3-fold cross-validation,</span>
<span class="sd">          - integer, to specify the number of folds.</span>
<span class="sd">          - An object to be used as a cross-validation generator.</span>
<span class="sd">          - An iterable yielding train/test splits.</span>
<span class="sd">        For integer/None inputs, StratifiedKFold is used.</span>

<span class="sd">    n_jobs : int, optional (default = 1)</span>
<span class="sd">        The number of parallel jobs to run for Word Mover&#39;s Distance computation.</span>
<span class="sd">        If ``-1``, then the number of jobs is set to the number of CPU cores.</span>

<span class="sd">    verbose : int, optional</span>
<span class="sd">        Controls the verbosity; the higher, the more messages. Defaults to 0.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    cv_scores_ : array, shape (n_folds, len(n_neighbors_try))</span>
<span class="sd">        Test set scores for each fold.</span>

<span class="sd">    n_neighbors_ : int,</span>
<span class="sd">        The best `n_neighbors` value found.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger</span>
<span class="sd">    From Word Embeddings To Document Distances</span>
<span class="sd">    The International Conference on Machine Learning (ICML), 2015</span>
<span class="sd">    http://mkusner.github.io/publications/WMD.pdf</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W_embed</span><span class="p">,</span> <span class="n">n_neighbors_try</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_neighbors_try</span> <span class="o">=</span> <span class="n">n_neighbors_try</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WordMoversKNNCV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">W_embed</span><span class="p">,</span>
                                              <span class="n">n_neighbors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                              <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
                                              <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit KNN model by choosing the best `n_neighbors`.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        X : scipy.sparse matrix, (n_samples, vocab_size)</span>
<span class="sd">            Data</span>
<span class="sd">        y : ndarray, shape (n_samples,) or (n_samples, n_targets)</span>
<span class="sd">            Target</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_neighbors_try</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_neighbors_try</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_neighbors_try</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_neighbors_try</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;brute&#39;</span><span class="p">)</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">check_scoring</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">train_ix</span><span class="p">,</span> <span class="n">test_ix</span> <span class="ow">in</span> <span class="n">cv</span><span class="p">:</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pairwise_wmd</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test_ix</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">train_ix</span><span class="p">])</span>
            <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train_ix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train_ix</span><span class="p">])</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">([</span>
                <span class="n">scorer</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">dist</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">test_ix</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">n_neighbors_try</span>
            <span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv_scores_</span> <span class="o">=</span> <span class="n">scores</span>

        <span class="n">best_k_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">best_k</span> <span class="o">=</span> <span class="n">n_neighbors_try</span><span class="p">[</span><span class="n">best_k_ix</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_neighbors_</span> <span class="o">=</span> <span class="n">best_k</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">WordMoversKNNCV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Overwriting word_movers_knn.py
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">knn_cv</span> <span class="o">=</span> <span class="n">WordMoversKNNCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                         <span class="n">n_neighbors_try</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
                         <span class="n">W_embed</span><span class="o">=</span><span class="n">W_common</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">knn_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   30.8s
[Parallel(n_jobs=3)]: Done  34 out of  34 | elapsed:  2.0min finished
[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   25.7s
[Parallel(n_jobs=3)]: Done  33 out of  33 | elapsed:  2.9min finished
[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   53.3s
[Parallel(n_jobs=3)]: Done  33 out of  33 | elapsed:  2.0min finished
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[10]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>WordMoversKNNCV(W_embed=memmap([[ 0.04283, -0.01124, ..., -0.05679, -0.00763],
       [ 0.02884, -0.05923, ..., -0.04744,  0.06698],
       ...,
       [ 0.08428, -0.15534, ..., -0.01413,  0.04561],
       [-0.02052,  0.08666, ...,  0.03659,  0.10445]]),
        cv=3, n_jobs=3, n_neighbors_try=range(1, 20), scoring=None,
        verbose=5)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CV score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">knn_cv</span><span class="o">.</span><span class="n">cv_scores_</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>CV score: 0.38
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">knn_cv</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   32.2s
[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  4.3min
[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed: 12.5min
[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed: 30.5min
[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed: 48.9min finished
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Test score: 0.31
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Comparison-with-other-models">Comparison with other models<a class="anchor-link" href="#Comparison-with-other-models">&#182;</a></h2><p>Now let&#8217;s see how <span class="caps">WMD</span> compares with some common approaches, on bag of words features.  The most apples-to-apples comparison would be
K nearest neighbors with a cosine similarity metric. This approach performs worse than using <span class="caps">WMD</span>. (All scores are&nbsp;accuracies.)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">knn_grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;brute&#39;</span><span class="p">),</span>
                        <span class="nb">dict</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">))),</span>
                        <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">knn_grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CV score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">knn_grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">knn_grid</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>CV score: 0.34
Test score: 0.22
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another common method for text classification is the linear support vector machine on bag of words.
This performs a bit better than vanilla cosine <span class="caps">KNN</span>, but worse than using <span class="caps">WMD</span> in this setting.  In our experience,
this seems to depend on the amount of training data&nbsp;available.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">svc_grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LinearSVC</span><span class="p">(),</span>
                        <span class="nb">dict</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">)),</span>
                        <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">svc_grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CV score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_grid</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>CV score: 0.35
Test score: 0.27
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-have-we-learned?">What have we learned?<a class="anchor-link" href="#What-have-we-learned?">&#182;</a></h2><p><span class="caps">WMD</span> is much better at capturing semantic similarity between documents than cosine, due to its ability to generalize to unseen words.  The <span class="caps">SVM</span> does somewhat better than cosine <span class="caps">KNN</span>, but still lacks such out-of-vocabulary generalization.   Given enough data, <span class="caps">WMD</span> can probably improve this margin, especially using something like metric learning on&nbsp;top.</p>
<p>The exact <span class="caps">WMD</span>, as we have used it here, is pretty slow.  This code is not optimized as much as it could be, there is potential through caching and using Cython.
However, a major limitation remains the cost of actually computing the <span class="caps">EMD</span>. To scale even higher, exactness can be relaxed by using lower bounds. In our next post, we will compare such optimization strategies, as discussed in <a href="http://mkusner.github.io/publications/WMD.pdf">the <span class="caps">WMD</span> paper</a>.</p>

</div>
</div>
</div>
 


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

    </div><!-- /.entry-content -->
    <div class="comments">
      <h2>Comments !</h2>
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_shortname = 'vene';
        var disqus_identifier = 'blog/word-movers-distance-in-python.html';
        var disqus_url = 'https://vene.ro/blog/word-movers-distance-in-python.html';
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://vene.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>

  </article>
</section>
 </div>
<footer>
  <p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>.
  <a href="/privacy.html">Privacy policy</a>.</p>
</footer>
</div>
</body>
</html>