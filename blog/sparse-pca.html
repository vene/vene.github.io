<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sparse PCA</title>
  <meta name="author" content="Vlad" />
  <base href="//vene.ro">
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/main.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/pygment.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/typogrify.css" />
  <link rel="shortcut icon" href="//vene.ro/favicon.ico" />
  <link href="//vene.ro/" type="application/atom+xml"
        rel="alternate" title="Vlad Niculae ALL Atom Feed" />
  <link href="//fonts.googleapis.com/css?family=PT+Mono|PT+Serif" rel="stylesheet"> 

  <!-- OpenGraph Info -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
      },
      displayAlign: 'left',
      TeX: {
        Macros: {
          RR: "{\\mathbb{R}}",
          argmin: "{\\mathop{\\mathrm{arg\\,min}}}",
          bold: ["{\\bf #1}",1]
        }
        },
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  </script>

  <script type="text/javascript" async
          src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

</head>

<body>
<div id="container">
<header>
  <nav class="navmenu" id="navmenu">
    <li id="homelink"><a href="/">Vlad Niculae</a>
    </li><li class="menu"><a href="//vene.ro/papers.html">Papers</a>
    </li><li class="menu"><a href="//vene.ro/blog/">Blog</a>
    </li><li class="menu"><a href="//vene.ro/teaching.html">Teaching</a>
   </li>
   </nav>
 </header>
 <div id="main">
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="//vene.ro/blog/sparse-pca.html" rel="bookmark"
           title="Permalink to Sparse PCA">Sparse <span class="caps">PCA</span></a></h1>
<p class="subtitle"><time datetime="2011-05-23T15:19:00+02:00">seg 23 maio 2011</time><label for="sparse-pca" class="margin-toggle"> ⊕</label><input type="checkbox" id="sparse-pca" class="margin-toggle" /><span class="marginnote">Category: <a href="//vene.ro/category/scikit-learn.html">scikit-learn</a><br />
 #<a href="//vene.ro/tag/dictionary-learning.html">dictionary learning</a> #<a href="//vene.ro/tag/pca.html">pca</a> #<a href="//vene.ro/tag/sparse-pca.html">sparse pca</a> #<a href="//vene.ro/tag/sparsepca.html">SparsePCA</a> #<a href="//vene.ro/tag/spca.html">spca</a> #<a href="//vene.ro/tag/scikit-learn.html">scikit-learn</a></span></p>    </header>

    <div class="entry-content">
      <p>I have been working on the integration into the scikits.learn codebase
of a sparse principal components analysis (SparsePCA) algorithm coded by
Gaël and Alexandre and based on [[1]][]. Because the name &#8220;sparse <span class="caps">PCA</span>&#8221;
has some inherent ambiguity, I will describe in greater depth what
problem we are actually solving, and what it can be used&nbsp;for.</p>
<h1>The&nbsp;problem</h1>
<p>Mathematically, this implementation of Sparse <span class="caps">PCA</span>&nbsp;solves:</p>
<p>\$latex (U\^*,
V\^*)=\underset{U,V}{\mathrm{argmin\,}}\frac{1}{2}||X-<span class="caps">UV</span>||_2\^2+\alpha||V||_1\$</p>
<p>with \$latex || U_k ||_2 = 1\$ for all \$latex 0 \leq k \&lt;&nbsp;n_{atoms}\$</p>
<p>This looks really abstract so let&#8217;s try to interpret it. We are looking
for a matrix factorization \$latex <span class="caps">UV</span>\$ of \$latex X \in
\mathbf{R}\^{n_{samples}\times n_{features}}\$, just like in
ordinary <span class="caps">PCA</span>. The interpretation is that the \$latex n_{atoms}\$ lines
of \$latex V\$ are the extracted components, while the lines of \$latex
U\$ are the coordinates of the samples in this&nbsp;projection.</p>
<p>The most important difference between this and <span class="caps">PCA</span> is that we enforce
sparsity on the <em>components</em>. In other words, we look for a
representation of the data as a linear combination of sparse&nbsp;signals.</p>
<p>Another difference is that, unlike in <span class="caps">PCA</span>, here we don&#8217;t constrain U to
be orthogonal, just to consist of normalized column vectors. There are
different approaches where this constraint appears too, and they are on
the list for this summer, but I&nbsp;digress.</p>
<h1>The&nbsp;approach</h1>
<p>As usual, such optimization problems are solved by alternatively
minimizing one of the variables while keeping the other fixed, until
convergence is&nbsp;reached.</p>
<p>The update of \$latex V\$ (the dictionary) is computed as the solution
of a Lasso least squares problem.  We allow the user to choose between
the least angle regression method (<span class="caps">LARS</span>) or stochastic gradient descent
as algorithms to solve the Lasso&nbsp;problem.</p>
<p>The update of \$latex U\$ is block coordinate descent with warm restart.
This is a batch adaptation of an online algorithm proposed by Mairal et
al. in&nbsp;[[1]][].</p>
<h1>Sparse <span class="caps">PCA</span> as a&nbsp;transformer</h1>
<p>Of course, in order to be of practical use, the code needs to be
refactored into a scikits.learn transformer object, just like
<code>scikits.learn.decomposition.pca</code>. This means that the optimization
problem described above corresponds to the fitting stage. The post-fit
state of the transformer is given by the learned components (the matrix
\$latex V\$&nbsp;above).</p>
<p>In order to transform new data according to the learned sparse <span class="caps">PCA</span> model
(for example, prior to classification of the test data), we simply need
to do a least squares projection of the new data on the sparse&nbsp;components.</p>
<h1>What is it good&nbsp;for?</h1>
<p>For applications such as text and image processing, its great advantage
is interpretability. When running a regular <span class="caps">PCA</span> on a set of documents in
bag of words format, we can find an interesting visualisation on a
couple of components, and it can show discrimination or clusters. The
biggest problem is that the maximum variance components found by <span class="caps">PCA</span>
have very dense expressions as linear combinations of the initial
features. In practice, sometimes interpretation is made by simply
marking the \$latex k\$ variables with the highest coefficients in this
representation, and basically interpreting as if the rest are truncated
to 0 (this has been taught to me in a class on <span class="caps">PCA</span>&nbsp;interpretation).</p>
<p>Such an approximation can be highly misleading, and now we offer you the
sparse <span class="caps">PCA</span> code that can extract components with only few non-zero
coefficients, and therefore easy to&nbsp;interpret.</p>
<p>For image data, sparse <span class="caps">PCA</span> should extract local components such as,
famously, parts of the face in the case of face&nbsp;recognition.</p>
<p>Personally I can&#8217;t wait to have it ready for the scikit so that I can
play with it in some of my projects. I have two tasks where I can&#8217;t wait
to see the results: one is related to <a href="http://venefrombucharest.wordpress.com/2011/04/14/a-look-at-romanian-verbs-with-scikits-learn/" title="A look at Romanian verbs with scikits-learn">Romanian infinitives</a>, where
<span class="caps">PCA</span> revealed structure, and I would love to see how it looks with sparse
n-gram components. The other task is to plug it in as feature extractor
for handwritten digit classification, for my undergraduate&nbsp;thesis.</p>
<p><span id="footnote_1">[1] <a href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a></span></p>
<p>[[1]]:&nbsp;#footnote_1</p>
    </div><!-- /.entry-content -->
    <div class="comments">
      <h2>Comments !</h2>
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_shortname = 'vene';
        var disqus_identifier = 'blog/sparse-pca.html';
        var disqus_url = '//vene.ro/blog/sparse-pca.html';
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//vene.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>

  </article>
</section>
 </div>
<footer>
  <p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>.
  <a href="/privacy.html">Privacy policy</a>.</p>
</footer>
</div>
</body>
</html>