<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sparse PCA</title>
  <meta name="author" content="Vlad" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
  integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
  crossorigin="anonymous"></script>
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/main.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/pygment.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/typogrify.css" />
  <link rel="shortcut icon" href="//vene.ro/favicon.ico" />
  <link href="//vene.ro/" type="application/atom+xml"
        rel="alternate" title="Vlad Niculae ALL Atom Feed" />
  <link href="//fonts.googleapis.com/css?family=PT+Mono|PT+Serif" rel="stylesheet"> 

  <meta property="og:type" content="website" />
  <meta property="twitter:creator" content=@vnfrombucharest" />
  <meta name="twitter:card" content="summary">

  <!-- OpenGraph Info -->

  <meta name="description" content="I have been working on the integration into the scikits.learn codebase of a sparse principal components analysis (SparsePCA) algorithm coded by Gaël and..." />
  <meta name="keywords" content="dictionary-learning, pca, sparse-pca, sparsepca, spca, scikit-learn" />
  <meta property="og:title" content="Sparse <span class="caps">PCA</span>" />
  <meta property="og:description" content="I have been working on the integration into the scikits.learn codebase of a sparse principal components analysis (SparsePCA) algorithm coded by Gaël and..." />
  <meta property="og:url" content="https://vene.ro/blog/sparse-pca.html" />



  <script src="//vene.ro/theme/js/main.js"></script>

</head>

<body>
<div id="container">
<header>
  <nav class="navmenu" id="navmenu">
    <li id="homelink"><a href="/">Vlad Niculae</a>
    </li><li class="menu"><a href="//vene.ro/papers.html">Research</a>
    </li><li class="menu"><a href="//vene.ro/blog/">Blog</a>
    </li><li class="menu"><a href="//vene.ro/teaching.html">Teaching</a>
    </li><li class="menu"><a href="//vene.ro/students.html">Students</a>
   </li>
   </nav>
 </header>
 <div id="main">
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="//vene.ro/blog/sparse-pca.html" rel="bookmark"
           title="Permalink to Sparse PCA">Sparse <span class="caps">PCA</span></a></h1>
<p class="subtitle"><time datetime="2011-05-23T15:19:00+02:00">Mon, 23 May 2011</time><label for="sparse-pca" class="margin-toggle"> ⊕</label><input type="checkbox" id="sparse-pca" class="margin-toggle" /><span class="marginnote">Category: <a href="//vene.ro/category/scikit-learn.html">scikit-learn</a><br />
 #<a href="//vene.ro/tag/dictionary-learning.html">dictionary learning</a> #<a href="//vene.ro/tag/pca.html">pca</a> #<a href="//vene.ro/tag/sparse-pca.html">sparse pca</a> #<a href="//vene.ro/tag/sparsepca.html">SparsePCA</a> #<a href="//vene.ro/tag/spca.html">spca</a> #<a href="//vene.ro/tag/scikit-learn.html">scikit-learn</a></span></p>    </header>

    <div class="entry-content">
      <p>I have been working on the integration into the scikits.learn codebase
of a sparse principal components analysis (SparsePCA) algorithm coded by
Gaël and Alexandre and based on [[1]][]. Because the name &#8220;sparse <span class="caps">PCA</span>&#8221;
has some inherent ambiguity, I will describe in greater depth what
problem we are actually solving, and what it can be used&nbsp;for.</p>
<h1 id="the-problem">The problem<a class="headerlink" href="#the-problem" title="Permanent link">&para;</a></h1>
<p>Mathematically, this implementation of Sparse <span class="caps">PCA</span>&nbsp;solves:</p>
<p>$latex (U\^*,
V\^*)=\underset{U,V}{\mathrm{argmin\,}}\frac{1}{2}||X-<span class="caps">UV</span>||_2\^2+\alpha||V||_1$</p>
<p>with $latex || U_k ||_2 = 1$ for all $latex 0 \leq k \&lt;&nbsp;n_{atoms}$</p>
<p>This looks really abstract so let&#8217;s try to interpret it. We are looking
for a matrix factorization $latex <span class="caps">UV</span>$ of $latex X \in
\mathbf{R}\^{n_{samples}\times n_{features}}$, just like in
ordinary <span class="caps">PCA</span>. The interpretation is that the $latex n_{atoms}$ lines
of $latex V$ are the extracted components, while the lines of $latex
U$ are the coordinates of the samples in this&nbsp;projection.</p>
<p>The most important difference between this and <span class="caps">PCA</span> is that we enforce
sparsity on the <em>components</em>. In other words, we look for a
representation of the data as a linear combination of sparse&nbsp;signals.</p>
<p>Another difference is that, unlike in <span class="caps">PCA</span>, here we don&#8217;t constrain U to
be orthogonal, just to consist of normalized column vectors. There are
different approaches where this constraint appears too, and they are on
the list for this summer, but I&nbsp;digress.</p>
<h1 id="the-approach">The approach<a class="headerlink" href="#the-approach" title="Permanent link">&para;</a></h1>
<p>As usual, such optimization problems are solved by alternatively
minimizing one of the variables while keeping the other fixed, until
convergence is&nbsp;reached.</p>
<p>The update of $latex V$ (the dictionary) is computed as the solution
of a Lasso least squares problem.  We allow the user to choose between
the least angle regression method (<span class="caps">LARS</span>) or stochastic gradient descent
as algorithms to solve the Lasso&nbsp;problem.</p>
<p>The update of $latex U$ is block coordinate descent with warm restart.
This is a batch adaptation of an online algorithm proposed by Mairal et
al. in&nbsp;[[1]][].</p>
<h1 id="sparse-pca-as-a-transformer">Sparse <span class="caps">PCA</span> as a transformer<a class="headerlink" href="#sparse-pca-as-a-transformer" title="Permanent link">&para;</a></h1>
<p>Of course, in order to be of practical use, the code needs to be
refactored into a scikits.learn transformer object, just&nbsp;like
<code>scikits.learn.decomposition.pca</code>. This means that the optimization
problem described above corresponds to the fitting stage. The post-fit
state of the transformer is given by the learned components (the matrix
$latex V$&nbsp;above).</p>
<p>In order to transform new data according to the learned sparse <span class="caps">PCA</span> model
(for example, prior to classification of the test data), we simply need
to do a least squares projection of the new data on the sparse&nbsp;components.</p>
<h1 id="what-is-it-good-for">What is it good for?<a class="headerlink" href="#what-is-it-good-for" title="Permanent link">&para;</a></h1>
<p>For applications such as text and image processing, its great advantage
is interpretability. When running a regular <span class="caps">PCA</span> on a set of documents in
bag of words format, we can find an interesting visualisation on a
couple of components, and it can show discrimination or clusters. The
biggest problem is that the maximum variance components found by <span class="caps">PCA</span>
have very dense expressions as linear combinations of the initial
features. In practice, sometimes interpretation is made by simply
marking the $latex k$ variables with the highest coefficients in this
representation, and basically interpreting as if the rest are truncated
to 0 (this has been taught to me in a class on <span class="caps">PCA</span>&nbsp;interpretation).</p>
<p>Such an approximation can be highly misleading, and now we offer you the
sparse <span class="caps">PCA</span> code that can extract components with only few non-zero
coefficients, and therefore easy to&nbsp;interpret.</p>
<p>For image data, sparse <span class="caps">PCA</span> should extract local components such as,
famously, parts of the face in the case of face&nbsp;recognition.</p>
<p>Personally I can&#8217;t wait to have it ready for the scikit so that I can
play with it in some of my projects. I have two tasks where I can&#8217;t wait
to see the results: one is related to <a href="http://venefrombucharest.wordpress.com/2011/04/14/a-look-at-romanian-verbs-with-scikits-learn/" title="A look at Romanian verbs with scikits-learn">Romanian infinitives</a>, where
<span class="caps">PCA</span> revealed structure, and I would love to see how it looks with sparse
n-gram components. The other task is to plug it in as feature extractor
for handwritten digit classification, for my undergraduate&nbsp;thesis.</p>
<p><span id="footnote_1">[1] <a href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a></span></p>
<p>[[1]]:&nbsp;#footnote_1</p>
    </div><!-- /.entry-content -->
    <div class="comments">
      <h2>Comments !</h2>
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_shortname = 'vene';
        var disqus_identifier = 'blog/sparse-pca.html';
        var disqus_url = 'https://vene.ro/blog/sparse-pca.html';
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://vene.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>

  </article>
</section>
 </div>
<footer>
  <p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>.
  <a href="/privacy.html">Privacy policy</a>.</p>
</footer>
</div>
</body>
</html>