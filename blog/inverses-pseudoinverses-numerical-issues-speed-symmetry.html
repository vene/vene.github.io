<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Inverses and pseudoinverses. Numerical issues, speed, symmetry.</title>
  <meta name="author" content="Vlad" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
  integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
  crossorigin="anonymous"></script>
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/main.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/pygment.css" />
  <link rel="stylesheet" type="text/css" 
        href="//vene.ro/theme/css/typogrify.css" />
  <link rel="shortcut icon" href="//vene.ro/favicon.ico" />
  <link href="//vene.ro/" type="application/atom+xml"
        rel="alternate" title="Vlad Niculae ALL Atom Feed" />
  <link href="//fonts.googleapis.com/css?family=PT+Mono|PT+Serif" rel="stylesheet"> 

  <meta property="og:type" content="website" />
  <meta property="twitter:creator" content=@vnfrombucharest" />
  <meta name="twitter:card" content="summary">

  <!-- OpenGraph Info -->


  <script src="//vene.ro/theme/js/main.js"></script>

</head>

<body>
<div id="container">
<header>
  <nav class="navmenu" id="navmenu">
    <li id="homelink"><a href="/">Vlad Niculae</a>
    </li><li class="menu"><a href="//vene.ro/papers.html">Research</a>
    </li><li class="menu"><a href="//vene.ro/blog/">Blog</a>
    </li><li class="menu"><a href="//vene.ro/teaching.html">Teaching</a>
    </li><li class="menu"><a href="//vene.ro/students.html">Students</a>
   </li>
   </nav>
 </header>
 <div id="main">
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="//vene.ro/blog/inverses-pseudoinverses-numerical-issues-speed-symmetry.html" rel="bookmark"
           title="Permalink to Inverses and pseudoinverses. Numerical issues, speed, symmetry.">Inverses and pseudoinverses. Numerical issues, speed,&nbsp;symmetry.</a></h1>
<p class="subtitle"><time datetime="2012-08-18T19:41:00+02:00">Sat, 18 Aug 2012</time><label for="inverses-pseudoinverses-numerical-issues-speed-symmetry" class="margin-toggle"> ⊕</label><input type="checkbox" id="inverses-pseudoinverses-numerical-issues-speed-symmetry" class="margin-toggle" /><span class="marginnote">Category: <a href="//vene.ro/category/benchmarking.html">benchmarking</a><br />
 #<a href="//vene.ro/tag/inv.html">inv</a> #<a href="//vene.ro/tag/matrix-inverse.html">matrix inverse</a> #<a href="//vene.ro/tag/numerical-analysis.html">numerical analysis</a> #<a href="//vene.ro/tag/numerical-methods.html">numerical methods</a> #<a href="//vene.ro/tag/pinv.html">pinv</a> #<a href="//vene.ro/tag/pinvh.html">pinvh</a> #<a href="//vene.ro/tag/positive-semidefinite.html">positive semidefinite</a> #<a href="//vene.ro/tag/pseudoinverse.html">pseudoinverse</a> #<a href="//vene.ro/tag/symmetric.html">symmetric</a> #<a href="//vene.ro/tag/benchmarking.html">benchmarking</a> #<a href="//vene.ro/tag/python.html">python</a></span></p>    </header>

    <div class="entry-content">
      <p>The matrix inverse is a cornerstone of linear algebra, taught, along
with its applications, since high school. The inverse of a matrix
$latex A$, if it exists, is the matrix $latex A\^{-1}$ such that
$latex <span class="caps">AA</span>\^{-1} = A\^{-1}A = I_n$. Based on the requirement that the
left and right multiplications should be equal, it follows that it only
makes sense to speak of inverting square matrices. But just the square
shape is not enough: for a matrix $latex A$ to have an inverse,
$latex A$ must be full&nbsp;rank.</p>
<p>The inverse provides an elegant (on paper) method of finding solutions
to systems of $latex n$ equations with $latex n$ unknowns, which
correspond to solving $latex Ax = b$ for $latex x$. If we&#8217;re lucky
and $latex A\^{-1}$ exists, then we can find $latex x = A\^{-1}b$.
For this to work, it must be the case&nbsp;that:</p>
<ul>
<li>We have exactly as many unknowns as&nbsp;equations</li>
<li>No equation is redundant, i.e. can be expressed as a linear
    combination of the&nbsp;others</li>
</ul>
<p>In this setting, there is a unique solution for $latex&nbsp;x$.</p>
<h2 id="the-moore-penrose-pseudoinverse">The Moore-Penrose pseudoinverse<a class="headerlink" href="#the-moore-penrose-pseudoinverse" title="Permanent link">&para;</a></h2>
<p>What if we have more equations than unknowns? It is most likely the case
that we cannot satisfy all the equations perfectly, so let&#8217;s settle for
a solution that best fits the constraints, in the sense of minimising
the sum of squared errors. We solve $latex \operatorname{arg\,min}_x
||b -&nbsp;Ax||$.</p>
<p>And how about the other extreme, where we have a lot of unknowns, but
just a few equations constraining them. We will probably have an
infinity of solutions, how can we choose one? A popular choice is to
take the one of least $latex \ell_2$ norm: $latex
\operatorname{arg\,min}_x ||x|| \operatorname{s.t.} Ax = b$. Is
there a way to generalize the idea of a matrix inverse for this&nbsp;setting?</p>
<p>The pseudoinverse of an arbitrary-shaped matrix $latex A$, written
$latex A\^{+}$, has the same shape as $latex A\^{T}$ and solves our
problem: the answer to both optimization methods above is given by
$latex x =&nbsp;A\^{+}y$.</p>
<p>The theoretical definition of the pseudoinverse is given by the
following conditions. The intuitive way to read them is as properties of
$latex <span class="caps">AA</span>\^+$ or $latex&nbsp;A\^+A$:</p>
<ul>
<li>$latex <span class="caps">AA</span>\^+A =&nbsp;A$</li>
<li>$latex A\^+<span class="caps">AA</span>\^+ =&nbsp;A\^+$</li>
<li>$latex (<span class="caps">AA</span>\^+)\^T = <span class="caps">AA</span>\^+$</li>
<li>$latex (A\^+A)\^T =&nbsp;A\^+A$</li>
</ul>
<p>These conditions do not however give us a way to get our hands on a
pseudoinverse, so we need something&nbsp;else.</p>
<h2 id="how-to-compute-the-pseudoinverse-on-paper">How to compute the pseudoinverse on paper<a class="headerlink" href="#how-to-compute-the-pseudoinverse-on-paper" title="Permanent link">&para;</a></h2>
<p>The first time I ran into the pseudoinverse, I didn&#8217;t even know its
definition, only the expression of the closed-form solution of such a
problem, and given&nbsp;as:</p>
<p>$latex A\^+ = (A\^T&nbsp;A)\^{-1}A\^T$</p>
<p>What can we see from this&nbsp;expression:</p>
<ul>
<li>It gives us a way to compute the pseudoinverse, and hence to solve
    the&nbsp;problem</li>
<li>If $latex A$ is actually invertible, it means $latex A\^T$ is
    invertible, so we have $latex A\^+ = A\^{-1}(A\^T)\^{-1}A\^T =&nbsp;A\^{-1}$</li>
<li>Something bad happens if $latex A\^<span class="caps">TA</span>$ is not&nbsp;invertible.</li>
</ul>
<p>The pseudoinverse is still defined, and unique, when $latex A\^<span class="caps">TA</span>$ is
not invertible, but we cannot use the expression above to compute&nbsp;it.</p>
<h2 id="numerical-issues">Numerical issues<a class="headerlink" href="#numerical-issues" title="Permanent link">&para;</a></h2>
<p>Before going on, we should clarify and demystify some of the urban
legends about numerical computation of least squares problems. You might
have heard the following unwritten&nbsp;rules:</p>
<ol>
<li>Never compute $latex A\^{-1}$, solve the system&nbsp;directly</li>
<li>If you really need $latex A\^{-1}$, use <code>pinv</code> and not <code>inv</code></li>
</ol>
<p>The first of these rules is based on some misguided beliefs, but is
still good advice. If your goal is a one-shot answer to a system,
there&#8217;s no use in explicitly computing a possibly large inverse, when
all you need is $latex x$. But <a href="http://arxiv.org/abs/1201.6035">this paper</a> shows that computing the
inverse is not necessarily a bad thing. The key to this is conditional
accuracy, and as long as the <code>inv</code> function used has good conditional
bounds, you will get as good results as with a least squares&nbsp;solver.</p>
<p>The second rule comes from numerical stability, and will definitely bite
you if misunderstood. If $latex A$ is a square matrix with a row full
of zeros, it&#8217;s clearly not invertible, so an algorithm attempting to
compute the inverse will fail and you will be able to catch that
failure. But what if the row is not exactly zero, but the sum of several
other rows, and a slight loss of precision is propagated at every&nbsp;step?</p>
<h2 id="numerical-rank-vs-actual-rank">Numerical rank vs. actual rank<a class="headerlink" href="#numerical-rank-vs-actual-rank" title="Permanent link">&para;</a></h2>
<p>The rank of a matrix $latex A$ is defined as the number of linearly
independent rows (or equivalently, columns) in $latex A$. In other
words, the number of non-redundant equations in the system. We&#8217;ve seen
before that if the rank is less than the total number of rows, the
system cannot have a unique solution anymore, so the matrix $latex A$
is not&nbsp;invertible.</p>
<p>The rank of a matrix is a computationally tricky problem. On paper, with
small matrices, you would look at minors of decreasing size, until you
find the first non-zero one. This is unfeasible to implement on a
computer, so numerical analysis has a different approach. Enter the
singular value&nbsp;decomposition!</p>
<p>The <span class="caps">SVD</span> of a matrix $latex A$ is $latex A = <span class="caps">USV</span>\^{T}$, where $latex
S$ is diagonal and $latex U, V$ are orthogonal. The elements on the
diagonal of $latex S$ are called the singular values of $latex A$.
It can be seen that to get a row full of zeros when multiplying three
such matrices, a singular value needs to be exactly&nbsp;zero.</p>
<p>The ugly thing that could happen is that one (or usually more) singular
values are not exactly zero, but very low values, due to propagated
imprecision. Why is this a problem? By looking at the <span class="caps">SVD</span> and noting its
properties, it becomes clear that $latex A\^{-1} = <span class="caps">VS</span>\^{-1}U\^{T}$ and
since $latex S$ is diagonal, its inverse is formed by taking the
inverse of all the elements on the diagonal. But if a singular value is
very small but not quite zero, its inverse is very large and it will
blow up the whole computation of the inverse. The right thing to do here
is either to tell the user that $latex A$ is numerically rank
deficient, or to return a pseudoinverse instead. A pseudoinverse would
mean: give up on trying to get $latex <span class="caps">AA</span>\^+$ to be the identity
matrix, simply aim for a diagonal matrix with approximately ones and
zeroes. In other words, when singular values are very low, set them to&nbsp;0.</p>
<p>How do you set the threshold? This is actually a delicate issue, being
discussed on <a href="http://thread.gmane.org/gmane.comp.python.numeric.general/50396/focus=50912">the numeric Python mailing list</a>.</p>
<h2 id="scipy-implementations">Scipy implementations<a class="headerlink" href="#scipy-implementations" title="Permanent link">&para;</a></h2>
<p>Scipy exposes <code>inv</code>, <code>pinv</code> and <code>pinv2</code>. <code>inv</code> secretly invokes <span class="caps">LAPACK</span>,
that ancient but crazy robust code that&#8217;s been used since the 70s, to
first compute a pivoted <span class="caps">LU</span> decomposition that is then used to compute
the inverse. <code>pinv</code> also uses <span class="caps">LAPACK</span>, but for computing the
least-squares solution to the system $latex <span class="caps">AX</span> = I$. <code>pinv2</code> computes
the <span class="caps">SVD</span> and transposes everything like shown above. Both <code>pinv</code> and
<code>pinv2</code> expose <code>cond</code> and <code>rcond</code> arguments to handle the treatment of
very small singular values, but (<em>attention!</em>) they behave&nbsp;differently!</p>
<p>The different implementations also lead to different speed. Let&#8217;s look
at inverting a random square&nbsp;matrix:</p>
<p>[sourcecode lang=&#8221;python&#8221;]<br />
In [1]: import numpy as&nbsp;np</p>
<p>In [2]: from scipy import&nbsp;linalg</p>
<p>In [3]: a = np.random.randn(1000,&nbsp;1000)</p>
<p>In [4]: timeit linalg.inv(a)<br />
10 loops, best of 3: 132 ms per&nbsp;loop</p>
<p>In [5]: timeit linalg.pinv(a)<br />
1 loops, best of 3: 18.8 s per&nbsp;loop</p>
<p>In [6]: timeit linalg.pinv2(a)<br />
1 loops, best of 3: 1.58 s per loop<br />&nbsp;[/sourcecode]</p>
<p>Woah, huge difference! But do all three methods return the &#8220;right&#8221;&nbsp;result?</p>
<p>[sourcecode lang=&#8221;python&#8221;]<br />
In [7]: linalg.inv(a)[:3, :3]<br />
Out[7]:<br />
array([[ 0.03636918, 0.01641725, 0.00736503],<br />
[-0.04575771, 0.03578062, 0.02937733],<br />
[ 0.00542367, 0.01246306, 0.0122156&nbsp;]])</p>
<p>In [8]: linalg.pinv(a)[:3, :3]<br />
Out[8]:<br />
array([[ 0.03636918, 0.01641725, 0.00736503],<br />
[-0.04575771, 0.03578062, 0.02937733],<br />
[ 0.00542367, 0.01246306, 0.0122156&nbsp;]])</p>
<p>In [9]: linalg.pinv2(a)[:3, :3]<br />
Out[9]:<br />
array([[ 0.03636918, 0.01641725, 0.00736503],<br />
[-0.04575771, 0.03578062, 0.02937733],<br />
[ 0.00542367, 0.01246306, 0.0122156&nbsp;]])</p>
<p>In [10]: np.testing.assert_array_almost_equal(linalg.inv(a),&nbsp;linalg.pinv(a))</p>
<p>In [11]: np.testing.assert_array_almost_equal(linalg.inv(a),
linalg.pinv2(a))<br />&nbsp;[/sourcecode]</p>
<p>Looks good! This is because we got lucky, though, and <code>a</code> was invertible
to start with. Let&#8217;s look at its&nbsp;spectrum:</p>
<p>[sourcecode lang=&#8221;python&#8221;]<br />
In [12]: _, s, _ =&nbsp;linalg.svd(a)</p>
<p>In [13]: np.min(s), np.max(s)<br />
Out[13]: (0.029850235603382822, 62.949785645178906)<br />&nbsp;[/sourcecode]</p>
<p>This is a lovely range for the singular values of a matrix, not too
small, not too large. But what if we built the matrix in a way that
would always pose problems? Specifically, let&#8217;s look at the case of
covariance&nbsp;matrices:</p>
<p>[sourcecode lang=&#8221;python&#8221;]<br />
In [14]: a = np.random.randn(1000,&nbsp;50)</p>
<p>In [15]: a = np.dot(a,&nbsp;a.T)</p>
<p>In [16]: _, s, _ =&nbsp;linalg.svd(a)</p>
<p>In [17]: s[-9:]<br />
Out[17]:<br />
array([ 7.40548924e-14, 6.48102455e-14, 5.75803505e-14,<br />
5.44263048e-14, 4.51528730e-14, 3.55317976e-14,<br />
2.46939141e-14, 1.54186776e-14,&nbsp;5.08135874e-15])</p>
<p>[/sourcecode]</p>
<p><code>a</code> has at least 9 tiny singular values. Actually it&#8217;s easy to see why
there are 950 of&nbsp;them:</p>
<p>[sourcecode lang=&#8221;python&#8221;]<br />
In [18]: np.sum(s \&lt; 1e-10)<br />
Out[18]: 950<br />&nbsp;[/sourcecode]</p>
<p>How do our functions behave in this case? Instead of just looking at a
corner, let&#8217;s use our gift of sight:[<img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/pseudoinverses-300x218.png" title="Pseudoinverses" />]<a href="http://localhost:8001/wp-content/uploads/2012/08/pseudoinverses-300x218.png" title="Pseudoinverses"></a></p>
<p>The small eigenvalues are large enough that <code>inv</code> thinks the matrix is
full rank. <code>pinv</code> does better but it still fails, you can see a group of
high-amplitude noisy columns. <code>pinv2</code> is faster and it also gives us a
useful result in this&nbsp;case.</p>
<p>Wait, does this mean that <code>pinv2</code> is simply better, and <code>pinv</code> is&nbsp;useless?</p>
<p>Not quite. Remember, we are now trying to actually invert matrices, and
degrade gracefully in case of rank deficiency. But what if we need the
pseudoinverse to solve an actual non-square, wide or tall&nbsp;system?</p>
<p>[sourcecode lang=&#8221;python&#8221;]<br />
In [19]: a = np.random.randn(1000,&nbsp;50)</p>
<p>In [20]: timeit linalg.pinv(a)<br />
10 loops, best of 3: 104 ms per&nbsp;loop</p>
<p>In [21]: timeit linalg.pinv(a.T)<br />
100 loops, best of 3: 7.08 ms per&nbsp;loop</p>
<p>In [22]: timeit linalg.pinv2(a)<br />
10 loops, best of 3: 114 ms per&nbsp;loop</p>
<p>In [23]: timeit linalg.pinv2(a.T)<br />
10 loops, best of 3: 126 ms per loop<br />&nbsp;[/sourcecode]</p>
<p>Huge victory for <code>pinv</code> in the wide case! Hurray! With all this insight,
we can draw a line and see what we&nbsp;learned.</p>
<ul>
<li>If you are 100% sure that your matrix is invertible, use <code>inv</code> for a
    huge speed gain. The implementation of <code>inv</code> from Scipy is based on
    <span class="caps">LAPACK</span>&#8217;s <code>*getrf</code> + <code>*getri</code>, known to have good&nbsp;bounds.</li>
<li>If you are trying to solve a tall or wide system, use <code>pinv</code>.</li>
<li>If your matrix is square but might be rank deficient, use <code>pinv2</code>
    for speed and numerical&nbsp;gain.</li>
</ul>
<h2 id="improving-the-symmetric-case">Improving the symmetric case<a class="headerlink" href="#improving-the-symmetric-case" title="Permanent link">&para;</a></h2>
<p>But wait a second, can&#8217;t we do better? $latex <span class="caps">AA</span>\^T$ is symmetric,
can&#8217;t we make use of that to speed up the computation even more?
Clearly, if $latex A$ is symmetric, in its <span class="caps">SVD</span> $latex A = <span class="caps">USV</span>\^T$,
we must have $latex U = V$. But this is exactly the eigendecomposition
of a symmetric matrix $latex A$. The eigendecomposition can be
computed cheaper than the <span class="caps">SVD</span> using Scipy <code>eigh</code>, that uses <span class="caps">LAPACK</span>&#8217;s
<code>*evr</code>. As part of my GSoC this year, with help from <a href="http://jakevdp.github.com/">Jake
VanderPlas</a>, we made a <a href="https://github.com/scipy/scipy/pull/289">pull request to Scipy</a> containing a <code>pinvh</code>
function that is equivalent to <code>pinv2</code> but faster for symmetric&nbsp;matrices.</p>
<p>[sourcecode lang=&#8221;python&#8221;]<br />
In [24]: timeit linalg.pinv2(a)<br />
1 loops, best of 3: 1.54 s per&nbsp;loop</p>
<p>In [25]: timeit linalg.pinvh(a)<br />
1 loops, best of 3: 621 ms per&nbsp;loop</p>
<p>In [26]: np.testing.assert_array_almost_equal(linalg.pinv2(a),
linalg.pinvh(a))<br />&nbsp;[/sourcecode]</p>
<p>[<img alt="" src="http://localhost:8001/wp-content/uploads/2012/08/pseudoinverses-300x218.png" title="Pseudoinverses" />]:&nbsp;http://localhost:8001/wp-content/uploads/2012/08/pseudoinverses.png</p>
    </div><!-- /.entry-content -->
    <div class="comments">
      <h2>Comments !</h2>
      <div id="disqus_thread"></div>
      <script type="text/javascript">
        var disqus_shortname = 'vene';
        var disqus_identifier = 'blog/inverses-pseudoinverses-numerical-issues-speed-symmetry.html';
        var disqus_url = 'https://vene.ro/blog/inverses-pseudoinverses-numerical-issues-speed-symmetry.html';
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://vene.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>

  </article>
</section>
 </div>
<footer>
  <p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a>.
  <a href="/privacy.html">Privacy policy</a>.</p>
</footer>
</div>
</body>
</html>